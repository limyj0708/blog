[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Lim's Code Archive",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\n9999-12-31\n\n\nPandas CheatSheet\n\n\nlimyj0708\n\n\n\n\n9999-12-30\n\n\nGit CheatSheet\n\n\nlimyj0708\n\n\n\n\n2022-10-29\n\n\nBigquery_Array, Struct 조합 사용과 Cartesian Product\n\n\nlimyj0708\n\n\n\n\n2022-10-29\n\n\nBigquery_사분위수 구하기\n\n\nlimyj0708\n\n\n\n\n2022-10-29\n\n\nBigquery_사용자 정의 함수(UDF)\n\n\nlimyj0708\n\n\n\n\n2022-10-25\n\n\nBigquery_7일 연속 미접속 시작일 쉽게 추출하기\n\n\nlimyj0708\n\n\n\n\n2022-05-26\n\n\nJupyter Lab Server 세팅\n\n\nlimyj0708\n\n\n\n\n2022-05-26\n\n\nPython Google Drive API v3로 파일 업로드\n\n\nlimyj0708\n\n\n\n\n2022-03-19\n\n\n맥 OS pyenv 세팅 101\n\n\nlimyj0708\n\n\n\n\n2021-11-17\n\n\n리눅스 Shell 명령어 Python 스크립트에서 실행하기 + Crontab\n\n\nlimyj0708\n\n\n\n\n2021-11-08\n\n\nPython 스크립트 Console 유저 입력 받기\n\n\nlimyj0708\n\n\n\n\n2021-09-07\n\n\nCrontab으로 Python 스크립트 주기적으로 실행하기\n\n\nlimyj0708\n\n\n\n\n2021-09-06\n\n\nLinux_비밀번호 만료 안 되게 하기\n\n\nlimyj0708\n\n\n\n\n2020-02-11\n\n\nRedshift에 데이터를 적재하는 과정에서 얻은 교훈 & psycopg2\n\n\nlimyj0708\n\n\n\n\n2020-02-06\n\n\n로컬 머신에서 AWS Redshift에 접근하기\n\n\nlimyj0708\n\n\n\n\n2019-11-11\n\n\nAsynchronous, Synchronous, Blocking, Non-Blocking\n\n\nlimyj0708\n\n\n\n\n2019-10-30\n\n\nPython_Decorator가 뭐지? with Scope, Namespace\n\n\nlimyj0708\n\n\n\n\n2019-10-12\n\n\nMap vs List Comprehension\n\n\nlimyj0708\n\n\n\n\n2019-10-12\n\n\nUbuntu 새 유저 SSH key 추가\n\n\nlimyj0708\n\n\n\n\n2019-10-11\n\n\nPython_Parameters, Arguments 정의와 차이점\n\n\nlimyj0708\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2019-10-11-Python_Parameters, Arguments 정의와 차이점.html",
    "href": "posts/2019-10-11-Python_Parameters, Arguments 정의와 차이점.html",
    "title": "Python_Parameters, Arguments 정의와 차이점",
    "section": "",
    "text": "Parameters are defined by the names that appear in a function definition, whereas arguments are the values actually passed to a function when calling it. Parameters define what types of arguments a function can accept. For example, given the function definition:\nfoo, bar and kwargs are parameters of func. However, when calling func, for example:\nthe values 42, 314, and somevar are argument\nPython FAQ\nParameter와 Argument의 차이는 알았다. 그럼 각각의 자세한 정의는 어떻게 될까?"
  },
  {
    "objectID": "posts/2019-10-11-Python_Parameters, Arguments 정의와 차이점.html#parameter",
    "href": "posts/2019-10-11-Python_Parameters, Arguments 정의와 차이점.html#parameter",
    "title": "Python_Parameters, Arguments 정의와 차이점",
    "section": "parameter",
    "text": "parameter\n\nGlossary > Parameter\nA named entity in a function (or method) definition that specifies an argument (or in some cases, arguments) that the function can accept. There are five kinds of parameter:\npositional-or-keyword: specifies an argument that can be passed either positionally or as a keyword argument. This is the default kind of parameter, for example foo and bar in the following:\n\n\ndef func(foo, bar=None): ...\n\n두 형태로 다 받아도 상관이 없는 형태의 예시\n\ndef sum(a, b=10):\n    print(a+b)\nsum(1,2)\nsum(1,b=2)\n# 출력값은 둘 다 3으로 잘 나온다.\n# b는 keyword parameter지만, positional처럼 값을 넣어도 잘 작동한다.\n\n3\n3\n\n\n\npositional-only: specifies an argument that can be supplied only by position. Python has no syntax for defining positional-only parameters. However, some built-in functions have positional-only parameters (e.g. abs()).\nabs()에 keyword argument를 넣으려고 하면, 시원하게 에러가 뜬다.\n\n\nabs(foo=10)\n\nTypeError: abs() takes no keyword arguments\n\n\n\nkeyword-only: specifies an argument that can be supplied only by keyword. Keyword-only parameters can be defined by including a single var-positional parameter or bare * in the parameter list of the function definition before them, for example kw_only1 and kw_only2 in the following:\n\n\ndef func(*arg, *, kw_only1, kw_only2): ...\n\n\n단일 Asterisk 뒤에 오는 parameter들은 무조건 keyword parameter여야 한다. PEP-3102\nvar-positional parameter 뒤에 오는 parameter들은 무조건 keyword parameter여야 한다.\n\n\ndef print1(a, b, *, kw_only1=None, kw_only2=None, positional):\n    return print(a,b,kw_only1, kw_only2, positional)\n\nprint1(3,4,kw_only1=1, kw_only2=2, 3)\n# 장렬한 에러 메세지\n\nSyntaxError: positional argument follows keyword argument (4141133395.py, line 3)\n\n\n\nvar-positional: specifies that an arbitrary sequence of positional arguments can be provided (in addition to any positional arguments already accepted by other parameters). Such a parameter can be defined by prepending the parameter name with *, for example args in the following:\n\n\ndef func(*args, **kwargs): ...\n\n\nvar-keyword: specifies that arbitrarily many keyword arguments can be provided (in addition to any keyword arguments already accepted by other parameters). Such a parameter can be defined by prepending the parameter name with, for example kwargs in the example above. Parameters can specify both optional and required arguments, as well as default values for some optional arguments.\n\n직접 임의의(arbitrary) argument들을 마음껏 넣어보자.\n\ndef print1(*args, **kwargs):\n    print(args)\n    print(kwargs)\n\nprint1(1,2,3,4,5, kw1='a', kw2='b', kw3='hoho')\n\n(1, 2, 3, 4, 5)\n{'kw1': 'a', 'kw2': 'b', 'kw3': 'hoho'}\n\n\nSee also the argument glossary entry, the FAQ question on the difference between arguments and parameters, the inspect.Parameter class, the Function definitions section, and PEP 362."
  },
  {
    "objectID": "posts/2019-10-11-Python_Parameters, Arguments 정의와 차이점.html#argument",
    "href": "posts/2019-10-11-Python_Parameters, Arguments 정의와 차이점.html#argument",
    "title": "Python_Parameters, Arguments 정의와 차이점",
    "section": "argument",
    "text": "argument\n\nGlossary > Argument\n\nA value passed to a function (or method) when calling the function. There are two kinds of argument:\n\nkeyword argument: an argument preceded by an identifier (e.g. name=) in a function call or passed as a value in a dictionary preceded by **. For example, 3 and 5 are both keyword arguments in the following calls to complex():\n\n\ncomplex(real=3, imag=5)\ncomplex(**{'real': 3, 'imag': 5})\n# 복소수를 출력하는 함수. 출력값은 (3+5j) 가 된다.\n# Asterisk가 data structure를 해체하여 전달하는 역할을 하는데, \n# 자세한 내용은 다른 포스트에서 다뤄보자.\n\n(3+5j)\n\n\n\npositional argument: an argument that is not a keyword argument. Positional arguments can appear at the beginning of an argument list and/or be passed as elements of an iterable preceded by *. For example, 3 and 5 are both positional arguments in the following calls:\n\n\ncomplex(3, 5)\n\n(3+5j)\n\n\n\ncomplex(*(3, 5))\n\n(3+5j)\n\n\nArguments are assigned to the named local variables in a function body. See the Calls section for the rules governing this assignment. Syntactically, any expression can be used to represent an argument; the evaluated value is assigned to the local variable.\nSee also the parameter glossary entry, the FAQ question on the difference between arguments and parameters, and PEP 362."
  },
  {
    "objectID": "posts/2019-10-12-Map vs List Comprehension.html",
    "href": "posts/2019-10-12-Map vs List Comprehension.html",
    "title": "Lim's Code Archive",
    "section": "",
    "text": "List comprehension vs Map\n위의 Stack Overflow 질문에 아주 좋은 답변들이 달려 있어서, Upvote 상위 두 개 답변을 살펴보았다.\n\n\n\nMap이 몇몇 경우에 아주 약간 더 빠르다. (lambda 안 쓰고, 같은 기능을 하는 함수를 사용할 경우) List Comprehension은 나머지 경우에서 더 빠르며, 대부분의 파이썬 사용자들은 List Comprehension이 더 직관적이고 명확하다고 생각한다.\n# 터미널에서 아래와 같이 실행해보자\n\n$ python -mtimeit -s'xs=range(10)' 'map(hex, xs)'\n100000 loops, best of 3: 4.86 usec per loop\n# hex() -> 16진수로 변경\n\n$ python -mtimeit -s'xs=range(10)' '[hex(x) for x in xs]'\n100000 loops, best of 3: 5.58 usec per loop\n# 그냥 함수를 사용했더니, map이 근소하게 더 빠르다\n하지만 lambda를 쓰면 어떨까?\n$ python -mtimeit -s'xs=range(10)' 'map(lambda x: x+2, xs)'\n100000 loops, best of 3: 4.24 usec per loop\n$ python -mtimeit -s'xs=range(10)' '[x+2 for x in xs]'\n100000 loops, best of 3: 2.32 usec per loop\n\n# 속도가 정 반대가 되었다.\n\n\n\nLaziness\nPython에서 Map은 게으르다. 무슨 말인고 하니, 계산 결과 전체를 반환하는 것이 아니라, 계산 로직을 보관하고 있다가 값 요청이 왔을 때 계산하여 값을 제공해준다는 것이다.\n>>> map(str, range(10**100))\n<map object at 0x2201d50>\n# 리스트가 아니다\nList Comprehension이라면 전체 계산결과 리스트를 반환한다. (Not lazy)\n>>> [str(n) for n in range(10**100)]\n# 이런 짓 하지 말라는 것이다.\n# DO NOT TRY THIS AT HOME OR YOU WILL BE SAD #\n‘게으른’ List Comprehension도 Generator expression의 형태로 지원한다.\n>>> (str(n) for n in range(10**100))\n<generator object <genexpr> at 0xacbdef>"
  },
  {
    "objectID": "posts/2019-10-30-Decorator가 뭐지 with Scope, Namespace.html",
    "href": "posts/2019-10-30-Decorator가 뭐지 with Scope, Namespace.html",
    "title": "Python_Decorator가 뭐지? with Scope, Namespace",
    "section": "",
    "text": "Decorator : 오브젝트의 구조를 변경하지 않고 새 기능을 추가 할 수 있게 해 주는 디자인 패턴. Python에서는 @를 키워드로 사용한다.\nDecorator의 간단한 예시와 실행결과를 보자.\n\n\ndef print_line(func): # Decorator가 될 함수\n    def wrapper_mine():\n        print('-'*30) # func의 사전작업이라 할 수 있다.\n        func()\n        print('#'*30) # func의 사후작업이라 할 수 있다.\n    return wrapper_mine # wrapper_mine을 호출한 게 아니고, 함수 객체를 그냥 반환한 거다.\n\n\ndef my_function():\n    print(\"my_function 실행\")\n\nm = print_line(my_function) # print_line에 직접 argument를 전달하여 실행해 보자\nm()\n\n------------------------------\nmy_function 실행\n##############################\n\n\n\n@print_line # decorator를 사용하자\ndef my_function2():\n    print(\"my_function2 실행2\")\n\nmy_function2()\n\n------------------------------\nmy_function2 실행2\n##############################\n\n\n여기서 몇 가지 궁금한 점이 생긴다. 1. function(여기서는 wrapper_mine)을 value처럼 막 return 하네? 2. wrapper_mine이 어떻게 print_line의 argument에 접근할 수 있지? - 그게 그냥 된대~ 하고 사용해 왔지만 정확한 철학을 알고 싶다\n일단 1번부터 알아 보자. 왜 함수를 return이 가능하죠?\n\n\n\n\nWikipedia의 친절한 설명을 보자. Python first class citizen 이라고 검색하면 나오는 포스트들 다 여기서 내용 가져온 거다.\n\nWikipedia - First-class citizen\n\nIn programming language design, a first-class citizen (also type, object, entity, or value) in a given programming language is an entity which supports all the operations generally available to other entities. These operations typically include being passed as an argument, returned from a function, modified, and assigned to a variable.\nFirst Class Citizen은 Argument로 넘기기, 함수에서 return되기, 조작되기, 변수에 할당되기와 같은, 다른 독립체들과 상호작용 할 수 있는 연산을 지원한다.\n\n\n그리고 Python에서는 함수도 first class object다.\n“function을 value처럼 막 return 하네?” 해결. 그럼 다음 문제, wrapper_mine이 어떻게 print_line의 argument에 접근할 수 있지?\n\n\n\n\n\n\nNamespace? : 객체와 이름이 매핑된 공간. 대부분의 네임스페이스는 딕셔너리로 적용된다.\n\n네임스페이스의 예시\n\nbuilt-in된 이름들 : abs()같은 함수명이나, 예외명들.\n모듈 내의 전역 이름들 (global names in module)\n함수 내의 지역 이름들 (local names)\ne.g) zzz.real과 ddd.real은 real이라는 attribute 이름은 같을지라도, 전혀 다른 네임스페이스에서 가져온 것이기 때문에 아무런 연관도 없다.\n\n네임스페이스의 수명주기(lifetime)\n\n각각 다른 시기에 생겨나며, 각각 다른 수명을 가짐\nbuilt-in name을 보유한 네임스페이스는 인터프리터가 시작할 때 생겨나고, 인터프리터가 꺼질 때까지 사라지지 않음.\n모듈의 글로벌 네임스페이스는 모듈의 정의가 읽혀질 때 생겨나며, 인터프리터가 꺼질 때까지 사라지지 않음.\n함수의 지역 네임스페이스는 함수가 호출될 때 생겨나고, 함수가 결과를 반환하거나 함수 내부에서 처리되지 않는 에러를 내보낼 때 사라진다.\n\n\n\n\n\n\n\nScope? : 네임스페이스가 ’직접 접근’할 수 있는 구문 영역. ’직접 접근’이라는 건, unqualified reference(비-제한 참조라고 하면 좋을까?)로 네임스페이스 안의 이름을 찾을 수 있는 것을 말한다.\n\nunqualified reference? : someclass.target 처럼 ’나 어디 있소’라고 someclass. 를 앞에 붙이지 않고, 바로 target으로 이름을 찾는 참조법.\n\nLEGB : Python Scope 탐색 규칙\n\nLocal : 함수 안에 정의된 이름 중 Global로 정의되지 않은 것\nEnclosing-function : 함수를 내포하고 있는 함수(enclosing function)의 영역 안에 있는 것\nGlobal (module) : 모듈 파일의 가장 상위 레벨에서 정의되었거나(어디 클래스나 함수 안에서 정의된 것 아니고) 함수 내부에서 global키워드로 실행된 것\nBuilt-in (Python) : Python에서 기본으로 정의하고 있는 것\n\n\n아래 코드를 살펴보자.\n\nx1 = -1 # Global\nclass spam:    \n    x2 = -2 # class body\n    def ham(self, bar):\n        # enclosing function\n        x3 = -3\n        print(f'print global x1 : {x1}') # global에서 받아옴\n        def egg():\n            x4 = -4\n            x1 = 'local x1'\n            print(f'print local x1 : {x1}') # local에서 정의된 x1을 먼저 받아옴\n            print(f'print class body x2 : {self.x2}') # class body에 정의된 객체를 가져오려면 self 키워드 필요.\n            # 여기서 그냥 x2로 가져오려고 하면, local에도 없고 enclosing에도 없으니\n            # global에서 찾게 된다 : print(f'print x2 : {x2}') 그리고 못 찾아서 에러를 낸다.\n            print(f'print enclosing x3 : {x3}') # enclosing function 영역에서 받아옴\n            print(f'print abs of x4 : {abs(x4)}') # Built-in에서 print, abs를 찾는다. local에서 x4를 찾는다.\n            print(bar) # enclosing-function의 argument를 받는다.\n        egg()\n\nspam = spam()\nspam.ham('foo')\n\nprint global x1 : -1\nprint local x1 : local x1\nprint class body x2 : -2\nprint enclosing x3 : -3\nprint abs of x4 : 4\nfoo\n\n\nClass body 영역은 scope에서 enclosing-function도 아니고 global도 아닌 독특한 위치를 차지하고 있는데, 이 StackOverFlow 답변을 참고하자.\n다시 원점으로 돌아오면,\n\ndef print_line(func): # Decorator가 될 함수\n    def wrapper_mine():\n        print('-'*30) # func의 사전작업이라 할 수 있다.\n        func()\n        print('#'*30) # func의 사후작업이라 할 수 있다.\n    return wrapper_mine # wrapper_mine을 호출한 게 아니고, 함수 객체를 그냥 반환한 거다\n\n“wrapper_mine이 어떻게 print_line의 argument에 접근할 수 있지?” 도 해결되었다.\n\n\n\n\n말 그대로, 이걸 어디다 쓸까? 간단한 예제를 살펴보자.\n코드 출처 : https://khanrc.tistory.com/entry/decorator%EC%99%80-closure\n\ndef verbose(func): \n    def new_func(*args, **kwargs):\n        print(\"Begin\", func.__name__)\n        func(*args, **kwargs)\n        print(\"End\", func.__name__)\n    return new_func\n\n함수 호출의 시작과 끝을 print 출력으로 알리는 함수다. 아무 함수나 들어올 수 있게 parameter가 설정되어 있다.\n\n@verbose\ndef simple_sum(x,y):\n    print(x+y)\n    return x+y\n\nsimple_sum(1,2)\n\nBegin simple_sum\n3\nEnd simple_sum\n\n\n어떤 함수를 집어넣어도 호출 시에, 연산 끝났을 시에 함수 이름과 함께 알려주는 재미있는 Decorator다.\n\n\n\n이 글을 보다보니, 재미있는 예제를 찾을 수 있었다. Decorator가 중첩되어 있으면 어떤 것 부터 적용될까? 아래 코드를 보자.\n\ndef star(func):\n    def inner(*args, **kwargs):\n        print(\"*\" * 30)\n        func(*args, **kwargs)\n        print(\"*\" * 30)\n    return inner\n\ndef percent(func):\n    def inner(*args, **kwargs):\n        print(\"%\" * 30)\n        func(*args, **kwargs)\n        print(\"%\" * 30)\n    return inner\n\n@star\n@percent\ndef printer(msg):\n    print(msg)\nprinter(\"Hello\")\n\n******************************\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nHello\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n******************************\n\n\n더 위쪽에 적혀진 Decorator부터 적용됨을 알 수 있다.\n\n\n\nhttps://dbader.org/blog/python-first-class-functions\nhttps://docs.python.org/3/tutorial/classes.html\nhttps://stackoverflow.com/questions/291978/short-description-of-the-scoping-rules\nhttps://blog.mozilla.org/webdev/2011/01/31/python-scoping-understanding-legb/\nhttps://khanrc.tistory.com/entry/decorator%EC%99%80-closure\nhttps://www.programiz.com/python-programming/decorator"
  },
  {
    "objectID": "posts/2019-11-11-Asynchronous, Synchronous, Blocking, Non-Blocking.html",
    "href": "posts/2019-11-11-Asynchronous, Synchronous, Blocking, Non-Blocking.html",
    "title": "Lim's Code Archive",
    "section": "",
    "text": "많은 자료의 산 중에서, 가장 알기 쉽고 직관적으로 설명한 자료는 https://stackoverflow.com/questions/2625493/asynchronous-vs-non-blocking 이 질문의 세 번째 답변이라는 결론을 내렸다. 이 답변의 번역 + 보충 설명을 위한 다른 자료들 + 사족을 섞어서 정리하였다.\n\nsynchronous / asynchronous : 두 모듈 사이의 관계에 대한 표현\nblocking / non-blocking : 모듈 하나의 상태에 대한 표현\n예를 들어,\n\n모듈 X : 나\n모듈 Y : 서점\nX가 Y에게 질문 : C++ primer 책 있나요?\n\n\n1. Blocking\n\nY가 X에게 답하기 전까지, X는 기다린다. X는 Blocking 상태에 빠진 것이다.\n\n2. Non-Blocking\n\nY가 X에게 답하기 전에, X는 다른 일을 할 수 있다.\nX가 2분마다 Y가 일을 끝냈는지 확인할까?(Synchronous라면 이렇게 될 것 같다.) 아니면 Y가 다 됐다고 부르면 확인할까?(Asynchronous라면 이렇게 될 것 같다.) 모른다.(= 상관이 없다.)\n우리가 아는 건 X가 Y가 일을 끝내기 전에 다른 일을 할 수 있다는 것 뿐이다. X는 Non-Blocking이다.\n\n3. Synchronous\n\nY가 X에게 답하기 전에는, X는 다른 일을 진행하지 않는다 - 라고 설명하고 있는데, 이러면 Blocking과 정의가 같다. 좀 다르게 생각해 보자.\nSynchronous에는 중요한 두 가지 키워드가 있다.\n\n작업의 순서를 맞추는 것. 왜 순서를 맞추냐고? 여러 작업이 동시에(Concurrently) Critical section에 진입하는것을 막기 위해서이기도 하고, 특정 순서에 맞게 작업들을 실행해야 할 필요가 있기(표를 사지도 않고 비행기에 탈 수는 없다) 때문이기도 하다. 작업 순서를 맞출 때 Blocking으로 처리하면 편하기 때문에 Blocking의 개념이 섞여서 등장하는 것 뿐이다.\n\n작업 순서의 관점에서 설명하는 글 두 개\n\nhttps://jins-dev.tistory.com/entry/동기Synchronous-작업과-비동기Asynchronous-작업-그리고-블락Blocking-과-넌블락NonBlocking-의-개념 (이 글의 경우 blocking 부분은 보면 더 헷갈리니 위에만 보자.)\nhttps://medium.com/from-the-scratch/wtf-is-synchronous-and-asynchronous-1a75afd039df\n\n\nCaller가 Callee의 완료 상태를 확인하는 것. Callee의 완료 여부가 Caller의 다음 작업에 영향을 미치기 때문으로, 작업의 순서를 맞추는 것의 하위 개념이다.\n\n좋은 예시 : 상사가 와서 어떤 일을 처리하라고 말한다. 그리고 내 등 뒤에서 시체를 노리는 독수리마냥 나를 쳐다보고 있다. “자네가 일을 다 끝낼 때까지 여기서 기다릴 걸세.”\n상태 확인의 관점에서 설명하는 글 : https://homoefficio.github.io/2017/02/19/Blocking-NonBlocking-Synchronous-Asynchronous/\n\n\n이 X,Y 예에서는, Synchronous는 X가 Y에게 책 찾았냐고 물어보고,(caller가 callee의 상태 확인) Y가 책이 있는지 없는지 X에게 알려 준 이후에야, X가 Y에게 “그래서 이 책 얼마죠?” 라고 물어보던가, “그 책 주문 좀 해주세요” 라고 요청할 수 있는 상황인 것이다.(작업 순서) 책이 있는지 확인한 다음에 가격을 물어보던지 주문을 요청하던지 할 수 있으니까. X가 Y가 책 찾는 동안 다른 무언가를 하는 건 상관이 없다. 서점 앞에서 줄넘기를 할 수도 있지 않은가? 책에 관련된 다음 일을 못하는 것 뿐.\n이렇게 봐도 상당히 헷갈리기 때문에, 코드를 보자. 아래의 코드는 Synchronous & Non-Blocking 한 간단한 코드이다.\n\n# thread X\nwhile (true)\n{\n    msg = recv(Y, NON_BLOCKING_FLAG);\n    if (msg is not empty)\n    {\n        break;\n    }\n    # 이 루프 안에서 다른 작업을 할 수 있다.\n    sleep(2000); // 2 sec\n}\n\n# thread Y\n# prepare the book for X\nsend(X, book);\n\nX가 2초마다 Y가 답을 줬는지 아닌지 확인한다.\nY가 결과를 반환하기 위해 준비 중이어도, X의 while 루프는 계속 돌아가고 그 안에서 다른 작업을 진행할 수 있다. 그래서 Non-Blocking이다.\n하지만, while문을 빠져나가서 다른 작업을 할 수는 없다. 그래서 Synchronous다.\n코드의 예시까지 보면, X가 서점을 떠나지 못한다고 해석할 수 있다. Y가 책을 찾아줘서 서점과 관련된 일을 마치기 전에는, 서점을 떠나서 다른 걸 할 수 없다. 서점 앞에서 줄넘기는 가능해도.(while문 안에서 뭔가 다른 작업)\nBlocking이었다면, X는 아무것도 못하고 기다려야 했겠지만.\n\n4. Asychronous\n\nY가 X에게 답하기 전에, X는 다른 곳에 가서 다른 일을 할 수 있다. X는 Y가 부르기 전까지 돌아오지 않는다. 이 때 X와 Y는 Asychronous 하다고 말한다.\n여기서도 Synchronous와 같은 두 가지 키워드로 살펴보자.\n\n작업의 순서가 보장되지 않음 : Asynchronous는 엄밀히 말하면, 작업들이 공통적으로 사용하는 global clock이 없고, 신호나 메세지의 도착 시간이 작업의 신뢰성에 영향을 미치지 않음을 뜻한다. 즉 작업의 순서가 보장되지 않는다.(A,B,C 순서로 실행되었으나 완료도 A,B,C 순서일 것이라 보장할 수 없음)\n\n신호나 메세지의 도착 시간이 작업의 신뢰성에 영향을 미치지 않음, 즉, 각 작업이 서로 연관되지 않아서 분리될 수 있으며, 작업 지연시간이 큰 경우에 잘 활용될 수 있다. (DB 접근, Http 요청, File I/O 등)\n\nCallee가 자신의 완료 상태를 확인하며, callback으로 Caller에게 자신의 완료를 알림\n\n근데 사실 완료 통보를 해도 되고 안해도 된다. 완료 통보가 caller에게 의미가 있느냐 없느냐의 문제이다.(내가 한 질문이다 :D)\n좋은 예시 : 상사가 와서 어떤 일을 처리하라고 말한다. 그리고 다른 일 하러 가버림. 일을 다 끝내면, 나는 상사에게 “나 다함!” 이라고 말한다.\n\n\nY에게 책 있냐고 물어본 후에 X가 카페에 가서 커피를 마시기 시작했지만, 책 찾기보다 커피 마시기가 더 빨리 끝날 수도 있다. 이 둘은 완전히 별개의 작업이며, 작업의 순서가 보장되지 않는다.(Asynchronous & Non-Blocking이라면.) 그리고 X는 Y가 X를 부를 때 서점으로 돌아간다.\n\n각 2개씩의 개념이 있으니, 총 4개의 조합이 나올 것이다.\n\nSynchronous - Blocking\nAsynchronous - Blocking\nSynchronous - Non-Blocking\nAsynchronous - Non-Blocking 이 조합들에 대해서는 여기를 참고하자. 아래 사진이 핵심인데, 출처의 글에서 가져온 사진이다.\n\n 출처 : https://homoefficio.github.io/2017/02/19/Blocking-NonBlocking-Synchronous-Asynchronous/\n\n번외 내용 : 정리하다 보니 핵심 개념을 직관적으로 알기에는 너무 응용에 가깝다고 생각되었던 내용. 지우기는 아까워서 넘겨두었다.\n\nBlocking I/O : application이 kernal에 I/O 해줘~ 라고 system call을 날린다. kernal이 I/O를 수행하는 동안, application은 아무것도 못 하고 기다린다. I/O가 완료되면 call에 대한 return값으로 원하던 데이터를 받는다. \nNon-Blocking I/O : application이 kernal에 I/O 해줘~ 라고 system call을 날린다. 그림의 recvfrom 함수는, 바로 결과를 return 하는데, 아직 I/O가 완료되지 않았으므로 에러인 EWOULDBLOCK을 return한다. 프로세스는 계속 recvfrom을 call 하게 되고, 데이터가 완료되었으면 그 때 데이터가 return된다. 이렇게 계속 요청하는 걸 polling 이라 한다. \n\ncall에 대한 return을 바로 받아서, application이 제어권을 넘겨받고 다른 일을 진행할 수 있는 것이 중요하다. Blocking I/O와는 정 반대로.\n\nNon-Blocking Algorithm : 어떤 쓰레드의 실패(failure)나 멈춤(suspension)이 다른 쓰레드에 영향을 미치지 않게 하는 알고리즘. 몇몇 상황에서는 이런 알고리즘이 전통적인 Blocking 적용(Lock)의 유용한 대안이 된다.\n\n\n\n\n정확한 용어의 정의를 알아보려고 했다. 하지만 일반적으로는 아래의 대략적인 의미로 사용되는 듯하다. 나는 정확하게 쓰도록 노력해야겠다.\n\nAsynchronous Programming(비동기 프로그래밍): 하나의 요청을 시작한 후, 완료를 기다리지 않고 제어권을 다음 요청으로 넘기는 방식.(Non-Blocking의 의미를 포함)\nSynchronous Programming(동기 프로그래밍): 하나의 요청이 처리되는 동안 다른 요청이 처리되지 못하는 방식. 전 요청이 완료되어야 다음 요청 처리가 가능함.(Blocking의 의미를 포함) 참고\n\n\n\n\n\nhttps://en.wikipedia.org/wiki/Synchronization_(computer_science)#Thread_or_process_synchronization\nhttps://en.wikipedia.org/wiki/Synchronous_circuit\nhttps://en.wikipedia.org/wiki/Asynchronous_system\nhttps://jins-dev.tistory.com/entry/동기Synchronous-작업과-비동기Asynchronous-작업-그리고-블락Blocking-과-넌블락NonBlocking-의-개념\nhttps://medium.com/from-the-scratch/wtf-is-synchronous-and-asynchronous-1a75afd039df\nhttps://homoefficio.github.io/2017/02/19/Blocking-NonBlocking-Synchronous-Asynchronous/\nhttps://ozt88.tistory.com/20\nhttp://www.masterraghu.com/subjects/np/introduction/unix_network_programming_v1.3/ch06lev1sec2.html\nhttps://en.wikipedia.org/wiki/Blocking_(computing)\nhttps://developer.ibm.com/articles/l-async/"
  },
  {
    "objectID": "posts/2020-02-06-로컬 머신에서 AWS Redshift에 접근하기.html",
    "href": "posts/2020-02-06-로컬 머신에서 AWS Redshift에 접근하기.html",
    "title": "로컬 머신에서 AWS Redshift에 접근하기",
    "section": "",
    "text": "Redshift 공식 메뉴얼에서 명료하게 설명하지 않았거나, 없는 내용에 대한 정리이다."
  },
  {
    "objectID": "posts/2020-02-06-로컬 머신에서 AWS Redshift에 접근하기.html#virtual-private-cloudvpc-보안-그룹-설정",
    "href": "posts/2020-02-06-로컬 머신에서 AWS Redshift에 접근하기.html#virtual-private-cloudvpc-보안-그룹-설정",
    "title": "로컬 머신에서 AWS Redshift에 접근하기",
    "section": "1. Virtual Private Cloud(VPC) 보안 그룹 설정",
    "text": "1. Virtual Private Cloud(VPC) 보안 그룹 설정\n\nVPC의 보안 그룹 설정이 필요하다.\n클러스터 선택 -> 속성 -> 네트워크 및 보안 -> VPC 보안 그룹 메뉴로\nInbound 설정에서, 유형 Redshift, 연결을 원하는 머신의 IP를 Source로 설정하자."
  },
  {
    "objectID": "posts/2020-02-06-로컬 머신에서 AWS Redshift에 접근하기.html#공개적으로-액세스-할-수-있음-설정",
    "href": "posts/2020-02-06-로컬 머신에서 AWS Redshift에 접근하기.html#공개적으로-액세스-할-수-있음-설정",
    "title": "로컬 머신에서 AWS Redshift에 접근하기",
    "section": "2. “공개적으로 액세스 할 수 있음” 설정",
    "text": "2. “공개적으로 액세스 할 수 있음” 설정\n\n너무 간단한 내용이지만, 메뉴얼에 없었다…\n클러스터 선택 -> 속성 -> 네트워크 및 보안 -> 공개적으로 액세스 할 수 있음 메뉴에서 ’예’로 바꿔주면 된다."
  },
  {
    "objectID": "posts/2020-02-06-로컬 머신에서 AWS Redshift에 접근하기.html#tableplus에서-연결하기",
    "href": "posts/2020-02-06-로컬 머신에서 AWS Redshift에 접근하기.html#tableplus에서-연결하기",
    "title": "로컬 머신에서 AWS Redshift에 접근하기",
    "section": "3. TablePlus에서 연결하기",
    "text": "3. TablePlus에서 연결하기\n\nMac용 SQL 클라이언트 중에서는 TablePlus가 제일 좋은 것 같다.\n클러스터 Endpoint는 이런 구조다.\n\nCLUSTER-NAME.CLUSTER-KEY.CLUSTER-REGION.redshift.amazonaws.com:PORT/DATABASE-NAME\n\nHost에 입력할 값\n\nCLUSTER-NAME.CLUSTER-KEY.CLUSTER-REGION.redshift.amazonaws.com\n\nPort, User, Password, Database는 설정했던 값을 입력\n나머지 옵션은 조절하지 않아도 됨"
  },
  {
    "objectID": "posts/2020-02-06-로컬 머신에서 AWS Redshift에 접근하기.html#psycopg2에서-연결하기",
    "href": "posts/2020-02-06-로컬 머신에서 AWS Redshift에 접근하기.html#psycopg2에서-연결하기",
    "title": "로컬 머신에서 AWS Redshift에 접근하기",
    "section": "4. psycopg2에서 연결하기",
    "text": "4. psycopg2에서 연결하기\n\n간단해서 코드로 대신함\n\nimport psycopg2\ndbname='YOUR-DB-NAME' # 최초의 기본 db명은 dev\nhost='CLUSTER-NAME.CLUESTER-KEY.CLUESTER-REGION.redshift.amazonaws.com'\nport=5439\nuser='USER-NAME'\npassword='********'\ncon=psycopg2.connect(dbname=dbname, host=host, port=port, user=user, password=password)\ncur = con.cursor()\n\n매번 connect, cursor를 닫기 번거로우니 with 구문을 사용하자.\n\ndbname='YOUR-DB-NAME' # 최초의 기본 db명은 dev\nhost='CLUSTER-NAME.CLUESTER-KEY.CLUESTER-REGION.redshift.amazonaws.com'\nport=5439\nuser='USER-NAME'\npassword='********'\nconnect_param = dict({'dbname':dbname, 'host':host, 'port':port, 'user':user, 'password':password})\n\nwith psycopg2.connect(**connect_param) as con:\n    with con.cursor() as cur:\n        do something"
  },
  {
    "objectID": "posts/2020-02-11-Redshift에 데이터를 적재하는 과정에서 얻은 교훈 & psycopg2.html",
    "href": "posts/2020-02-11-Redshift에 데이터를 적재하는 과정에서 얻은 교훈 & psycopg2.html",
    "title": "Lim's Code Archive",
    "section": "",
    "text": "차라리 로컬 DB에서 연산한 후, S3에 올리고 COPY로 집어넣는 것이 훨씬 빠르다.\nRedshift에서 직접 SELECT, INSERT 처리를 하면 : 300만행 추가에 예상 완료시간 4일\n로컬 DB에서 연산 후 COPY로 업로드하면 : 300만 행 연산시간 3.5시간, COPY 업로드 시간 5분\nAWS DW 설명 문서의 말을 들었어야 했는데.\n\n# 같은 코드를 로컬 DB에서 돌리면 3.5시간, Redshift에 연결해서 돌리면 4일\n\nwith psycopg2.connect(**connect_param_local) as con:\n    cur_2 = con.cursor()\n    # fetchall을 사용해서 커서를 재활용하지 않고 커서를 두 개 두는 이유는, redshift의 single-node cluster에서는 fetchall이 지원되지 않기 떄문이다.\n        # InternalError_: Fetch ALL is not supported on single-node clusters.\n        # Please specify the fetch size (maximum 1000 for single-node clusters) \n        # or upgrade to a multi node installation.\n    # 로컬 머신에서 연산하면 fetchall을 사용할 수 있으므로 이렇게 안 해도 되지만,\n    # 코드 수정이 더 번거로웠으므로 그냥 사용하였다.\n    \n    get_companylist_sql = '''select * from target_company_list;'''\n    target_company_list = sqlio.read_sql_query(get_companylist_sql, con)\n    # 1mb도 안되는 작은 테이블이라서 데이터프레임으로 한 번에 받아옴\n\n    for each_code in target_company_list['stock_code']:\n        cur_1 = con.cursor('ss_cursor') # server side cursor\n        # cur_1.itersize = 1000 # redshift single-node cluster에서의 server side cursor의 최대 제한값\n        \n        print(each_code,'_start')\n        predict_start = target_company_list[target_company_list['stock_code'] == each_code]['pre_6m'].values[0] - datetime.timedelta(days=1)\n        predict_end = predict_start - datetime.timedelta(days=1096)\n\n        cur_1.execute(\n            \"\"\"\n            select * from stock_data_2000_2020_raw\n            where (date between %(predict_end)s and %(predict_start)s) and (stock_code = %(stock_code)s);\n            \"\"\",\n            {'predict_end':predict_end.strftime(\"%Y-%m-%d\"),'predict_start':predict_start.strftime(\"%Y-%m-%d\"),'stock_code': each_code}\n        )\n\n        for each_row in cur_1: \n            # next(cur_1)로 한줄씩 불러와서 insert\n            cur_2.execute(\"\"\"insert into stock_data_3years_raw_6m values %s\"\"\", [each_row])\n            # each_row는 tuple이다.\n        con.commit()\n        print(each_code,'_commit complete')\n    cur_2.close()\n\n\n\n\n\n\n\npsycopg2 document에는, 빨간색으로 엄청 잘 보이게 써 있는 경고문이 있다.\n\n\nWarning: Never, never, NEVER use Python string concatenation (+) or string parameters interpolation (%) to pass variables to a SQL query string. Not even at gunpoint.\n\n\nSQL Injection의 위험이 있기 때문인데, 어떻게 위험한지는 여기를 참고하자.\n그럼 어떻게 하라는 걸까?\n\n최종적으로 사용한 형식은 아래와 같다.\n\nimport psycopg2\nfrom psycopg2 import sql\n\ncredentials = 'aws_access_key_id=**************;aws_secret_access_key=**************'\ns3_bucket_param = 's3://BUCKET-NAME/FILE-NAME'\n\ncopy_query = sql.SQL(\"\"\"\n        copy {table_name}\n        from %(s3_bucket_param)s\n        credentials %(credentials)s\n        IGNOREHEADER 1\n        CSV;\n    \"\"\").format(table_name = sql.Identifier('TABLE-NAME'))\n\ncur.execute(copy_query, {'s3_bucket_param':s3_bucket_param, 'credentials':credentials})\n\n{} : 테이블 이름 등의 identifier를 받는다. %s 형식으로 identifier를 받으려고 하면, 제대로 인식이 안 되기 때문에 번거롭지만 .foramt(table_name = sql.Identifier('TABLE-NAME'))형식으로 인자를 넘겨야 한다. keyword parameter로 안 해도 되지만, 어떤 자리에 무엇이 들어가는 지 명확하게 정의하는 것을 좋아하므로 몽땅 keyword parameter로 진행하였다.\n\nsql 모듈 설명\n같은 문제로 고통받던 사람의 이슈제기\n\n%(keyword)s : value를 받는다. execute 함수 내부에서 인자로 전달하면 된다. keyword parameter로 정의했을 경우 dictionary로 전달하자.\n\n\n\n\n\n\nClient Side Cursor를 사용하면, 일단 데이터를 클라이언트의 메모리에 저장한 후 거기서 결과값을 계산하게 된다.\n엄청 큰 테이블의 일부를 select 하려고 하면 반드시 메모리 부족으로 문제가 생기게 된다.\nServer Side Cursor를 사용하면, 서버에서 연산 처리 후 결과값만 반환해주기 때문에, 클라이언트 메모리 문제에서 좀 자유로워진다.\n서버 리소스는 더 쓰게 되고, 네트워크 부하는 줄어들게 된다.\n\n# 편리하게도 커서에 이름만 지정해주면 된다!\ncur_1 = con.cursor('ss_cursor')\n\n\n\n\n\n\n\n작업 폴더 내에 있는 CSV 파일을 그대로 COPY하려고 하면, 무조건 permission error가 발생한다.\nDB 서버 사용자가 해당 파일에 접근할 권한이 없기 때문에 발생하는 문제로, 파일이나 폴더의 권한설정을 만져주면 해결된다. 그런데 권한 설정하는 것 보다는 DB 서버 사용자가 접근할 수 있는 폴더에 CSV 파일을 옮기는 것이 더 빠르지 않을까?\n\npostgreSQL을 Mac에서 Homebrew로 설치했다면, /usr/local/var/postgres\n예시\n\n\nwith psycopg2.connect(**connect_param_local) as con:\n    with con.cursor() as cur:\n        cur.execute(\n            \"\"\"\n            COPY stock_data_2000_2020_raw\n            from '/usr/local/var/postgres/stock_data_raw_2.csv'\n            DELIMITER ','\n            CSV HEADER;\n            \"\"\")\ncon.commit()"
  },
  {
    "objectID": "posts/2021-09-06-Linux_비밀번호 만료 안 되게 하기.html",
    "href": "posts/2021-09-06-Linux_비밀번호 만료 안 되게 하기.html",
    "title": "Linux_비밀번호 만료 안 되게 하기",
    "section": "",
    "text": "chage -E -1 -M 99999 계정명\n\n\n\n\nchage : 사용자의 패스워드 정보를 관리하는 명령어\n\n-E : 계정의 만료일 설정\n-l : 지정한 계정의 정보를 보여 줌\n-M : 패스워드 최종 변경일로부터 패스워드 변경 없이 사용할 수 있는 최대 일수를 설정\n\n-E에는 -1을 할당 : 영원히 계정을 만료시키지 않음\n-M에는 99999를 할당 : 패스워드 변경 이후 99999일 동안 변경 없이 사용 가능\n위의 명령어 입력 후, sudo chage -l 계정명 으로 계정/패스워드 정보를 확인해 보면 아래와 같다.\nLast password change              : Aug 23, 2021\nPassword expires                  : never\nPassword inactive                 : never\nAccount expires                       : never\nMinimum number of days between password change        : 5\nMaximum number of days between password change        : 99999\nNumber of days of warning before password expires : 7\n\n\n\n\n\n리눅스 패스워드 만료 안되게 하기 - 제타위키 (zetawiki.com)\n[Linux] chage 명령어 (사용자 패스워드 만기 정보 관리) (tistory.com)"
  },
  {
    "objectID": "posts/2021-09-07-Crontab으로 Python 스크립트 주기적으로 실행하기.html",
    "href": "posts/2021-09-07-Crontab으로 Python 스크립트 주기적으로 실행하기.html",
    "title": "Crontab으로 Python 스크립트 주기적으로 실행하기",
    "section": "",
    "text": "주기적으로 외부 API를 통해 데이터를 수집하여, Bigquery에 적재하고 싶다. CentOS 서버에서, 주기적으로 Python 스크립트를 실행하여 해결해 보자.\n\nsudo crontab -e : crontab 설정 오픈. 자동으로 root가 작업하는 것으로 인지됨\n설정\n\n경로는 절대경로를 입력해야 제대로 작동\n\nPython 경로도 절대경로로 입력해 줘야 함\n\n시간설정은 아래 링크에서 직관적으로 확인 가능\n\nCrontab.guru - The cron schedule expression editor\n\n\n\n30 8 * * * /usr/local/bin/python3.9 /home/limyj0708/cw_daily_bigquery/cw_daily.py\n\ncron 재시작\n\n재시작해야 적용됨\nservice cron restart\nCentOS일 경우, service crond restart"
  },
  {
    "objectID": "posts/2021-11-05-pandas_cheatsheet.html",
    "href": "posts/2021-11-05-pandas_cheatsheet.html",
    "title": "Pandas CheatSheet",
    "section": "",
    "text": "” 안 쓰면 잊어버리는, pandas에서의 주요 Dataframe 조작 방법들을 정리”"
  },
  {
    "objectID": "posts/2021-11-05-pandas_cheatsheet.html#dataframe-생성",
    "href": "posts/2021-11-05-pandas_cheatsheet.html#dataframe-생성",
    "title": "Pandas CheatSheet",
    "section": "1. Dataframe 생성",
    "text": "1. Dataframe 생성\n\n1-1. Dictionary에서 Dataframe 생성\n\ndata = {'col_1': [3, 2, 1, 0], 'col_2': ['a', 'b', 'c', 'd']}\npd.DataFrame.from_dict(data)\n# key가 컬럼, value로 들어간 리스트가 컬럼의 row 하나하나가 된다.\n\n\n\n\n\n  \n    \n      \n      col_1\n      col_2\n    \n  \n  \n    \n      0\n      3\n      a\n    \n    \n      1\n      2\n      b\n    \n    \n      2\n      1\n      c\n    \n    \n      3\n      0\n      d\n    \n  \n\n\n\n\n\ndict_list = [\n    { \"id\" : 1001001, \"address\" : \"AABCC\"}\n    ,{ \"id\" : 2101001, \"address\" : \"BBBDD\"}\n    ,{ \"id\" : 3201001, \"address\" : \"백두산\"}\n    ,{ \"id\" : 4301001, \"address\" : \"한라산\"}\n    ,{ \"id\" : 5401001, \"address\" : \"몰디브\"}\n] # 같은 key들을 가진 딕셔너리들이 담긴 리스트\npd.DataFrame.from_dict(dict_list) # 이렇게 넣어도, key들이 컬럼이 되어 데이터프레임이 만들어진다.\n# 실무적으로는 이 형태를 더 많이 쓰게 된다.\n\n\n\n\n\n  \n    \n      \n      id\n      address\n    \n  \n  \n    \n      0\n      1001001\n      AABCC\n    \n    \n      1\n      2101001\n      BBBDD\n    \n    \n      2\n      3201001\n      백두산\n    \n    \n      3\n      4301001\n      한라산\n    \n    \n      4\n      5401001\n      몰디브\n    \n  \n\n\n\n\n\n\n1-2. Column만 존재하는 빈 Dataframe을 만들고, 내용 채워 넣기\n\ndf = pd.DataFrame(columns=['A','B','BB','C','D'])\n# 컬럼들이 될 리스트를 columns parameter에 argument로 넘김\ndf\n\n\n\n\n\n  \n    \n      \n      A\n      B\n      BB\n      C\n      D\n    \n  \n  \n  \n\n\n\n\n\ndf['A'] = [1,3,1]\ndf\n\n\n\n\n\n  \n    \n      \n      A\n      B\n      BB\n      C\n      D\n    \n  \n  \n    \n      0\n      1\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      1\n      3\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2\n      1\n      NaN\n      NaN\n      NaN\n      NaN\n    \n  \n\n\n\n\n\ndf['B'] = [4,4,6]\ndf\n\n\n\n\n\n  \n    \n      \n      A\n      B\n      BB\n      C\n      D\n    \n  \n  \n    \n      0\n      1\n      4\n      NaN\n      NaN\n      NaN\n    \n    \n      1\n      3\n      4\n      NaN\n      NaN\n      NaN\n    \n    \n      2\n      1\n      6\n      NaN\n      NaN\n      NaN\n    \n  \n\n\n\n\n\ndf.loc[((df['A'] == 1) & (df['B'] == 4)), 'C'] = 444\ndf\n# 컬럼 값 조건을 걸고 값을 변경\n\n\n\n\n\n  \n    \n      \n      A\n      B\n      BB\n      C\n      D\n    \n  \n  \n    \n      0\n      1\n      4\n      NaN\n      444\n      NaN\n    \n    \n      1\n      3\n      4\n      NaN\n      NaN\n      NaN\n    \n    \n      2\n      1\n      6\n      NaN\n      NaN\n      NaN\n    \n  \n\n\n\n\n\ndf.loc[(df['B'] == 4), 'C'] = 0\ndf\n# 컬럼 값 조건을 걸고 값을 변경 2\n\n\n\n\n\n  \n    \n      \n      A\n      B\n      BB\n      C\n      D\n    \n  \n  \n    \n      0\n      1\n      4\n      NaN\n      0\n      NaN\n    \n    \n      1\n      3\n      4\n      NaN\n      0\n      NaN\n    \n    \n      2\n      1\n      6\n      NaN\n      NaN\n      NaN\n    \n  \n\n\n\n\n\nsample_list = [1,2,3,4,5]\n# 해당 데이터프레임 가장 아래에 리스트를 row로 넣음\ndf.loc[len(df)] = sample_list\n# 이 방식은 좀 느린 편이며, 데이터프레임에 행을 추가해야 한다면\n# 자료를 dictionary로 관리하다가 모든 데이터 추가가 다 끝나고 데이터프레임으로 변환하는 것이 빠름\ndf\n\n\n\n\n\n  \n    \n      \n      A\n      B\n      BB\n      C\n      D\n    \n  \n  \n    \n      0\n      1\n      4\n      NaN\n      0\n      NaN\n    \n    \n      1\n      3\n      4\n      NaN\n      0\n      NaN\n    \n    \n      2\n      1\n      6\n      NaN\n      NaN\n      NaN\n    \n    \n      3\n      1\n      2\n      3\n      4\n      5"
  },
  {
    "objectID": "posts/2021-11-05-pandas_cheatsheet.html#indexing-값-변경-추가",
    "href": "posts/2021-11-05-pandas_cheatsheet.html#indexing-값-변경-추가",
    "title": "Pandas CheatSheet",
    "section": "2. Indexing, 값 변경 & 추가",
    "text": "2. Indexing, 값 변경 & 추가\n\n2-1. loc : 라벨 인덱싱\n\nprint(type(df.loc[0]))\ndf.loc[0]\n# loc의 첫 번째 인자는 '행 라벨' 이다.\n# 그래서 0을 넣으면, index가 0인 행을 series로 반환하고 있다.\n\n<class 'pandas.core.series.Series'>\n\n\nA       1\nB       4\nBB    NaN\nC       0\nD     NaN\nName: 0, dtype: object\n\n\n\nprint(type(df.loc[0, 'A']))\ndf.loc[0, 'A']\n# 두 번째 인자는 컬럼명이다.\n\n<class 'numpy.int64'>\n\n\n1\n\n\n\ndf.loc[[0,1,2,3], ['A','B']]\n# 이런 식으로 접근하면, 다중 컬럼과 행을 데이터프레임으로 가져올 수 있다.\n\n\n\n\n\n  \n    \n      \n      A\n      B\n    \n  \n  \n    \n      0\n      1\n      4\n    \n    \n      1\n      3\n      4\n    \n    \n      2\n      1\n      6\n    \n    \n      3\n      1\n      2\n    \n  \n\n\n\n\n\ndf.loc[df.index[0:3], ['A','B']]\n# df.index로도 접근 가능\n\n\n\n\n\n  \n    \n      \n      A\n      B\n    \n  \n  \n    \n      0\n      1\n      4\n    \n    \n      1\n      3\n      4\n    \n    \n      2\n      1\n      6\n    \n  \n\n\n\n\n\ndf.loc[df['B'] == 4]\n# row에 값 조건을 걸 수도 있다.\n\n\n\n\n\n  \n    \n      \n      A\n      B\n      BB\n      C\n      D\n    \n  \n  \n    \n      0\n      1\n      4\n      NaN\n      0\n      NaN\n    \n    \n      1\n      3\n      4\n      NaN\n      0\n      NaN\n    \n  \n\n\n\n\n\ndf.loc[df['B'] == 4, df.columns.str.contains('B')]\n# 컬럼 이름에도 조건을 걸 수 있다. 위의 경우, 컬럼 이름에 B를 포함하는 컬럼만 가져옴.\n\n\n\n\n\n  \n    \n      \n      B\n      BB\n    \n  \n  \n    \n      0\n      4\n      NaN\n    \n    \n      1\n      4\n      NaN\n    \n  \n\n\n\n\n\ndf.loc[:,df.columns.str.contains('B')]\n# 행 조건 자리에 :를 넣으면, 행에 대해서는 전체를 다 가져오라는 뜻이다.\n\n\n\n\n\n  \n    \n      \n      B\n      BB\n    \n  \n  \n    \n      0\n      4\n      NaN\n    \n    \n      1\n      4\n      NaN\n    \n    \n      2\n      6\n      NaN\n    \n    \n      3\n      2\n      3\n    \n  \n\n\n\n\n\nprint(df.columns) # 컬럼명을 가져옴\nprint(df.columns.str)\nprint(df.columns.str.contains('B')) # boolean indexing이 가능한 형태가 된다.\nprint(type(df.columns.str.contains('B'))) # 결과물은 false와 true가 들어간 ndarray\nprint(df.columns.str.startswith('A')) # 이렇게 하면 A로 시작하는 컬럼을 가져올 수 있음\n# 결론은, 다른 외부 함수를 사용해서 어쩄든 boolean 타입 값이 담긴 리스트를 만들면, loc에 넣어서 boolean indexing이 가능하다는 것.\n\nIndex(['A', 'B', 'BB', 'C', 'D'], dtype='object')\n<pandas.core.strings.StringMethods object at 0x000001E1FEC0B250>\n[False  True  True False False]\n<class 'numpy.ndarray'>\n[ True False False False False]\n\n\n\ndf.loc[:,'new'] = 3\ndf\n# loc으로도 기존에 없던 새 컬럼을 추가할 수 있음\n\n\n\n\n\n  \n    \n      \n      A\n      B\n      BB\n      C\n      D\n      new\n    \n  \n  \n    \n      0\n      1\n      4\n      NaN\n      0\n      NaN\n      3\n    \n    \n      1\n      3\n      4\n      NaN\n      0\n      NaN\n      3\n    \n    \n      2\n      1\n      6\n      NaN\n      NaN\n      NaN\n      3\n    \n    \n      3\n      1\n      2\n      3\n      4\n      5\n      3\n    \n  \n\n\n\n\n\ndf.loc[4] = [1] * len(df.columns)\ndf.loc[99] = [1] * len(df.columns)\ndf.loc['cool'] = [22] * len(df.columns)\n# dataframe에 행을 추가함. index가 늘어난다.\ndf\n\n\n\n\n\n  \n    \n      \n      A\n      B\n      BB\n      C\n      D\n      new\n    \n  \n  \n    \n      0\n      1\n      4\n      NaN\n      0\n      NaN\n      3\n    \n    \n      1\n      3\n      4\n      NaN\n      0\n      NaN\n      3\n    \n    \n      2\n      1\n      6\n      NaN\n      NaN\n      NaN\n      3\n    \n    \n      3\n      1\n      2\n      3\n      4\n      5\n      3\n    \n    \n      4\n      1\n      1\n      1\n      1\n      1\n      1\n    \n    \n      99\n      1\n      1\n      1\n      1\n      1\n      1\n    \n    \n      cool\n      22\n      22\n      22\n      22\n      22\n      22\n    \n  \n\n\n\n\n\n\n2-2. iloc : 위치 인덱싱\n\ndf.iloc[0:2,0:4]\n# 기본적 동작은 loc과 동일하나, 받는 인자가 라벨이 아니고 '위치'다.\n\n\n\n\n\n  \n    \n      \n      A\n      B\n      BB\n      C\n    \n  \n  \n    \n      0\n      1\n      4\n      NaN\n      0\n    \n    \n      1\n      3\n      4\n      NaN\n      0\n    \n  \n\n\n\n\n\ndf.iloc[4:7,0:4]\n# 위치를 받기 때문에, index는 99여도 5번째 줄로 인식됨\n\n\n\n\n\n  \n    \n      \n      A\n      B\n      BB\n      C\n    \n  \n  \n    \n      4\n      1\n      1\n      1\n      1\n    \n    \n      99\n      1\n      1\n      1\n      1\n    \n    \n      cool\n      22\n      22\n      22\n      22\n    \n  \n\n\n\n\n\ndf.iloc[7] = [2] * len(df.columns)\n# IndexError: iloc cannot enlarge its target object\n# 위치를 인자로 받기 때문에, 새로운 컬럼, 행을 만든다거나 하는 행위는 불가능하다.\n\nIndexError: iloc cannot enlarge its target object\n\n\n\n\n2-3. at : 스칼라값 접근\n\ndf.at[1,'A']\n# 한 번에 1개의 스칼라값에만 접근 가능\n# 여러 개의 값에 접근하려고 범위를 지정하면, 에러를 출력한다.\n# 단일 값에 접근하는 목적이라면 loc보다 훨씬 빠름\n\n3\n\n\n\ndf.at[1,'A'] = 100\ndf\n# 값을 딱 하나만 바꾸고 싶다! 라고 하면 at을 활용해보자.\n\n\n\n\n\n  \n    \n      \n      A\n      B\n      BB\n      C\n      D\n      new\n    \n  \n  \n    \n      0\n      1\n      4\n      NaN\n      0\n      NaN\n      3\n    \n    \n      1\n      100\n      4\n      NaN\n      0\n      NaN\n      3\n    \n    \n      2\n      1\n      6\n      NaN\n      NaN\n      NaN\n      3\n    \n    \n      3\n      1\n      2\n      3\n      4\n      5\n      3\n    \n    \n      4\n      1\n      1\n      1\n      1\n      1\n      1\n    \n    \n      99\n      1\n      1\n      1\n      1\n      1\n      1\n    \n    \n      cool\n      22\n      22\n      22\n      22\n      22\n      22\n    \n  \n\n\n\n\n\ndf.at[99, 'new']\n# 그 이외에는 label base인 것이 loc과 똑같음\n\n1\n\n\n\n\n2-4. iat : iloc의 스칼라 버전\n\ndf.iat[4,2]\n# iloc의 스칼라 버전.\n# 이외의 동작은 at과 같다.\n\n1\n\n\n\ndf.iat[df.index.get_loc('cool'),df.columns.get_loc('new')]\n# get_loc을 쓰면, 해당 인덱스와 컬럼의 위치를 반환받을 수 있음.\n# 그럼 인덱스와 컬럼의 이름으로도 iat, iloc을 이용 가능\n\n22\n\n\n\n\n2-5 map : Series의 원소 하나하나에 함수 적용\n\nmap함수는 DataFrame 타입이 아니라, 반드시 Series 타입에서만 사용해야 한다.\nSeries를 한마디로 정의하면 딱 이거다.\n\n값(value) + 인덱스(index) = 시리즈 클래스(Series)\n\nSeries는 NumPy에서 제공하는 1차원 배열과 비슷하지만 각 데이터의 의미를 표시하는 인덱스(index)를 붙일 수 있다. 하지만 데이터 자체는 그냥 값(value)의 1차원 배열이다.\nmap함수는 Series의 이러한 값 하나하나에 접근하면서 해당 함수를 수행한다.\n\n\nimport math as m # sqrt 함수 사용을 위해 부름\n# http://www.leejungmin.org/post/2018/04/21/pandas_apply_and_map/\ndf[\"map_b\"] = df[\"B\"].map(lambda x : m.sqrt(x)) \n# B컬럼의 값 하나하나에 sqrt 함수를 적용한 결과를 map_b 컬럼으로 추가\ndf\n\n\n\n\n\n  \n    \n      \n      A\n      B\n      BB\n      C\n      D\n      new\n      map_b\n    \n  \n  \n    \n      0\n      1\n      4\n      NaN\n      0\n      NaN\n      3\n      2.000000\n    \n    \n      1\n      100\n      4\n      NaN\n      0\n      NaN\n      3\n      2.000000\n    \n    \n      2\n      1\n      6\n      NaN\n      NaN\n      NaN\n      3\n      2.449490\n    \n    \n      3\n      1\n      2\n      3\n      4\n      5\n      3\n      1.414214\n    \n    \n      4\n      1\n      1\n      1\n      1\n      1\n      1\n      1.000000\n    \n    \n      99\n      1\n      1\n      1\n      1\n      1\n      1\n      1.000000\n    \n    \n      cool\n      22\n      22\n      22\n      22\n      22\n      22\n      4.690416\n    \n  \n\n\n\n\n\n\n2-6. apply : 커스텀 함수에 복수 개의 컬럼이 필요하다면\n\n커스텀 함수를 사용하기 위해 DataFrame에서 복수 개의 컬럼이 필요하다면, apply 함수를 사용해야 한다.\n\n\nimport math as m # sqrt 함수 사용을 위해 부름\n# 두 컬럼의 제곱근의 값을 각각 곱하는 함수\ndef sqrt_multi(x,y):\n    return m.sqrt(x) * m.sqrt(y)\n\n\ndf.loc[:,'new'] = df.apply(lambda x : sqrt_multi(x['A'], x['B']), axis=1) # axis=1 이면 각 열의 원소에 대해 연산 수행\ndf\n\n\n\n\n\n  \n    \n      \n      A\n      B\n      BB\n      C\n      D\n      new\n      map_b\n    \n  \n  \n    \n      0\n      1\n      4\n      NaN\n      0\n      NaN\n      2.000000\n      2.000000\n    \n    \n      1\n      100\n      4\n      NaN\n      0\n      NaN\n      20.000000\n      2.000000\n    \n    \n      2\n      1\n      6\n      NaN\n      NaN\n      NaN\n      2.449490\n      2.449490\n    \n    \n      3\n      1\n      2\n      3\n      4\n      5\n      1.414214\n      1.414214\n    \n    \n      4\n      1\n      1\n      1\n      1\n      1\n      1.000000\n      1.000000\n    \n    \n      99\n      1\n      1\n      1\n      1\n      1\n      1.000000\n      1.000000\n    \n    \n      cool\n      22\n      22\n      22\n      22\n      22\n      22.000000\n      4.690416\n    \n  \n\n\n\n\n\ndf[\"apply_bb_d\"] = df.apply(lambda x : sqrt_multi(x['BB'], x['B']), axis=1) # axis=1 이면 각 열의 원소에 대해 연산 수행\ndf # NaN과의 연산은 NaN이 됨을 참고하자.\n\n\n\n\n\n  \n    \n      \n      A\n      B\n      BB\n      C\n      D\n      new\n      map_b\n      apply_bb_d\n    \n  \n  \n    \n      0\n      1\n      4\n      NaN\n      0\n      NaN\n      2.000000\n      2.000000\n      NaN\n    \n    \n      1\n      100\n      4\n      NaN\n      0\n      NaN\n      20.000000\n      2.000000\n      NaN\n    \n    \n      2\n      1\n      6\n      NaN\n      NaN\n      NaN\n      2.449490\n      2.449490\n      NaN\n    \n    \n      3\n      1\n      2\n      3\n      4\n      5\n      1.414214\n      1.414214\n      2.44949\n    \n    \n      4\n      1\n      1\n      1\n      1\n      1\n      1.000000\n      1.000000\n      1.00000\n    \n    \n      99\n      1\n      1\n      1\n      1\n      1\n      1.000000\n      1.000000\n      1.00000\n    \n    \n      cool\n      22\n      22\n      22\n      22\n      22\n      22.000000\n      4.690416\n      22.00000\n    \n  \n\n\n\n\n\n\n2-7. index, columns로 index와 컬럼명 직접 지정\n\nprint(df.columns)\nprint(type(df.columns))\nprint(df.index)\nprint(type(df.index))\nprint(df.columns[2]) # 위치값으로 개별 요소에 접근 가능\nprint(df.index[6])\n\nIndex(['A', 'B', 'BB', 'C', 'D', 'new', 'map_b', 'apply_bb_d'], dtype='object')\n<class 'pandas.core.indexes.base.Index'>\nIndex([0, 1, 2, 3, 4, 99, 'cool'], dtype='object')\n<class 'pandas.core.indexes.base.Index'>\nBB\ncool\n\n\n\ndf.columns = ['가', '나', '다', '라', '마', '바', '사', '아'] \n# df.columns에 직접 컬럼명 리스트를 할당하여 컬럼명 변경 가능\n# 기존 컬럼 수와 같은 길이의 리스트를 넣지 않으면 오류가 발생함\nprint(df.columns)\nprint(type(df.columns))\n\nIndex(['가', '나', '다', '라', '마', '바', '사', '아'], dtype='object')\n<class 'pandas.core.indexes.base.Index'>\n\n\n\ndf.index = [1,2,3,4,5,6,7] \n# df.columns에 직접 컬럼명 리스트를 할당하여 컬럼명 변경 가능\nprint(df.index)\nprint(type(df.index))\n\nInt64Index([1, 2, 3, 4, 5, 6, 7], dtype='int64')\n<class 'pandas.core.indexes.numeric.Int64Index'>\n\n\n\n\n2-8. set_index로 index 설정\nDataFrame.set_index(keys, drop=True, append=False, inplace=False)\n\nkeys에는 index로 할당하고자 하는 열의 레이블을 입력한다.\n\nmulti-index를 하고 싶으면, [‘가’, ‘나’] 이렇게 열 레이블 배열을 입력한다.\n\ndrop : index로 할당한 열을 삭제할까요?\nappend : 기존에 존재하던 index를 삭제할까요?\ninplace : 원본 데이터프레임을 변경할까요?\n\n\ndf\n\n\n\n\n\n  \n    \n      \n      가\n      나\n      다\n      라\n      마\n      바\n      사\n      아\n    \n  \n  \n    \n      1\n      1\n      4\n      NaN\n      0\n      NaN\n      2.000000\n      2.000000\n      NaN\n    \n    \n      2\n      100\n      4\n      NaN\n      0\n      NaN\n      20.000000\n      2.000000\n      NaN\n    \n    \n      3\n      1\n      6\n      NaN\n      NaN\n      NaN\n      2.449490\n      2.449490\n      NaN\n    \n    \n      4\n      1\n      2\n      3\n      4\n      5\n      1.414214\n      1.414214\n      2.44949\n    \n    \n      5\n      1\n      1\n      1\n      1\n      1\n      1.000000\n      1.000000\n      1.00000\n    \n    \n      6\n      1\n      1\n      1\n      1\n      1\n      1.000000\n      1.000000\n      1.00000\n    \n    \n      7\n      22\n      22\n      22\n      22\n      22\n      22.000000\n      4.690416\n      22.00000\n    \n  \n\n\n\n\n\ndf.set_index('가') # 기본값\n\n\n\n\n\n  \n    \n      \n      나\n      다\n      라\n      마\n      바\n      사\n      아\n    \n    \n      가\n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1\n      4\n      NaN\n      0\n      NaN\n      2.000000\n      2.000000\n      NaN\n    \n    \n      100\n      4\n      NaN\n      0\n      NaN\n      20.000000\n      2.000000\n      NaN\n    \n    \n      1\n      6\n      NaN\n      NaN\n      NaN\n      2.449490\n      2.449490\n      NaN\n    \n    \n      1\n      2\n      3\n      4\n      5\n      1.414214\n      1.414214\n      2.44949\n    \n    \n      1\n      1\n      1\n      1\n      1\n      1.000000\n      1.000000\n      1.00000\n    \n    \n      1\n      1\n      1\n      1\n      1\n      1.000000\n      1.000000\n      1.00000\n    \n    \n      22\n      22\n      22\n      22\n      22\n      22.000000\n      4.690416\n      22.00000\n    \n  \n\n\n\n\n\ndf.set_index('가', drop=False) # index로 선택된 열 삭제 안 함\n\n\n\n\n\n  \n    \n      \n      가\n      나\n      다\n      라\n      마\n      바\n      사\n      아\n    \n    \n      가\n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1\n      1\n      4\n      NaN\n      0\n      NaN\n      2.000000\n      2.000000\n      NaN\n    \n    \n      100\n      100\n      4\n      NaN\n      0\n      NaN\n      20.000000\n      2.000000\n      NaN\n    \n    \n      1\n      1\n      6\n      NaN\n      NaN\n      NaN\n      2.449490\n      2.449490\n      NaN\n    \n    \n      1\n      1\n      2\n      3\n      4\n      5\n      1.414214\n      1.414214\n      2.44949\n    \n    \n      1\n      1\n      1\n      1\n      1\n      1\n      1.000000\n      1.000000\n      1.00000\n    \n    \n      1\n      1\n      1\n      1\n      1\n      1\n      1.000000\n      1.000000\n      1.00000\n    \n    \n      22\n      22\n      22\n      22\n      22\n      22\n      22.000000\n      4.690416\n      22.00000\n    \n  \n\n\n\n\n\ndf.set_index('가', append=True) # 기존 index 삭제 안 함\n\n\n\n\n\n  \n    \n      \n      \n      나\n      다\n      라\n      마\n      바\n      사\n      아\n    \n    \n      \n      가\n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1\n      1\n      4\n      NaN\n      0\n      NaN\n      2.000000\n      2.000000\n      NaN\n    \n    \n      2\n      100\n      4\n      NaN\n      0\n      NaN\n      20.000000\n      2.000000\n      NaN\n    \n    \n      3\n      1\n      6\n      NaN\n      NaN\n      NaN\n      2.449490\n      2.449490\n      NaN\n    \n    \n      4\n      1\n      2\n      3\n      4\n      5\n      1.414214\n      1.414214\n      2.44949\n    \n    \n      5\n      1\n      1\n      1\n      1\n      1\n      1.000000\n      1.000000\n      1.00000\n    \n    \n      6\n      1\n      1\n      1\n      1\n      1\n      1.000000\n      1.000000\n      1.00000\n    \n    \n      7\n      22\n      22\n      22\n      22\n      22\n      22.000000\n      4.690416\n      22.00000\n    \n  \n\n\n\n\n\ndf.set_index(['가','나']) # 동시에 여러 열을 index로 설정하기\n\n\n\n\n\n  \n    \n      \n      \n      다\n      라\n      마\n      바\n      사\n      아\n    \n    \n      가\n      나\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1\n      4\n      NaN\n      0\n      NaN\n      2.000000\n      2.000000\n      NaN\n    \n    \n      100\n      4\n      NaN\n      0\n      NaN\n      20.000000\n      2.000000\n      NaN\n    \n    \n      1\n      6\n      NaN\n      NaN\n      NaN\n      2.449490\n      2.449490\n      NaN\n    \n    \n      2\n      3\n      4\n      5\n      1.414214\n      1.414214\n      2.44949\n    \n    \n      1\n      1\n      1\n      1\n      1.000000\n      1.000000\n      1.00000\n    \n    \n      1\n      1\n      1\n      1\n      1.000000\n      1.000000\n      1.00000\n    \n    \n      22\n      22\n      22\n      22\n      22\n      22.000000\n      4.690416\n      22.00000\n    \n  \n\n\n\n\n\n\n2-9. reset_index로 index 초기화\nDataFrame.reset_index(drop=False, inplace=False)\n\n기존에 있던 index 대신에, 0부터 시작하여 1씩 늘어나는 정수 index를 추가한다.\ndrop : 기존에 index였던 열을 삭제할까요?\ninplace : 원본 데이터프레임을 변경할까요?\n\n\ndf.reset_index()\n\n\n\n\n\n  \n    \n      \n      index\n      가\n      나\n      다\n      라\n      마\n      바\n      사\n      아\n    \n  \n  \n    \n      0\n      1\n      1\n      4\n      NaN\n      0\n      NaN\n      2.000000\n      2.000000\n      NaN\n    \n    \n      1\n      2\n      100\n      4\n      NaN\n      0\n      NaN\n      20.000000\n      2.000000\n      NaN\n    \n    \n      2\n      3\n      1\n      6\n      NaN\n      NaN\n      NaN\n      2.449490\n      2.449490\n      NaN\n    \n    \n      3\n      4\n      1\n      2\n      3\n      4\n      5\n      1.414214\n      1.414214\n      2.44949\n    \n    \n      4\n      5\n      1\n      1\n      1\n      1\n      1\n      1.000000\n      1.000000\n      1.00000\n    \n    \n      5\n      6\n      1\n      1\n      1\n      1\n      1\n      1.000000\n      1.000000\n      1.00000\n    \n    \n      6\n      7\n      22\n      22\n      22\n      22\n      22\n      22.000000\n      4.690416\n      22.00000\n    \n  \n\n\n\n\n\ndf.reset_index(drop=True)\n\n\n\n\n\n  \n    \n      \n      가\n      나\n      다\n      라\n      마\n      바\n      사\n      아\n    \n  \n  \n    \n      0\n      1\n      4\n      NaN\n      0\n      NaN\n      2.000000\n      2.000000\n      NaN\n    \n    \n      1\n      100\n      4\n      NaN\n      0\n      NaN\n      20.000000\n      2.000000\n      NaN\n    \n    \n      2\n      1\n      6\n      NaN\n      NaN\n      NaN\n      2.449490\n      2.449490\n      NaN\n    \n    \n      3\n      1\n      2\n      3\n      4\n      5\n      1.414214\n      1.414214\n      2.44949\n    \n    \n      4\n      1\n      1\n      1\n      1\n      1\n      1.000000\n      1.000000\n      1.00000\n    \n    \n      5\n      1\n      1\n      1\n      1\n      1\n      1.000000\n      1.000000\n      1.00000\n    \n    \n      6\n      22\n      22\n      22\n      22\n      22\n      22.000000\n      4.690416\n      22.00000"
  },
  {
    "objectID": "posts/2021-11-05-pandas_cheatsheet.html#값-삭제-대체",
    "href": "posts/2021-11-05-pandas_cheatsheet.html#값-삭제-대체",
    "title": "Pandas CheatSheet",
    "section": "3. 값 삭제, 대체",
    "text": "3. 값 삭제, 대체\n\n특정 조건의 값을 삭제하고 싶은 경우에는, 해당 조건의 반대 조건을 걸어서 반환 결과를 사용하는 식으로 처리한다.\n\n\n3-1. drop : 원하는 행, 열 지우기\nDataFrame.drop(labels=None, axis=0, index=None, columns=None, level=None, inplace=False, errors='raise')\n\n특정 레이블의 행이나 열을 제거한다.\nlabels : 제거할 index, 레이블 하나 혹은 리스트 (list-like)\naxis : 0이면 행, 1이면 컬럼 대상\nindex : labels, axis=0 대신 사용가능\ncolumns : labels, axis=1 대신 사용가능\nlevel : MultiIndex일 경우, 어떤 레벨을 제거할 것인지\ninplace : 원본 변경 할 건가요?\nerrors : ’ignore’로 세팅하면, 에러 출력 안 하고 존재하는 레이블만 제거한다.\n\n\ndf\n\n\n\n\n\n  \n    \n      \n      가\n      나\n      다\n      라\n      마\n      바\n      사\n      아\n    \n  \n  \n    \n      1\n      1\n      4\n      NaN\n      0\n      NaN\n      2.000000\n      2.000000\n      NaN\n    \n    \n      2\n      100\n      4\n      NaN\n      0\n      NaN\n      20.000000\n      2.000000\n      NaN\n    \n    \n      3\n      1\n      6\n      NaN\n      NaN\n      NaN\n      2.449490\n      2.449490\n      NaN\n    \n    \n      4\n      1\n      2\n      3\n      4\n      5\n      1.414214\n      1.414214\n      2.44949\n    \n    \n      5\n      1\n      1\n      1\n      1\n      1\n      1.000000\n      1.000000\n      1.00000\n    \n    \n      6\n      1\n      1\n      1\n      1\n      1\n      1.000000\n      1.000000\n      1.00000\n    \n    \n      7\n      22\n      22\n      22\n      22\n      22\n      22.000000\n      4.690416\n      22.00000\n    \n  \n\n\n\n\n\ndf.drop(labels=['가','아'], axis=1)\n\n\n\n\n\n  \n    \n      \n      나\n      다\n      라\n      마\n      바\n      사\n    \n  \n  \n    \n      1\n      4\n      NaN\n      0\n      NaN\n      2.000000\n      2.000000\n    \n    \n      2\n      4\n      NaN\n      0\n      NaN\n      20.000000\n      2.000000\n    \n    \n      3\n      6\n      NaN\n      NaN\n      NaN\n      2.449490\n      2.449490\n    \n    \n      4\n      2\n      3\n      4\n      5\n      1.414214\n      1.414214\n    \n    \n      5\n      1\n      1\n      1\n      1\n      1.000000\n      1.000000\n    \n    \n      6\n      1\n      1\n      1\n      1\n      1.000000\n      1.000000\n    \n    \n      7\n      22\n      22\n      22\n      22\n      22.000000\n      4.690416\n    \n  \n\n\n\n\n\ndf.drop(labels=[1,7], axis=0)\n\n\n\n\n\n  \n    \n      \n      가\n      나\n      다\n      라\n      마\n      바\n      사\n      아\n    \n  \n  \n    \n      2\n      100\n      4\n      NaN\n      0\n      NaN\n      20.000000\n      2.000000\n      NaN\n    \n    \n      3\n      1\n      6\n      NaN\n      NaN\n      NaN\n      2.449490\n      2.449490\n      NaN\n    \n    \n      4\n      1\n      2\n      3\n      4\n      5\n      1.414214\n      1.414214\n      2.44949\n    \n    \n      5\n      1\n      1\n      1\n      1\n      1\n      1.000000\n      1.000000\n      1.00000\n    \n    \n      6\n      1\n      1\n      1\n      1\n      1\n      1.000000\n      1.000000\n      1.00000\n    \n  \n\n\n\n\n\ndf.drop(columns=['가','아'])\n# df.drop(labels=['가','아'], axis=1)와 같은 결과\n\n\n\n\n\n  \n    \n      \n      나\n      다\n      라\n      마\n      바\n      사\n    \n  \n  \n    \n      1\n      4\n      NaN\n      0\n      NaN\n      2.000000\n      2.000000\n    \n    \n      2\n      4\n      NaN\n      0\n      NaN\n      20.000000\n      2.000000\n    \n    \n      3\n      6\n      NaN\n      NaN\n      NaN\n      2.449490\n      2.449490\n    \n    \n      4\n      2\n      3\n      4\n      5\n      1.414214\n      1.414214\n    \n    \n      5\n      1\n      1\n      1\n      1\n      1.000000\n      1.000000\n    \n    \n      6\n      1\n      1\n      1\n      1\n      1.000000\n      1.000000\n    \n    \n      7\n      22\n      22\n      22\n      22\n      22.000000\n      4.690416\n    \n  \n\n\n\n\n\n\n3-2. Na 대응\n\n3-2-1. isna : NaN인지 각 값에 대해 확인\n\nNaN인지 각 값에 대해 확인하여 boolean으로 표현\nisnull() 도 완전히 같은 기능을 한다.\n왜 같은 기능을 하는 함수가 두 개나 있는지는 아래 링크를 참조\n\nhttps://datascience.stackexchange.com/questions/37878/difference-between-isna-and-isnull-in-pandas\nThis is because pandas’ DataFrames are based on R’s DataFrames. In R na and null are two separate things. Read this post for more information. However, in python, pandas is built on top of numpy, which has neither na nor null values. Instead numpy has NaN values (which stands for “Not a Number”). Consequently, pandas also uses NaN values.\n\n완전히 반대의 기능을 하는 함수로 notnull()이 있다.\n\npandas.notnull(obj)\n\n\n\ndf.isna()\n# 특정 컬럼, 행에 대해서도 사용 가능\n\n\n\n\n\n  \n    \n      \n      가\n      나\n      다\n      라\n      마\n      바\n      사\n      아\n    \n  \n  \n    \n      1\n      False\n      False\n      True\n      False\n      True\n      False\n      False\n      True\n    \n    \n      2\n      False\n      False\n      True\n      False\n      True\n      False\n      False\n      True\n    \n    \n      3\n      False\n      False\n      True\n      True\n      True\n      False\n      False\n      True\n    \n    \n      4\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n    \n    \n      5\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n    \n    \n      6\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n    \n    \n      7\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n    \n  \n\n\n\n\n\n\n3-2-2. dropna : NA 드랍\nDataFrame.dropna(axis=0, how='any', thresh=None, subset=None, inplace=False)\n\naxis\n\n0 혹은 ‘index’ : missing value가 있는 행을 드랍\n1 혹은 ‘columns’ : missing value가 있는 열을 드랍\n\nhow\n\nany : missing value가 하나라도 있으면 드랍\nall : 전체 값이 다 missing value여야 드랍\n\nthresh : 문턱값. 정수를 입력 시, 정상값이 해당 정수 갯수만큼은 있어야 제거 안 함\nsubset : list-like 오브젝트를 넣으면, 해당 index나 컬럼에서만 missing value 체크\ninplace : 원본 변경 할 건가요?\n\n\ndf\n\n\n\n\n\n  \n    \n      \n      가\n      나\n      다\n      라\n      마\n      바\n      사\n      아\n    \n  \n  \n    \n      1\n      1\n      4\n      NaN\n      0\n      NaN\n      2.000000\n      2.000000\n      NaN\n    \n    \n      2\n      100\n      4\n      NaN\n      0\n      NaN\n      20.000000\n      2.000000\n      NaN\n    \n    \n      3\n      1\n      6\n      NaN\n      NaN\n      NaN\n      2.449490\n      2.449490\n      NaN\n    \n    \n      4\n      1\n      2\n      3\n      4\n      5\n      1.414214\n      1.414214\n      2.44949\n    \n    \n      5\n      1\n      1\n      1\n      1\n      1\n      1.000000\n      1.000000\n      1.00000\n    \n    \n      6\n      1\n      1\n      1\n      1\n      1\n      1.000000\n      1.000000\n      1.00000\n    \n    \n      7\n      22\n      22\n      22\n      22\n      22\n      22.000000\n      4.690416\n      22.00000\n    \n  \n\n\n\n\n\ndf.dropna() # 기본적으로 행 드랍\n\n\n\n\n\n  \n    \n      \n      가\n      나\n      다\n      라\n      마\n      바\n      사\n      아\n    \n  \n  \n    \n      4\n      1\n      2\n      3\n      4\n      5\n      1.414214\n      1.414214\n      2.44949\n    \n    \n      5\n      1\n      1\n      1\n      1\n      1\n      1.000000\n      1.000000\n      1.00000\n    \n    \n      6\n      1\n      1\n      1\n      1\n      1\n      1.000000\n      1.000000\n      1.00000\n    \n    \n      7\n      22\n      22\n      22\n      22\n      22\n      22.000000\n      4.690416\n      22.00000\n    \n  \n\n\n\n\n\ndf.dropna(axis=1) # 열 드랍\n\n\n\n\n\n  \n    \n      \n      가\n      나\n      바\n      사\n    \n  \n  \n    \n      1\n      1\n      4\n      2.000000\n      2.000000\n    \n    \n      2\n      100\n      4\n      20.000000\n      2.000000\n    \n    \n      3\n      1\n      6\n      2.449490\n      2.449490\n    \n    \n      4\n      1\n      2\n      1.414214\n      1.414214\n    \n    \n      5\n      1\n      1\n      1.000000\n      1.000000\n    \n    \n      6\n      1\n      1\n      1.000000\n      1.000000\n    \n    \n      7\n      22\n      22\n      22.000000\n      4.690416\n    \n  \n\n\n\n\n\ndf.dropna(thresh=5) # index 3인 행은 정상값이 4개였음\n\n\n\n\n\n  \n    \n      \n      가\n      나\n      다\n      라\n      마\n      바\n      사\n      아\n    \n  \n  \n    \n      1\n      1\n      4\n      NaN\n      0\n      NaN\n      2.000000\n      2.000000\n      NaN\n    \n    \n      2\n      100\n      4\n      NaN\n      0\n      NaN\n      20.000000\n      2.000000\n      NaN\n    \n    \n      4\n      1\n      2\n      3\n      4\n      5\n      1.414214\n      1.414214\n      2.44949\n    \n    \n      5\n      1\n      1\n      1\n      1\n      1\n      1.000000\n      1.000000\n      1.00000\n    \n    \n      6\n      1\n      1\n      1\n      1\n      1\n      1.000000\n      1.000000\n      1.00000\n    \n    \n      7\n      22\n      22\n      22\n      22\n      22\n      22.000000\n      4.690416\n      22.00000\n    \n  \n\n\n\n\n\ndf.dropna(axis=0, subset=['라']) # '라'열만 검사해서 NaN이 있는 행을 제거함\n\n\n\n\n\n  \n    \n      \n      가\n      나\n      다\n      라\n      마\n      바\n      사\n      아\n    \n  \n  \n    \n      1\n      1\n      4\n      NaN\n      0\n      NaN\n      2.000000\n      2.000000\n      NaN\n    \n    \n      2\n      100\n      4\n      NaN\n      0\n      NaN\n      20.000000\n      2.000000\n      NaN\n    \n    \n      4\n      1\n      2\n      3\n      4\n      5\n      1.414214\n      1.414214\n      2.44949\n    \n    \n      5\n      1\n      1\n      1\n      1\n      1\n      1.000000\n      1.000000\n      1.00000\n    \n    \n      6\n      1\n      1\n      1\n      1\n      1\n      1.000000\n      1.000000\n      1.00000\n    \n    \n      7\n      22\n      22\n      22\n      22\n      22\n      22.000000\n      4.690416\n      22.00000\n    \n  \n\n\n\n\n\n\n3-2-3. fillna : NaN 데이터 대체하기\nDataFrame.fillna(value=None, method=None, axis=None, inplace=False, limit=None, downcast=None)\n\nvalue : NaN을 무엇으로 채울 것인가?\n\nscalar : 0, 1 따위의 값을 넣음\ndict : {“A”: 0, “B”: 1, “C”: 2, “D”: 3}\n\n컬럼 A의 NaN은 0으로, 컬럼 B의 NaN은 1로, 컬럼 C의 NaN은 2로, 컬럼 D의 NaN은 3으로 대체\n\ndataframe : 대체 대상 dataframe와 같은 크기의 dataframe을 준비한 후, value에 dataframe을 넣으면 NaN 값만 넣은 dataframe의 값으로 대체된다. 컬럼명이나 인덱스는 원본 dataframe의 것이 유지된다.\n\nmethod : 어떤 방법으로 채울까? (value와 같이 사용할 수 없음)\n\nbackfill, bfill : NaN의 다음 값으로 NaN 채우기.\nffill, pad : NaN의 직전 값으로 NaN 채우기.\n\naxis\n\n0 혹은 ‘index’\n1 혹은 ‘columns’\n\ninplace : 원본 변경 할 건가요?\nlimit : 위에서부터 NaN 몇 개만 바꿀래? 기본값 None이면 모든 NaN을 바꾸는 것.\n\n\ndf = pd.DataFrame([[np.nan, 2, np.nan, 0],\n                   [3, 4, np.nan, 1],\n                   [np.nan, np.nan, np.nan, 5],\n                   [np.nan, 3, np.nan, 4]],\n                  columns=list(\"ABCD\"))\ndf\n\n\n\n\n\n  \n    \n      \n      A\n      B\n      C\n      D\n    \n  \n  \n    \n      0\n      NaN\n      2.0\n      NaN\n      0\n    \n    \n      1\n      3.0\n      4.0\n      NaN\n      1\n    \n    \n      2\n      NaN\n      NaN\n      NaN\n      5\n    \n    \n      3\n      NaN\n      3.0\n      NaN\n      4\n    \n  \n\n\n\n\n\ndf.fillna(value=0) # 0으로 NaN 채우기\n\n\n\n\n\n  \n    \n      \n      A\n      B\n      C\n      D\n    \n  \n  \n    \n      0\n      0.0\n      2.0\n      0.0\n      0\n    \n    \n      1\n      3.0\n      4.0\n      0.0\n      1\n    \n    \n      2\n      0.0\n      0.0\n      0.0\n      5\n    \n    \n      3\n      0.0\n      3.0\n      0.0\n      4\n    \n  \n\n\n\n\n\ndf.fillna(method='ffill') # NaN의 직전 값으로 NaN 채우기. 'pad'를 써도 마찬가지\n\n\n\n\n\n  \n    \n      \n      A\n      B\n      C\n      D\n    \n  \n  \n    \n      0\n      NaN\n      2.0\n      NaN\n      0\n    \n    \n      1\n      3.0\n      4.0\n      NaN\n      1\n    \n    \n      2\n      3.0\n      4.0\n      NaN\n      5\n    \n    \n      3\n      3.0\n      3.0\n      NaN\n      4\n    \n  \n\n\n\n\n\ndf.fillna(method='bfill') # NaN의 다음 값으로 NaN 채우기. 'backfill'을 써도 마찬가지\n\n\n\n\n\n  \n    \n      \n      A\n      B\n      C\n      D\n    \n  \n  \n    \n      0\n      3.0\n      2.0\n      NaN\n      0\n    \n    \n      1\n      3.0\n      4.0\n      NaN\n      1\n    \n    \n      2\n      NaN\n      3.0\n      NaN\n      5\n    \n    \n      3\n      NaN\n      3.0\n      NaN\n      4\n    \n  \n\n\n\n\n\nvalues = {\"A\": 0, \"B\": 1, \"C\": 2, \"D\": 3}\ndf.fillna(value=values) # values에 dictionary를 넣어서 컬럼마다 NaN을 다른 값으로 대체\n\n\n\n\n\n  \n    \n      \n      A\n      B\n      C\n      D\n    \n  \n  \n    \n      0\n      0.0\n      2.0\n      2.0\n      0\n    \n    \n      1\n      3.0\n      4.0\n      2.0\n      1\n    \n    \n      2\n      0.0\n      1.0\n      2.0\n      5\n    \n    \n      3\n      0.0\n      3.0\n      2.0\n      4\n    \n  \n\n\n\n\n\ndf.fillna(value=values, limit=1) # limit=1이어서, 최초의 NaN 하나만 대체\n\n\n\n\n\n  \n    \n      \n      A\n      B\n      C\n      D\n    \n  \n  \n    \n      0\n      0.0\n      2.0\n      2.0\n      0\n    \n    \n      1\n      3.0\n      4.0\n      NaN\n      1\n    \n    \n      2\n      NaN\n      1.0\n      NaN\n      5\n    \n    \n      3\n      NaN\n      3.0\n      NaN\n      4\n    \n  \n\n\n\n\n\ndf2 = pd.DataFrame(np.zeros((4, 4)), columns=list(\"ABCE\"))\ndf2 # 4 by 4 영행렬을 만들어 보자\n\n\n\n\n\n  \n    \n      \n      A\n      B\n      C\n      E\n    \n  \n  \n    \n      0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      1\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      2\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      3\n      0.0\n      0.0\n      0.0\n      0.0\n    \n  \n\n\n\n\n\ndf.fillna(df2) #원본 df의 컬럼이 유지됨\n\n\n\n\n\n  \n    \n      \n      A\n      B\n      C\n      D\n    \n  \n  \n    \n      0\n      0.0\n      2.0\n      0.0\n      0\n    \n    \n      1\n      3.0\n      4.0\n      0.0\n      1\n    \n    \n      2\n      0.0\n      0.0\n      0.0\n      5\n    \n    \n      3\n      0.0\n      3.0\n      0.0\n      4\n    \n  \n\n\n\n\n\ndf.loc[:,'A'].fillna(df.loc[:,'A'].mean()) # A열의 NaN 값을 A열의 평균으로 채움\n\n0    3.0\n1    3.0\n2    3.0\n3    3.0\nName: A, dtype: float64\n\n\n\n\n\n3-3. drop_duplicates : 중복값 제거\nDataFrame.drop_duplicates(subset=None, keep='first', inplace=False, ignore_index=False)\n\nsubset : 컬럼 라벨, 혹은 컬럼 라벨 리스트\n\n넣은 특정 컬럼만 중복값을 체크함. 기본으로는 전체 컬럼의 값이 다 같아야 제거\n\nkeep\n\nfirst : 첫 번째 등장한 것을 제외하면 다 제거\nlast : 마지막에 등장한 것을 제외하면 다 제거\nFalse : 몽땅 다 제거\n\ninplace : 원본 변경 할 건가요?\nignore_index : True 값을 넣으면, 결과값의 인덱스를 0, 1, … n-1로 라벨링함\n\n\ndf = pd.DataFrame({\n    'brand': ['Yum Yum', 'Yum Yum', 'Indomie', 'Indomie', 'Indomie'],\n    'style': ['cup', 'cup', 'cup', 'pack', 'pack'],\n    'rating': [4, 4, 3.5, 15, 5]\n})\ndf\n\n\n\n\n\n  \n    \n      \n      brand\n      style\n      rating\n    \n  \n  \n    \n      0\n      Yum Yum\n      cup\n      4.0\n    \n    \n      1\n      Yum Yum\n      cup\n      4.0\n    \n    \n      2\n      Indomie\n      cup\n      3.5\n    \n    \n      3\n      Indomie\n      pack\n      15.0\n    \n    \n      4\n      Indomie\n      pack\n      5.0\n    \n  \n\n\n\n\n\ndf.drop_duplicates() # 모든 열의 값이 다 같으면 제거\n\n\n\n\n\n  \n    \n      \n      brand\n      style\n      rating\n    \n  \n  \n    \n      0\n      Yum Yum\n      cup\n      4.0\n    \n    \n      2\n      Indomie\n      cup\n      3.5\n    \n    \n      3\n      Indomie\n      pack\n      15.0\n    \n    \n      4\n      Indomie\n      pack\n      5.0\n    \n  \n\n\n\n\n\ndf.drop_duplicates(subset=['brand']) # brand 컬럼 하나에서만 값이 같아도 제거\n\n\n\n\n\n  \n    \n      \n      brand\n      style\n      rating\n    \n  \n  \n    \n      0\n      Yum Yum\n      cup\n      4.0\n    \n    \n      2\n      Indomie\n      cup\n      3.5\n    \n  \n\n\n\n\n\ndf.drop_duplicates(subset=['brand', 'style'], keep='last')\n# brand, style 모두 같으면, 마지막 값만 남김\n\n\n\n\n\n  \n    \n      \n      brand\n      style\n      rating\n    \n  \n  \n    \n      1\n      Yum Yum\n      cup\n      4.0\n    \n    \n      2\n      Indomie\n      cup\n      3.5\n    \n    \n      4\n      Indomie\n      pack\n      5.0"
  },
  {
    "objectID": "posts/2021-11-05-pandas_cheatsheet.html#dataframe-결합",
    "href": "posts/2021-11-05-pandas_cheatsheet.html#dataframe-결합",
    "title": "Pandas CheatSheet",
    "section": "4. Dataframe 결합",
    "text": "4. Dataframe 결합\n\n4-1. 위아래로 붙이는 단순 결합의 경우, 어떤 방식이 가장 빠른가?\n\n결론부터 말하자면 데이터를 Dictionary의 리스트로 관리하다가 마지막에 Dataframe으로 만드는 것이 가장 빠르다.\n\nhttps://stackoverflow.com/questions/57000903/what-is-the-fastest-and-most-efficient-way-to-append-rows-to-a-dataframe\n\n  start_time = time.time()\n  dictinary_list = []\n  for i in range(0, end_value, 1):\n      dictionary_data = {k: random.random() for k in range(30)}\n      dictionary_list.append(dictionary_data)\n\n  df_final = pd.DataFrame.from_dict(dictionary_list)\n\n  end_time = time.time()\n  print('Execution time = %.6f seconds' % (end_time-start_time))\n\n그럼 리스트 합치는 건 뭐가 제일 빠르지?\n\nhttps://www.realpythonproject.com/day15-the-fastest-way-to-combine-lists-in-python/\nappend() is the fastest but it doesn’t combine the elements of both the lists. The + operator seems to be the ideal option. However, this has been done on a comparatively smaller dataset and results may vary when you try it on your own.\n\n\n인생은 항상 원하는대로 흘러가지 않기에, 다른 방법도 알아보자.\n\n\n4-1-1. concat\npandas.concat(objs, axis=0, join='outer', ignore_index=False, keys=None, levels=None, names=None, verify_integrity=False, sort=False, copy=True)\n\ns1 = pd.Series(['a', 'b'])\ns2 = pd.Series(['c', 'd'])\npd.concat([s1, s2]) # Series 두 개 합치기\n\n0    a\n1    b\n0    c\n1    d\ndtype: object\n\n\n\npd.concat([s1, s2], ignore_index=True) # 합치면서 index 새로 만들어줌\n\n0    a\n1    b\n2    c\n3    d\ndtype: object\n\n\n\ns3 = pd.concat([s1, s2], keys=['s1', 's2']) # 최외각 레벨에 새로운 index를 만들어줌\nprint(s3)\nprint(s3['s1']) \nprint(s3['s2'][0]) # 이렇게 조회가능\n\ns1  0    a\n    1    b\ns2  0    c\n    1    d\ndtype: object\n0    a\n1    b\ndtype: object\nc\n\n\n\ns3 = pd.concat([s1, s2], keys=['s1', 's2'], names=['Series name', 'Row ID'])\n# index에 이름 붙이기\nprint(s3)\nprint(s3.index)\nprint(s3.index.names)\n\nSeries name  Row ID\ns1           0         a\n             1         b\ns2           0         c\n             1         d\ndtype: object\nMultiIndex([('s1', 0),\n            ('s1', 1),\n            ('s2', 0),\n            ('s2', 1)],\n           names=['Series name', 'Row ID'])\n['Series name', 'Row ID']\n\n\n\ndf1 = pd.DataFrame([['a', 1], ['b', 2]], columns=['letter', 'number'])\nprint(df1)\ndf2 = pd.DataFrame([['c', 3], ['d', 4]], columns=['letter', 'number'])\nprint(df2)\npd.concat([df1, df2]) # Dataframe 합치기\n\n  letter  number\n0      a       1\n1      b       2\n  letter  number\n0      c       3\n1      d       4\n\n\n\n\n\n\n  \n    \n      \n      letter\n      number\n    \n  \n  \n    \n      0\n      a\n      1\n    \n    \n      1\n      b\n      2\n    \n    \n      0\n      c\n      3\n    \n    \n      1\n      d\n      4\n    \n  \n\n\n\n\n\ndf3 = pd.DataFrame([['c', 3, 'cat'], ['d', 4, 'dog']],\n                   columns=['letter', 'number', 'animal'])\nprint(df3)\npd.concat([df1, df3], sort=False) # 한 쪽에 없는 컬럼의 값은 NaN으로 삽입됨\n\n  letter  number animal\n0      c       3    cat\n1      d       4    dog\n\n\n\n\n\n\n  \n    \n      \n      letter\n      number\n      animal\n    \n  \n  \n    \n      0\n      a\n      1\n      NaN\n    \n    \n      1\n      b\n      2\n      NaN\n    \n    \n      0\n      c\n      3\n      cat\n    \n    \n      1\n      d\n      4\n      dog\n    \n  \n\n\n\n\n\npd.concat([df1, df3], join=\"inner\") # join=\"inner\"로 하면 양쪽에 다 있는 컬럼만 합쳐서 반환함\n\n\n\n\n\n  \n    \n      \n      letter\n      number\n    \n  \n  \n    \n      0\n      a\n      1\n    \n    \n      1\n      b\n      2\n    \n    \n      0\n      c\n      3\n    \n    \n      1\n      d\n      4\n    \n  \n\n\n\n\n\ndf4 = pd.DataFrame([['bird', 'polly'], ['monkey', 'george']],\n                   columns=['animal', 'name'])\npd.concat([df1, df4], axis=1) # axis=1이면 컬럼을 붙임\n\n\n\n\n\n  \n    \n      \n      letter\n      number\n      animal\n      name\n    \n  \n  \n    \n      0\n      a\n      1\n      bird\n      polly\n    \n    \n      1\n      b\n      2\n      monkey\n      george\n    \n  \n\n\n\n\n\ndf4 = pd.DataFrame([['bird', 'polly'], ['monkey', 'george'], ['dog', 'sam']],\n                   columns=['animal', 'name'])\npd.concat([df1, df4], axis=1) \n# axis=1이면 컬럼을 붙임\n# 행의 수가 다르면, 행이 적은 쪽에 NaN이 삽입된 행이 추가됨\n\n\n\n\n\n  \n    \n      \n      letter\n      number\n      animal\n      name\n    \n  \n  \n    \n      0\n      a\n      1.0\n      bird\n      polly\n    \n    \n      1\n      b\n      2.0\n      monkey\n      george\n    \n    \n      2\n      NaN\n      NaN\n      dog\n      sam\n    \n  \n\n\n\n\n\n#collapse-output\ndf5 = pd.DataFrame([1], index=['a'])\ndf6 = pd.DataFrame([2], index=['a'])\npd.concat([df5, df6], verify_integrity=True)\n# verify_integrity=True를 하면, index가 같은 것을 허용하지 않음.\n\nValueError: Indexes have overlapping values: Index(['a'], dtype='object')\n\n\n\n\n\n4-2. Merge : Database의 Join처럼 Dataframe 합치기\nDataFrame.merge(right, how='inner', on=None, left_on=None, right_on=None, left_index=False, right_index=False, sort=False, suffixes=('_x', '_y'), copy=True, indicator=False, validate=None)\n\nright : 합칠 Dataframe\nhow : {‘left’, ‘right’, ‘outer’, ‘inner’, ‘cross’}, default ‘inner’\n\nleft: use only keys from left frame, similar to a SQL left outer join; preserve key order.\nright: use only keys from right frame, similar to a SQL right outer join; preserve key order.\nouter: use union of keys from both frames, similar to a SQL full outer join; sort keys lexicographically.\ninner: use intersection of keys from both frames, similar to a SQL inner join; preserve the order of the left keys.\ncross: creates the cartesian product from both frames, preserves the order of the left keys.\n\non : label or list\n\n조인할 컬럼이나 인덱스 레벨의 이름. 두 Dataframe에 무조건 있어야 한다.\n\nleft_on : label or list, or array-like\n\n왼쪽 Dataframe의 조인할 컬럼이나 인덱스 레벨의 이름.\n\nright_on : label or list, or array-like\n\n오른쪽 Dataframe의 조인할 컬럼이나 인덱스 레벨의 이름.\n\nleft_index : bool, default False\n\n왼쪽 index를 조인의 key로 사용할까요?\nMultiIndex인 경우, 상대 Dataframe의 key 수가 level의 수와 동일해야 함.\n\nright_index : bool, default False\n\n오른쪽 index를 조인의 key로 사용할까요?\nMultiIndex인 경우, 상대 Dataframe의 key 수가 level의 수와 동일해야 함.\n\nsort : bool, default False\n\n조인 결과 Dataframe에서 key를 사전 순서(lexicographically)로 배열함\nFalse인 경우, 조인 방법에 정의된 방법을 따라감\n\nsuffixes : list-like, default is (“_x”, “_y”)\n\n컬럼에 접미사를 붙인다. 왼쪽 오른쪽 구분용.\n기본적으로 왼쪽에 _x, 오른쪽에 _y가 붙는다.\n\ncopy : bool, default True\n\nFalse면, 가능하면 복사를 피한다.\n\nindicator : bool or str, default False\nvalidate : str, optional\n\nIf specified, checks if merge is of specified type.\n\n“one_to_one” or “1:1”: check if merge keys are unique in both left and right datasets.\n“one_to_many” or “1:m”: check if merge keys are unique in left dataset.\n“many_to_one” or “m:1”: check if merge keys are unique in right dataset.\n“many_to_many” or “m:m”: allowed, but does not result in checks.\n\n\n\n\ndf1 = pd.DataFrame({'lkey': ['foo', 'bar', 'baz', 'foo'], 'value': [1, 2, 3, 5]})\ndf2 = pd.DataFrame({'rkey': ['foo', 'bar', 'baz', 'foo'], 'value': [5, 6, 7, 8]})\ndisplay(df1)\ndisplay(df2)\n\n\n\n\n\n  \n    \n      \n      lkey\n      value\n    \n  \n  \n    \n      0\n      foo\n      1\n    \n    \n      1\n      bar\n      2\n    \n    \n      2\n      baz\n      3\n    \n    \n      3\n      foo\n      5\n    \n  \n\n\n\n\n\n\n\n\n  \n    \n      \n      rkey\n      value\n    \n  \n  \n    \n      0\n      foo\n      5\n    \n    \n      1\n      bar\n      6\n    \n    \n      2\n      baz\n      7\n    \n    \n      3\n      foo\n      8\n    \n  \n\n\n\n\n\ndisplay(df1.merge(df2, left_on='lkey', right_on='rkey'))\n# 뒤에 _x, _y가 붙은 것을 확인.\ndf1.merge(df2, left_on='lkey', right_on='rkey', suffixes=('_left', '_right'))\n# _left, _right로 바꿔 보았음\n\n\n\n\n\n  \n    \n      \n      lkey\n      value_x\n      rkey\n      value_y\n    \n  \n  \n    \n      0\n      foo\n      1\n      foo\n      5\n    \n    \n      1\n      foo\n      1\n      foo\n      8\n    \n    \n      2\n      foo\n      5\n      foo\n      5\n    \n    \n      3\n      foo\n      5\n      foo\n      8\n    \n    \n      4\n      bar\n      2\n      bar\n      6\n    \n    \n      5\n      baz\n      3\n      baz\n      7\n    \n  \n\n\n\n\n\n\n\n\n  \n    \n      \n      lkey\n      value_left\n      rkey\n      value_right\n    \n  \n  \n    \n      0\n      foo\n      1\n      foo\n      5\n    \n    \n      1\n      foo\n      1\n      foo\n      8\n    \n    \n      2\n      foo\n      5\n      foo\n      5\n    \n    \n      3\n      foo\n      5\n      foo\n      8\n    \n    \n      4\n      bar\n      2\n      bar\n      6\n    \n    \n      5\n      baz\n      3\n      baz\n      7\n    \n  \n\n\n\n\n\ndf1 = pd.DataFrame({'a': ['foo', 'bar'], 'b': [1, 2]})\ndf2 = pd.DataFrame({'a': ['foo', 'baz'], 'c': [3, 4]})\ndisplay(df1)\ndisplay(df2)\n\n\n\n\n\n  \n    \n      \n      a\n      b\n    \n  \n  \n    \n      0\n      foo\n      1\n    \n    \n      1\n      bar\n      2\n    \n  \n\n\n\n\n\n\n\n\n  \n    \n      \n      a\n      c\n    \n  \n  \n    \n      0\n      foo\n      3\n    \n    \n      1\n      baz\n      4\n    \n  \n\n\n\n\n\ndf1.merge(df2, how='inner', on='a')\n# a 컬럼을 키로 잡음. 두 Dataframe에 다 a 컬럼이 있어서 가능한것\n# inner join\n\n\n\n\n\n  \n    \n      \n      a\n      b\n      c\n    \n  \n  \n    \n      0\n      foo\n      1\n      3\n    \n  \n\n\n\n\n\ndf1.merge(df2, how='left', on='a')\n# left outer join\n\n\n\n\n\n  \n    \n      \n      a\n      b\n      c\n    \n  \n  \n    \n      0\n      foo\n      1\n      3.0\n    \n    \n      1\n      bar\n      2\n      NaN\n    \n  \n\n\n\n\n\ndf1.merge(df2, how='left', left_on='a', right_on='a')\n# left outer join\n\n\n\n\n\n  \n    \n      \n      a\n      b\n      c\n    \n  \n  \n    \n      0\n      foo\n      1\n      3.0\n    \n    \n      1\n      bar\n      2\n      NaN\n    \n  \n\n\n\n\n\n4-2-1. Dataframe Join의 속도를 향상시키기 위해서는?\n\nhttps://stackoverflow.com/questions/40860457/improve-pandas-merge-performance\nkey를 index로 사용한다.\n\nindex 검색 시에는 hash table을 이용하기 때문\nA short explanation why it is faster to merge by index instead of by a “normal” column: Indices have a hash table. Meaning you can look them up in amortized O(1). For a normal column you need O(n) in worst case, meaning merging two dfs with len n takes O(n^2) in worst case.\n\njoin을 쓴다.\nconcat을 쓴다.\n\n여기서의 결론 : key를 index로 사용한 후 join을 쓴다.\n\nimport random\ndf1 = pd.DataFrame({'uid_sample': random.sample(range(100000), 80000), 'value': random.sample(range(10000000), 80000)})\ndf2 = pd.DataFrame({'userId_sample2': random.sample(range(100000), 80000), 'value': random.sample(range(10000000), 80000)})\n# 80000명의 정보를 담고 있는 두 Dataframe이 있다고 하자.\n# uid_sample, userId_sample2를 key로 조인하고 싶다.\n\n\n%%timeit\ndf1.merge(df2, how='left', left_on='uid_sample', right_on='userId_sample2')\n\n19.4 ms ± 981 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\n\n%%timeit\n# key로 사용하려는 컬럼을 index로 할당\n# 36%정도 빨라졌다!\ndf3 = df1.set_index('uid_sample')\ndf4 = df2.set_index('userId_sample2')\ndf3.merge(df4, right_index=True, left_index=True)\n\n12.6 ms ± 181 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\n\n%%timeit\n# key로 사용하려는 컬럼을 index로 할당\n# join 함수 사용\n# 여기서 이미 2.5배 빨라졌다\ndf3 = df1.set_index('uid_sample')\ndf4 = df2.set_index('userId_sample2')\ndf3.join(df4, how='left', lsuffix='left', rsuffix='right')\n\n8.04 ms ± 1.21 ms per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\n\n%%timeit\n# inner, outer밖에 안 되는데 join보다 느리다.\ndf3 = df1.set_index('uid_sample')\ndf4 = df2.set_index('userId_sample2')\npd.concat([df3, df4], axis=1, join='inner')\n\n11.5 ms ± 341 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\n\n\n\n4-3. Join : Merge보다 빠르다\nDataFrame.join(other, on=None, how='left', lsuffix='', rsuffix='', sort=False)\n\nother : 다른 데이터프레임, 혹은 시리즈, 혹은 데이터프레임 리스트\n\n함수를 호출한 데이터프레임에 붙일 대상\n\non : 조인 키가 될 컬럼 이름, 혹은 컬럼 이름 리스트(array-like 자료형이면 됨)\nhow\n\nleft : 함수를 호출한 데이터프레임(caller)의 index를 조인 키로 사용. on에서 컬럼을 지정했을 경우, 그 컬럼을 사용\nright : other 패러미터에 할당된 객체의 index를 사용\nouter : outer join 실행 후, 사전 순으로 정렬함. 기본적으론 양쪽 다 index를 사용. on에서 지정하면 caller만 해당 컬럼 사용.\ninner : inner join 실행. caller의 순서 보존됨.\ncross : 양쪽의 곱집합 생성. left key(caller)의 순서 보존됨.\n\nlsuffix, rsuffix : join된 결과물 컬럼의 접미사 세팅.\nsort : TRUE면, join key의 사전 순서대로 정렬됨. FALSE면, how에서의 기본 처리방식을 따름.\n\n\ncaller = pd.DataFrame({'key': ['K0', 'K1', 'K2', 'K3', 'K4', 'K5'], 'A': ['A0', 'A1', 'A2', 'A3', 'A4', 'A5']})\nother = pd.DataFrame({'key': ['K0', 'K1', 'K2'], 'B': ['B0', 'B1', 'B2']})\nother2 = pd.DataFrame({'key': ['K0', 'K1', 'K2'], 'C': ['C0', 'C1', 'C2']})\n\n\ncaller_styler = caller.style.set_table_attributes(\"style='display:inline;margin:5px'\").set_caption('caller')\nother_styler = other.style.set_table_attributes(\"style='display:inline;margin:5px'\").set_caption('other')\nother2_styler = other2.style.set_table_attributes(\"style='display:inline;margin:5px'\").set_caption('other2')\ndisplay_html(caller_styler._repr_html_() + other_styler._repr_html_() + other2_styler._repr_html_(), raw=True)\n\n\ncaller                    key        A    \n                \n                        0\n                        K0\n                        A0\n            \n            \n                        1\n                        K1\n                        A1\n            \n            \n                        2\n                        K2\n                        A2\n            \n            \n                        3\n                        K3\n                        A3\n            \n            \n                        4\n                        K4\n                        A4\n            \n            \n                        5\n                        K5\n                        A5\n            \n    other                    key        B    \n                \n                        0\n                        K0\n                        B0\n            \n            \n                        1\n                        K1\n                        B1\n            \n            \n                        2\n                        K2\n                        B2\n            \n    other2                    key        C    \n                \n                        0\n                        K0\n                        C0\n            \n            \n                        1\n                        K1\n                        C1\n            \n            \n                        2\n                        K2\n                        C2\n            \n    \n\n\n\n# 이러면 index 0,1,2,3,4,5 기준으로 join됨\ncaller.join(other, lsuffix='_caller', rsuffix='_other')\n\n\n\n\n\n  \n    \n      \n      key_caller\n      A\n      key_other\n      B\n    \n  \n  \n    \n      0\n      K0\n      A0\n      K0\n      B0\n    \n    \n      1\n      K1\n      A1\n      K1\n      B1\n    \n    \n      2\n      K2\n      A2\n      K2\n      B2\n    \n    \n      3\n      K3\n      A3\n      NaN\n      NaN\n    \n    \n      4\n      K4\n      A4\n      NaN\n      NaN\n    \n    \n      5\n      K5\n      A5\n      NaN\n      NaN\n    \n  \n\n\n\n\n\n# set_index로 조인 키로 쓰고 싶은 컬럼을 index로 만들어주는 방법이 있음\ncaller.set_index('key').join(other.set_index('key'))\n\n\n\n\n\n  \n    \n      \n      A\n      B\n    \n    \n      key\n      \n      \n    \n  \n  \n    \n      K0\n      A0\n      B0\n    \n    \n      K1\n      A1\n      B1\n    \n    \n      K2\n      A2\n      B2\n    \n    \n      K3\n      A3\n      NaN\n    \n    \n      K4\n      A4\n      NaN\n    \n    \n      K5\n      A5\n      NaN\n    \n  \n\n\n\n\n\n# 혹은, other만 조인 키로 쓰고 싶은 컬럼을 index로 만들어 주고,\n# on에다 조인 키로 쓰고 싶은 caller의 컬럼을 할당하는 방법이 있음\ncaller.join(other.set_index('key'), on='key')\n\n\n\n\n\n  \n    \n      \n      key\n      A\n      B\n    \n  \n  \n    \n      0\n      K0\n      A0\n      B0\n    \n    \n      1\n      K1\n      A1\n      B1\n    \n    \n      2\n      K2\n      A2\n      B2\n    \n    \n      3\n      K3\n      A3\n      NaN\n    \n    \n      4\n      K4\n      A4\n      NaN\n    \n    \n      5\n      K5\n      A5\n      NaN\n    \n  \n\n\n\n\n\n# 한 번에 여러 개의 Dataframe을 Join 할 때에는, index로 join하는 것만 지원한다.\n# 즉, on을 쓰지 못한다는 이야기이다.\ncaller.set_index('key').join([other.set_index('key'), other2.set_index('key')])\n\n\n\n\n\n  \n    \n      \n      A\n      B\n      C\n    \n    \n      key\n      \n      \n      \n    \n  \n  \n    \n      K0\n      A0\n      B0\n      C0\n    \n    \n      K1\n      A1\n      B1\n      C1\n    \n    \n      K2\n      A2\n      B2\n      C2\n    \n    \n      K3\n      A3\n      NaN\n      NaN\n    \n    \n      K4\n      A4\n      NaN\n      NaN\n    \n    \n      K5\n      A5\n      NaN\n      NaN"
  },
  {
    "objectID": "posts/2021-11-05-pandas_cheatsheet.html#데이터-재구조화",
    "href": "posts/2021-11-05-pandas_cheatsheet.html#데이터-재구조화",
    "title": "Pandas CheatSheet",
    "section": "5. 데이터 재구조화",
    "text": "5. 데이터 재구조화\n\n5.1 Pivot : 엑셀에서 보던 그것\nDataFrame.pivot(index=None, columns=None, values=None)\n\nindex : str or object or a list of str, optional\n\n새로운 프레임의 index로 사용할 컬럼\n\ncolumns : str of object or a list of str\n\n새로운 프레임의 컬럼으로 사용할 컬럼\n\nvalues : str, object or a list of the previous, optional\n\n새로운 프레임의 값을 계산하기 위해 사용하는 컬럼\n지정하지 않으면, 남아있는 모든 컬럼을 사용한다.\n\n\n\ndf = pd.DataFrame({'foo': ['one', 'one', 'one', 'two', 'two',\n                           'two'],\n                   'bar': ['A', 'B', 'C', 'A', 'B', 'C'],\n                   'baz': [1, 2, 3, 4, 5, 6],\n                   'zoo': ['x', 'y', 'z', 'q', 'w', 't']})\ndf\n\n\n\n\n\n  \n    \n      \n      foo\n      bar\n      baz\n      zoo\n    \n  \n  \n    \n      0\n      one\n      A\n      1\n      x\n    \n    \n      1\n      one\n      B\n      2\n      y\n    \n    \n      2\n      one\n      C\n      3\n      z\n    \n    \n      3\n      two\n      A\n      4\n      q\n    \n    \n      4\n      two\n      B\n      5\n      w\n    \n    \n      5\n      two\n      C\n      6\n      t\n    \n  \n\n\n\n\n\ndf.pivot(index='foo', columns='bar', values='baz')\n\n\n\n\n\n  \n    \n      bar\n      A\n      B\n      C\n    \n    \n      foo\n      \n      \n      \n    \n  \n  \n    \n      one\n      1\n      2\n      3\n    \n    \n      two\n      4\n      5\n      6\n    \n  \n\n\n\n\n\ndf.pivot(index='foo', columns='bar')\n\n\n\n\n\n  \n    \n      \n      baz\n      zoo\n    \n    \n      bar\n      A\n      B\n      C\n      A\n      B\n      C\n    \n    \n      foo\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      one\n      1\n      2\n      3\n      x\n      y\n      z\n    \n    \n      two\n      4\n      5\n      6\n      q\n      w\n      t\n    \n  \n\n\n\n\n\ndf.pivot(index='foo', columns='bar')['baz']\n\n\n\n\n\n  \n    \n      bar\n      A\n      B\n      C\n    \n    \n      foo\n      \n      \n      \n    \n  \n  \n    \n      one\n      1\n      2\n      3\n    \n    \n      two\n      4\n      5\n      6\n    \n  \n\n\n\n\n\ndf = pd.DataFrame({\n       \"lev1\": [1, 1, 1, 2, 2, 2],\n       \"lev2\": [1, 1, 2, 1, 1, 2],\n       \"lev3\": [1, 2, 1, 2, 1, 2],\n       \"lev4\": [1, 2, 3, 4, 5, 6],\n       \"values\": [0, 1, 2, 3, 4, 5]})\ndf\n\n\n\n\n\n  \n    \n      \n      lev1\n      lev2\n      lev3\n      lev4\n      values\n    \n  \n  \n    \n      0\n      1\n      1\n      1\n      1\n      0\n    \n    \n      1\n      1\n      1\n      2\n      2\n      1\n    \n    \n      2\n      1\n      2\n      1\n      3\n      2\n    \n    \n      3\n      2\n      1\n      2\n      4\n      3\n    \n    \n      4\n      2\n      1\n      1\n      5\n      4\n    \n    \n      5\n      2\n      2\n      2\n      6\n      5\n    \n  \n\n\n\n\n\ndf.pivot(index=\"lev1\", columns=[\"lev2\", \"lev3\"] ,values=\"values\")\n# Multilevel Column\n# 해당하는 조건에 맞는 값이 없으면 NaN이 들어가게 됨\n\n\n\n\n\n  \n    \n      lev2\n      1\n      2\n    \n    \n      lev3\n      1\n      2\n      1\n      2\n    \n    \n      lev1\n      \n      \n      \n      \n    \n  \n  \n    \n      1\n      0.0\n      1.0\n      2.0\n      NaN\n    \n    \n      2\n      4.0\n      3.0\n      NaN\n      5.0\n    \n  \n\n\n\n\n\ndf.pivot(index=[\"lev1\", \"lev2\"], columns=[\"lev3\"],values=\"values\")\n# Multiindex\n\n\n\n\n\n  \n    \n      \n      lev3\n      1\n      2\n    \n    \n      lev1\n      lev2\n      \n      \n    \n  \n  \n    \n      1\n      1\n      0.0\n      1.0\n    \n    \n      2\n      2.0\n      NaN\n    \n    \n      2\n      1\n      4.0\n      3.0\n    \n    \n      2\n      NaN\n      5.0\n    \n  \n\n\n\n\n\n#collapse-output\ndf.pivot(index=[\"lev1\"], columns=[\"lev2\"],values=\"values\")\n# 인덱스, 컬럼 쌍에 중복이 발생하면 에러가 출력됨\n# ValueError: Index contains duplicate entries, cannot reshape\n\nValueError: Index contains duplicate entries, cannot reshape\n\n\n\n\n5.2 Pivot_table : Pivot의 확장 버전\npandas.pivot_table(data, values=None, index=None, columns=None, aggfunc='mean', fill_value=None, margins=False, dropna=True, margins_name='All', observed=False, sort=True)\n\n#hide_input\ndf = pd.DataFrame({\"A\": [\"foo\", \"foo\", \"foo\", \"foo\", \"foo\",\n                         \"bar\", \"bar\", \"bar\", \"bar\"],\n                   \"B\": [\"one\", \"one\", \"one\", \"two\", \"two\",\n                         \"one\", \"one\", \"two\", \"two\"],\n                   \"C\": [\"small\", \"large\", \"large\", \"small\",\n                         \"small\", \"large\", \"small\", \"small\",\n                         \"large\"],\n                   \"D\": [1, 2, 2, 3, 3, 4, 5, 6, 7],\n                   \"E\": [2, 4, 5, 5, 6, 6, 8, 9, 9]})\ndf\n\n\n\n\n\n  \n    \n      \n      A\n      B\n      C\n      D\n      E\n    \n  \n  \n    \n      0\n      foo\n      one\n      small\n      1\n      2\n    \n    \n      1\n      foo\n      one\n      large\n      2\n      4\n    \n    \n      2\n      foo\n      one\n      large\n      2\n      5\n    \n    \n      3\n      foo\n      two\n      small\n      3\n      5\n    \n    \n      4\n      foo\n      two\n      small\n      3\n      6\n    \n    \n      5\n      bar\n      one\n      large\n      4\n      6\n    \n    \n      6\n      bar\n      one\n      small\n      5\n      8\n    \n    \n      7\n      bar\n      two\n      small\n      6\n      9\n    \n    \n      8\n      bar\n      two\n      large\n      7\n      9\n    \n  \n\n\n\n\n\ntable = pd.pivot_table(df, values='D', index=['A', 'B'], columns=['C'], aggfunc=np.sum)\ntable # aggfunc에 집계함수를 넣게 된다. 여기서는 총합\n\n\n\n\n\n  \n    \n      \n      C\n      large\n      small\n    \n    \n      A\n      B\n      \n      \n    \n  \n  \n    \n      bar\n      one\n      4.0\n      5.0\n    \n    \n      two\n      7.0\n      6.0\n    \n    \n      foo\n      one\n      4.0\n      1.0\n    \n    \n      two\n      NaN\n      6.0\n    \n  \n\n\n\n\n\ntable = pd.pivot_table(df, values='D', index=['A', 'B'],\n                    columns=['C'], aggfunc=np.sum, fill_value=0)\ntable # fill_value에 할당된 값으로 NaN을 대체하게 됨\n\n\n\n\n\n  \n    \n      \n      C\n      large\n      small\n    \n    \n      A\n      B\n      \n      \n    \n  \n  \n    \n      bar\n      one\n      4\n      5\n    \n    \n      two\n      7\n      6\n    \n    \n      foo\n      one\n      4\n      1\n    \n    \n      two\n      0\n      6\n    \n  \n\n\n\n\n\ntable = pd.pivot_table(df, values=['D', 'E'], index=['A', 'C'],\n                    aggfunc={'D': np.mean,\n                             'E': np.sum})\ntable # aggfunc에 Dictionary를 할당하여 값마다 집계함수를 각각 다르게 설정할 수 있다.\n\n\n\n\n\n  \n    \n      \n      \n      D\n      E\n    \n    \n      A\n      C\n      \n      \n    \n  \n  \n    \n      bar\n      large\n      5.500000\n      15\n    \n    \n      small\n      5.500000\n      17\n    \n    \n      foo\n      large\n      2.000000\n      9\n    \n    \n      small\n      2.333333\n      13\n    \n  \n\n\n\n\n\ntable = pd.pivot_table(df, values=['D', 'E'], index=['A', 'C'],\n                    aggfunc={'D': np.mean,\n                             'E': [min, max, np.mean]})\ntable # 한 값에 여러 개의 집계함수 할당도 가능하다.\n\n\n\n\n\n  \n    \n      \n      \n      D\n      E\n    \n    \n      \n      \n      mean\n      max\n      mean\n      min\n    \n    \n      A\n      C\n      \n      \n      \n      \n    \n  \n  \n    \n      bar\n      large\n      5.500000\n      9.0\n      7.500000\n      6.0\n    \n    \n      small\n      5.500000\n      9.0\n      8.500000\n      8.0\n    \n    \n      foo\n      large\n      2.000000\n      5.0\n      4.500000\n      4.0\n    \n    \n      small\n      2.333333\n      6.0\n      4.333333\n      2.0\n    \n  \n\n\n\n\n\ntable = pd.pivot_table(df, values=['D', 'E'], index=['A', 'C'],\n                    aggfunc={'D': np.mean,\n                             'E': np.mean},\n                    margins=True, margins_name=\"mean\")\ntable # Values에 적용된 집계함수를 컬럼 전체에 적용한 행을 추가한다.\n# 한 Value에 집계함수를 하나만 사용했을 때 적용 가능.\n# margins_name을 지정하지 않으면 기본적으로 행 Index 이름은 All이 된다.\n\n\n\n\n\n  \n    \n      \n      \n      D\n      E\n    \n    \n      A\n      C\n      \n      \n    \n  \n  \n    \n      bar\n      large\n      5.500000\n      7.500000\n    \n    \n      small\n      5.500000\n      8.500000\n    \n    \n      foo\n      large\n      2.000000\n      4.500000\n    \n    \n      small\n      2.333333\n      4.333333\n    \n    \n      mean\n      \n      3.666667\n      6.000000\n    \n  \n\n\n\n\n\n\n5.3 melt : Unpivot 하기\npandas.melt(frame, id_vars=None, value_vars=None, var_name=None, value_name='value', col_level=None, ignore_index=True)\n\nid_vars : tuple, list, or ndarray, optional\n\n식별자로 사용할 컬럼\n\nvalue_vars : tuple, list, or ndarray, optional\n\nUnpivot 할 컬럼. 지정하지 않으면, id_vars에 할당되지 않은 모든 컬럼을 사용\n\n\n\ndf = pd.DataFrame({'A': {0: 'a', 1: 'b', 2: 'c'},\n                   'B': {0: 1, 1: 3, 2: 5},\n                   'C': {0: 2, 1: 4, 2: 6}})\ndf\n\n\n\n\n\n  \n    \n      \n      A\n      B\n      C\n    \n  \n  \n    \n      0\n      a\n      1\n      2\n    \n    \n      1\n      b\n      3\n      4\n    \n    \n      2\n      c\n      5\n      6\n    \n  \n\n\n\n\n\npd.melt(df, id_vars=['A'], value_vars=['B'])\n\n\n\n\n\n  \n    \n      \n      A\n      variable\n      value\n    \n  \n  \n    \n      0\n      a\n      B\n      1\n    \n    \n      1\n      b\n      B\n      3\n    \n    \n      2\n      c\n      B\n      5\n    \n  \n\n\n\n\n\npd.melt(df, id_vars=['A'], value_vars=['B', 'C'])\n\n\n\n\n\n  \n    \n      \n      A\n      variable\n      value\n    \n  \n  \n    \n      0\n      a\n      B\n      1\n    \n    \n      1\n      b\n      B\n      3\n    \n    \n      2\n      c\n      B\n      5\n    \n    \n      3\n      a\n      C\n      2\n    \n    \n      4\n      b\n      C\n      4\n    \n    \n      5\n      c\n      C\n      6\n    \n  \n\n\n\n\n\npd.melt(df, id_vars=['A'], value_vars=['B'],\n        var_name='myVarname', value_name='myValname')\n# 이름은 커스터마이징 가능\n\n\n\n\n\n  \n    \n      \n      A\n      myVarname\n      myValname\n    \n  \n  \n    \n      0\n      a\n      B\n      1\n    \n    \n      1\n      b\n      B\n      3\n    \n    \n      2\n      c\n      B\n      5\n    \n  \n\n\n\n\n\npd.melt(df, id_vars=['A'], value_vars=['B', 'C'], ignore_index=False)\n# 원본 index 유지\n\n\n\n\n\n  \n    \n      \n      A\n      variable\n      value\n    \n  \n  \n    \n      0\n      a\n      B\n      1\n    \n    \n      1\n      b\n      B\n      3\n    \n    \n      2\n      c\n      B\n      5\n    \n    \n      0\n      a\n      C\n      2\n    \n    \n      1\n      b\n      C\n      4\n    \n    \n      2\n      c\n      C\n      6"
  },
  {
    "objectID": "posts/2021-11-05-pandas_cheatsheet.html#데이터-타입",
    "href": "posts/2021-11-05-pandas_cheatsheet.html#데이터-타입",
    "title": "Pandas CheatSheet",
    "section": "6. 데이터 타입",
    "text": "6. 데이터 타입\n\n6-1. dtypes : 컬럼들의 type 출력\n\ndf = pd.DataFrame({'float': [1.0],\n                   'int': [1],\n                   'datetime': [pd.Timestamp('20180310')],\n                   'string': ['foo']})\ndf.dtypes\n# 더 이상의 설명은 필요 없다!\n\nfloat              float64\nint                  int64\ndatetime    datetime64[ns]\nstring              object\ndtype: object\n\n\n\n\n6-2. select_dtypes : 특정 타입의 컬럼을 선택, 혹은 배제\nDataFrame.select_dtypes(include=None, exclude=None)\n\nTo select all numeric types, use np.number or ‘number’\nTo select strings you must use the object dtype, but note that this will return all object dtype columns See the numpy dtype hierarchy\nTo select datetimes, use np.datetime64, ‘datetime’ or ‘datetime64’\nTo select timedeltas, use np.timedelta64, ‘timedelta’ or ‘timedelta64’\nTo select Pandas categorical dtypes, use ‘category’\nTo select Pandas datetimetz dtypes, use ‘datetimetz’ (new in 0.20.0) or ‘datetime64[ns, tz]’\n\n\ndf = pd.DataFrame({'a': [1, 2] * 3,\n                   'b': [True, False] * 3,\n                   'c': [1.0, 2.0] * 3})\ndf\n\n\n\n\n\n  \n    \n      \n      a\n      b\n      c\n    \n  \n  \n    \n      0\n      1\n      True\n      1.0\n    \n    \n      1\n      2\n      False\n      2.0\n    \n    \n      2\n      1\n      True\n      1.0\n    \n    \n      3\n      2\n      False\n      2.0\n    \n    \n      4\n      1\n      True\n      1.0\n    \n    \n      5\n      2\n      False\n      2.0\n    \n  \n\n\n\n\n\ndf.select_dtypes(include='bool')\n\n\n\n\n\n  \n    \n      \n      b\n    \n  \n  \n    \n      0\n      True\n    \n    \n      1\n      False\n    \n    \n      2\n      True\n    \n    \n      3\n      False\n    \n    \n      4\n      True\n    \n    \n      5\n      False\n    \n  \n\n\n\n\n\ndf.select_dtypes(include=['float64'])\n\n\n\n\n\n  \n    \n      \n      c\n    \n  \n  \n    \n      0\n      1.0\n    \n    \n      1\n      2.0\n    \n    \n      2\n      1.0\n    \n    \n      3\n      2.0\n    \n    \n      4\n      1.0\n    \n    \n      5\n      2.0\n    \n  \n\n\n\n\n\ndf.select_dtypes(exclude=['int64'])\n\n\n\n\n\n  \n    \n      \n      b\n      c\n    \n  \n  \n    \n      0\n      True\n      1.0\n    \n    \n      1\n      False\n      2.0\n    \n    \n      2\n      True\n      1.0\n    \n    \n      3\n      False\n      2.0\n    \n    \n      4\n      True\n      1.0\n    \n    \n      5\n      False\n      2.0\n    \n  \n\n\n\n\n\n\n6-3. astype : 타입 변경. Bigquery에 df 업로드 시 반드시 사용\nDataFrame.astype(dtype, copy=True, errors='raise')\n\ncopy : False를 하면, 복사를 하는 게 아니고 원본에 연결되므로 변경사항이 원본에까지 전파됨\nerrors : ignore로 세팅하면, 에러 발생 시 원본을 반환하고 끝냄\n\n\nd = {'col1': [1, 2], 'col2': [3, 4]}\ndf = pd.DataFrame(data=d)\ndf.dtypes\n\ncol1    int64\ncol2    int64\ndtype: object\n\n\n\ndf.astype({'col1': 'int32'}).dtypes\n# 잘 변경됐습니다~\n\ncol1    int32\ncol2    int64\ndtype: object"
  },
  {
    "objectID": "posts/2021-11-05-pandas_cheatsheet.html#데이터-집계",
    "href": "posts/2021-11-05-pandas_cheatsheet.html#데이터-집계",
    "title": "Pandas CheatSheet",
    "section": "7. 데이터 집계",
    "text": "7. 데이터 집계\n\n7-1. groupby : agg\n\n{컬럼명 : 집계함수} 구조의 dictionary를 변수로 주면, 각 컬럼마다 할당된 집계함수로 집계한다.\n\n집계함수는 커스텀 함수를 사용해도 됨. 아래 코드처럼.\n\n\n\ndf = pd.DataFrame({\n    'month': ['10월', '10월', '10월', '10월', '11월', '12월', '12월'],\n    'fruits': ['apple', 'orange', 'banana', 'banana', 'apple', 'apple', 'banana'],\n    'price': [100, 200, 250, 300, 150, 200, 400],\n    'quantity': [1, 2, 3, 4, 5, 6, 7]\n})\n\ndef mean_and_sqrt(x): # 이런 묘한 출력값이 필요한 상황이 있다고 치자.\n        return np.sqrt(np.mean(x))\n    \ndf.groupby(by=['month','fruits']).agg({'price':mean_and_exp, 'quantity':np.sum})\n\n\n\n\n\n  \n    \n      \n      \n      price\n      quantity\n    \n    \n      month\n      fruits\n      \n      \n    \n  \n  \n    \n      10월\n      apple\n      10.000000\n      1\n    \n    \n      banana\n      16.583124\n      7\n    \n    \n      orange\n      14.142136\n      2\n    \n    \n      11월\n      apple\n      12.247449\n      5\n    \n    \n      12월\n      apple\n      14.142136\n      6\n    \n    \n      banana\n      20.000000\n      7\n    \n  \n\n\n\n\n\n데이터 집계 방식 중, 가장 범용성이 좋고, 함수화하기도 편한 방식이었다.\n실제로 사용했던 예시\n\n\nagg_dict = {key:np.sum for key in target_column} # 전부 집계함수 sum으로 세팅\nagg_dict['accountId'] = 'count' # accountId 수만 count로 세팅. accountId 수를 센다.\n\nready_for_stacked = for_stackbar.groupby(by=['log_cate']).agg(agg_dict) # 집계 적용"
  },
  {
    "objectID": "posts/2021-11-05-pandas_cheatsheet.html#파일-입출력",
    "href": "posts/2021-11-05-pandas_cheatsheet.html#파일-입출력",
    "title": "Pandas CheatSheet",
    "section": "8. 파일 입출력",
    "text": "8. 파일 입출력\n\n8-1. read_excel\n\nstring_quest = pd.read_excel(r\"C:\\Users\\Documents\\data\\string\\string_quest.xlsx\",\n                             header=6, usecols=\"H,I\", sheet_name = \"string_quest\", engine=\"openpyxl\")\n\n\nheader : 몇 번째 row를 header로 할까?\nusercols : 어떤 열을 가져올까?\nsheet_name : 어떤 시트를 가져올까?\nengine : openpyxl을 사용하여야 xlsx 파일의 불러오기가 가능\n\n\n\n8-2. to_excel\n\n데이터프레임을 잘 구성했다면, 기본적으로는 아래와 같이 특별한 argument 없이 사용해도 별 문제는 없다.\n\n\nfor each_key in dict_account_id.keys():\n    target_dataframe = pd.DataFrame.from_dict(dict_account_id[each_key])\n    target_dataframe.to_excel(f\"{each_key}_성장재료_출처.xlsx\")\n\n\n내용 추가 모드 (append)를 사용하고 싶다면, 아래와 같이 할 수 있다.\n\n\nwith pd.ExcelWriter('output.xlsx', mode='a') as writer: # mode='a'로 append 설정\n    df.to_excel(writer, sheet_name='Sheet_name_3') #sheet 이름도 지정함\n\n\n추가 내용 : https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_excel.html\n\n\n\n8-3. read_csv\n\ntarget_df = pd.read_csv('data.csv')\n\n\nsep : delimeter. 기본값은 쉼표(,)이다.\nheader : 몇 번째 row를 header로 할까?\nusercols : 어떤 열을 가져올까?\nchunksize : 파일이 너무 클 경우, 한 번에 불러올 행 수를 지정 가능. 이후 iterable하게 다음 size를 불러올 수 있다. 아래와 같이 사용.\n\nchunksize = 10 ** 6\nwith pd.read_csv(filename, chunksize=chunksize) as reader:\n    for chunk in reader:\n        process(chunk)\n\n추가 내용 : https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html\n\n\n\n8-4. to_csv\n\ntarget_df = pd.to_csv('data.csv')\n\n\nsep : delimeter. 기본값은 쉼표(,)이다.\ncolumns : 쓰기에 사용할 컬럼 이름 리스트.\nheader : bool or list of str, default True\n\n문자열 리스트가 주어지면, 각 컬럼에 대한 alias로 취급된다.\n\n추가 내용 : https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_csv.html\n\n\n\n8-5. Dataframe을 이미지로 추출\n\nmatplotlib를 이용\n\ndataframe-image 패키지를 이용 시, linux에서 crontab으로 실행할 경우 복잡한 권한 문제에 직면하게 됨\ndataframe-image 패키지도 matplotlib 기반이므로, 그냥 matplotlib를 사용\n\n\nimport six\nimport matplotlib.pyplot as plt\nimport matplotlib.font_manager as fm\n\n# matplotlib에서 한글이 안 나오는 문제 해결\nNANUM = fm.FontProperties(fname=r'C:\\Users\\limyj0708\\AppData\\Local\\Microsoft\\Windows\\Fonts\\NanumBarunGothic.ttf')\nNANUM_bold = fm.FontProperties(fname=r'C:\\Users\\limyj0708\\AppData\\Local\\Microsoft\\Windows\\Fonts\\NanumBarunGothicBold.ttf')\n\n# centos라면 폰트 경로는 아래와 같음\n ## /usr/share/fonts/NanumFont/NanumBarunGothic.ttf\n ## /usr/share/fonts/NanumFont/NanumGothicBold.ttf\n\ndef render_mpl_table(data, col_width=3.0, row_height=0.625, font_size_header=16, font_size=14,\n                     header_color='#C2DED1', row_colors=['#f1f1f2', 'w'], edge_color='black',\n                     bbox=[0, 0, 1, 1], header_columns=0,\n                     ax=None, align_head='center', align_cell='center', **kwargs):\n    \"\"\"\n    align_head, align_cell : [ 'center' | 'right' | 'left' ] \n    \"\"\"\n    \n    if ax is None:\n        size = (np.array(data.shape[::-1]) + np.array([0, 1])) * np.array([col_width, row_height])\n        fig, ax = plt.subplots(figsize=size)\n        ax.axis('off')\n\n    mpl_table = ax.table(cellText=data.values, bbox=bbox, colLabels=data.columns, **kwargs)\n    mpl_table.auto_set_font_size(False)\n\n    for k, cell in  six.iteritems(mpl_table._cells):\n        cell.set_edgecolor(edge_color)\n        if k[0] == 0 or k[1] < header_columns:\n            cell.set_facecolor(header_color)\n            cell.set_text_props(color='black', fontproperties = NANUM_bold, fontsize=font_size_header, ha=align_head)\n        else:\n            cell.set_facecolor(row_colors[k[0]%len(row_colors)])\n            cell.set_text_props(fontproperties = NANUM, fontsize=font_size, ha=align_cell)\n    return ax\n\nimage = render_mpl_table(caller, col_width=2.0, align_head='left')\nimage\nimage.figure.savefig(\"caller.png\") # 이미지 저장\n# crontab으로 돌릴 것이라면 이미지 저장 경로도 절대경로로 지정"
  },
  {
    "objectID": "posts/2022-03-19-맥 OS pyenv 세팅 101.html",
    "href": "posts/2022-03-19-맥 OS pyenv 세팅 101.html",
    "title": "맥 OS pyenv 세팅 101",
    "section": "",
    "text": "https://brew.sh 확인하여 설치\n설치 후 ~/.zprofile에 eval \"$(/opt/homebrew/bin/brew shellenv)\" 추가\n\n다 설치하면 맨 마지막에 안내문으로 추가하라고 나오니 따라하기만 하자.\n\n\n\n\n\n\nbrew install pyenv : 설치가 끝났다면…\n\n.zshrc에 eval \"$(pyenv init -)\" 추가\n.zprofile에 eval \"$(pyenv init --path)\" 추가\n\n\n\n\n\n\npyenv install -list : 설치가능한 파이썬 버전 목록 확인\npyenv install 3.10.3 : 예) 3.10.3 버전 설치\n\n\n\n\n\nbrew install pyenv-virtualenv\n\n.zshrc에 eval \"$(pyenv virtualenv-init -)\" 추가\n\n\n\n\n\n\npyenv virtualenv [파이썬 버전] [가상환경 이름]\n\n예) pyenv virtualenv 3.10.3 requests-3.10.3\n\npyenv versions : 생성한 가상환경이 추가되었음을 알 수 있음\n\n\n\n\n\n직접 on/off\n\npyenv activate requests-3.10.3\n\n실행하면 아래와 같은 메세지가 출력된다.\npyenv-virtualenv: prompt changing will be removed from future release. configure “export PYENV_VIRTUALENV_DISABLE_PROMPT=1” to simulate the behavior\n곧 이 기능은 사라질 모양이다.\n\npyenv deactivate\n\nshell의 세션이 유지되는 동안 가상환경 유지\n\npyenv shell requests-3.10.3\n\n원하는 폴더에 가서 실행하면, 이후 shell에서 해당 폴더로 가면 자동으로 원하는 가상환경이 켜지게 됨 (.python-version 파일이 해당 폴더에 생성)\n\npyenv local requests-3.10.3\npyenv local system : 다시 기본 시스템 버전으로 돌리고 싶을 때\n해당 폴더에서 나가면 자동으로 기본 환경으로 돌아가게 된다. 편리하네!\n\n전체 적용\n\npyenv global requests-3.10.3\npyenv global system : 다시 기본 시스템 버전으로 돌리고 싶을 때\n\nshell의 세션이 유지되는 동안 가상환경 유지\n\npyenv shell requests-3.10.3"
  },
  {
    "objectID": "posts/2022-05-26-Jupyter Lab Server 세팅.html",
    "href": "posts/2022-05-26-Jupyter Lab Server 세팅.html",
    "title": "Jupyter Lab Server 세팅",
    "section": "",
    "text": "CentOS에 Jupyter Lab 설치가 완료되었다고 가정하자.\n\n구글에 Jupyter lab server라고 검색하면, 아래 페이지가 가장 먼저 뜨게 되는데, 이 페이지 말고\n\nhttps://jupyter-notebook.readthedocs.io/en/stable/public_server.html\n\n이 페이지를 확인하는 것이 좋다.\n\nhttps://jupyter-server.readthedocs.io/en/latest/operators/public-server.html\n\n\n\n\n\n\njupyter server --generate-config를 하면, /home/“유저이름”/.jupyter/jupyter_server_config.py가 생성된다. 여기에서 세팅을 해야 한다.\n\nc.ServerApp.open_browser = False (브라우저 띄우지 않음)\nc.ServerApp.password = ‘argon2…’\n\nfrom jupyter_server.auth import passwd; passwd()를 실행하여 생성하는, 암호화된 비밀번호를 입력한다.\n\nc.ServerApp.port = 원하는 포트\nc.ServerApp.certfile = openssl로 만든 certfile 등록 (예: mycert.pem)\nc.ServerApp.keyfile = openssl로 만든 keyfile 등록 (예: mykey.key)\nc.ServerApp.ip = ’*’, 혹은 접근 가능하게 하고싶은 ip\nc.ServerApp.root_dir = 원하는 경로\n\nnotebook_dir is deprecated, use root_dir\n\nc.ServerApp.allow_origin = ’*’\n\nUse ’*’ to allow any origin to access your server.\n\n\n\n\n\n\n\nself-signed certificate 오류 메세지\n\nopenssl req -x509 -nodes -days 999 -newkey rsa:2048 -keyout mykey.key -out mycert.pem\n\n이런 식으로 만든 self-signed certificate를 쓰면 jupyter lab 실행 시, 뭘 하기만 하면 SSL Error를 띄운다.\n\n이런 식으로 : SSL Error on 13 (‘ip’, 13786): [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:997)\nSafari에서는 안 뜨고, Edge에서는 뜨는 걸로 봐서 크로미움 기반 브라우저에서 접속하면 뜨는 것 같다.\n\n\nLet’s Encrypt 같은 서비스를 이용해서 인증서를 받아도 되는데, 도메인 네임도 없는, 혼자 쓰는 무료 클라우드 서버에서 그렇게까지 해야 하나 싶다.\ntmux에서 새 pane을 만들고, jupyter lab > /dev/null 2>&1 &으로 jupyter lab을 실행하여 콘솔 output을 없애고 백그라운드에서 jupyter lab을 실행하자."
  },
  {
    "objectID": "posts/2022-05-26-Python Google Drive API v3로 파일 업로드.html",
    "href": "posts/2022-05-26-Python Google Drive API v3로 파일 업로드.html",
    "title": "Python Google Drive API v3로 파일 업로드",
    "section": "",
    "text": "!pip3 install --upgrade google-api-python-client google-auth-httplib2 google-auth-oauthlib\n\n\n\n\n\nhttps://console.cloud.google.com/ 접속\n원하는 프로젝트 선택\nAPI & Services\n\nEnabled APIs & Services에서 Google Drive API 활성화\nCredentials\n\nCreate Credentials -> OAuth client ID 생성\n\nservice account를 사용하고 싶었으나, 대상 폴더가 회사 조직 내 계정이 아니면 공유가 되지 않는 폴더여서 service account 사용이 불가능\n가능한 상황이면, service account를 대상 폴더의 편집자로 추가하는 편이, 더 보안상 좋다.\n\nDownload OAuth Client\n\nclinet-secret JSON 파일이 받아진다.\n\n\n\nhttps://developers.google.com/drive/api/quickstart/python\n\nquickstart 스크립트를 적절하게 바꾸어서 실행한다.\n최초로 실행하면 로그인 과정 후에 token.json이 생성되고, 이후에는 token.json을 읽어서 실행된다.\n아래 스크립트는 xlsx 파일 하나를 원하는 폴더에 업로드 하는 스크립트이다.\n\n\n\nimport os.path\n\nfrom google.auth.transport.requests import Request\nfrom google.oauth2.credentials import Credentials\nfrom google_auth_oauthlib.flow import InstalledAppFlow\nfrom googleapiclient.discovery import build\nfrom googleapiclient.errors import HttpError\n\n# If modifying these scopes, delete the file token.json.\n# \nSCOPES = ['https://www.googleapis.com/auth/drive.file']\n\n\ndef main():\n    \"\"\"Shows basic usage of the Drive v3 API.\n    Prints the names and ids of the first 10 files the user has access to.\n    \"\"\"\n    creds = None\n    # The file token.json stores the user's access and refresh tokens, and is\n    # created automatically when the authorization flow completes for the first\n    # time.\n    if os.path.exists('token.json'):\n        creds = Credentials.from_authorized_user_file('token.json', SCOPES)\n    # If there are no (valid) credentials available, let the user log in.\n    if not creds or not creds.valid:\n        if creds and creds.expired and creds.refresh_token:\n            creds.refresh(Request())\n        else:\n            flow = InstalledAppFlow.from_client_secrets_file('Download OAuth Clinet에서 받은 client-secret JSON 파일', SCOPES)\n            creds = flow.run_local_server(port=0)\n        # Save the credentials for the next run\n        with open('token.json', 'w') as token:\n            token.write(creds.to_json())\n\n    try:\n        # 원하는 작업 코드 작성\n        # 이 경우에는, xlsx 파일 하나를 원하는 폴더에 업로드        \n        folder_id = '원하는 폴더 ID'\n        service = build('drive', 'v3', credentials=creds)\n        file_metadata = {'name': 'quest_main_join_string_name.xlsx','parents': [folder_id]}\n        media = MediaFileUpload('quest_main_join_string_name.xlsx',\n                            mimetype=None, resumable=True)\n        # 파일이 커질 것 같으면 resumable을 켜 주는 것이 좋다.\n        file = service.files().create(body=file_metadata,media_body=media,fields='id').execute\n        \n        except HttpError as error:\n            # TODO(developer) - Handle errors from drive API.\n            print(f'An error occurred: {error}')\n\nif __name__ == '__main__':\n    main()"
  },
  {
    "objectID": "posts/2022-06-13-git_cheatsheet.html",
    "href": "posts/2022-06-13-git_cheatsheet.html",
    "title": "Git CheatSheet",
    "section": "",
    "text": "안 쓰면 잊어버리는, git 주요 조작법들을 정리\n교과서 : https://git-scm.com/book/ko/v2"
  },
  {
    "objectID": "posts/2022-06-13-git_cheatsheet.html#기존-디렉토리를-git-저장소로-만들기",
    "href": "posts/2022-06-13-git_cheatsheet.html#기존-디렉토리를-git-저장소로-만들기",
    "title": "Git CheatSheet",
    "section": "1. 기존 디렉토리를 Git 저장소로 만들기",
    "text": "1. 기존 디렉토리를 Git 저장소로 만들기\n\n원하는 폴더로 이동 후 git init"
  },
  {
    "objectID": "posts/2022-06-13-git_cheatsheet.html#수정과-저장",
    "href": "posts/2022-06-13-git_cheatsheet.html#수정과-저장",
    "title": "Git CheatSheet",
    "section": "2. 수정과 저장",
    "text": "2. 수정과 저장\n\n\n\nlifecycle.png\n\n\n워킹 디렉토리의 모든 파일은 크게 Tracked(관리대상임)와 Untracked(관리대상이 아님)로 나눈다. Tracked 파일은 이미 스냅샷에 포함돼 있던 파일이다. Tracked 파일은 또 Unmodified(수정하지 않음)와 Modified(수정함) 그리고 Staged(커밋으로 저장소에 기록할) 상태 중 하나이다. 간단히 말하자면 Git이 알고 있는 파일이라는 것이다.\n그리고 나머지 파일은 모두 Untracked 파일이다. Untracked 파일은 워킹 디렉토리에 있는 파일 중 스냅샷에도 Staging Area에도 포함되지 않은 파일이다. 처음 저장소를 Clone 하면 모든 파일은 Tracked이면서 Unmodified 상태이다. 파일을 Checkout 하고 나서 아무것도 수정하지 않았기 때문에 그렇다.\n마지막 커밋 이후 아직 아무것도 수정하지 않은 상태에서 어떤 파일을 수정하면 Git은 그 파일을 Modified 상태로 인식한다. 실제로 커밋을 하기 위해서는 이 수정한 파일을 Staged 상태로 만들고, Staged 상태의 파일을 커밋한다. 이런 라이프사이클을 계속 반복한다.\n\n2-1. 상태 확인\n\ngit status\n\n  PS C:\\Users\\limyj0708\\fastpages> git status\nOn branch master\nYour branch is up to date with 'origin/master'.\n\nUntracked files:\n  (use \"git add <file>...\" to include in what will be committed)\n        _notebooks/2022-06-13-git_cheatsheet.ipynb\n\nnothing added to commit but untracked files present (use \"git add\" to track)\n\n2022-06-13-git_cheatsheet.ipynb 파일이 untracked 상태\nGit은 Untracked 파일을 아직 스냅샷(커밋)에 넣어지지 않은 파일이라고 본다. 파일이 Tracked 상태가 되기 전까지는 Git은 절대 그 파일을 커밋하지 않는다. 그래서 일하면서 생성하는 바이너리 파일 같은 것을 커밋하는 실수는 하지 않게 된다.\n\n\n\n2-2. 파일을 새로 추적하기\n\ngit add _notebooks/2022-06-13-git_cheatsheet.ipynb\n이후 다시 status를 보면\n\nPS C:\\Users\\limyj0708\\fastpages> git status\nOn branch master\nYour branch is up to date with 'origin/master'.\n\nChanges to be committed:\n  (use \"git restore --staged <file>...\" to unstage)\n        new file:   _notebooks/2022-06-13-git_cheatsheet.ipynb\n\n“Changes to be committed” 에 들어 있는 파일은 Staged 상태라는 것을 의미한다. 커밋하면 git add 를 실행한 시점의 파일이 커밋되어 저장소 히스토리에 남는다.\ngit add 명령은 파일 또는 디렉토리의 경로를 argument로 받는다. 디렉토리면 아래에 있는 모든 파일들까지 재귀적으로 추가한다.\ngit add . 의 경우, .은 현재 디렉토리를 나타내므로, 현재 디렉토리와 하위 디렉토리의 모든 파일들을 Staged 상태로 만든다.\n\n\n\n2-3. Modified 상태의 파일을 Staging 하기\n\n2022-06-13-git_cheatsheet.ipynb를 수정한 후에 git status를 해 보면?\n\nOn branch master\nYour branch is up to date with 'origin/master'.\n\nChanges to be committed:\n  (use \"git restore --staged <file>...\" to unstage)\n        new file:   _notebooks/2022-06-13-git_cheatsheet.ipynb\n\nChanges not staged for commit:\n  (use \"git add <file>...\" to update what will be committed)\n  (use \"git restore <file>...\" to discard changes in working directory)\n        modified:   _notebooks/2022-06-13-git_cheatsheet.ipynb\n\n“Changes not staged for commit” 에 있다. 이것은 수정한 파일이 Tracked 상태이지만 아직 Staged 상태는 아니라는 것이다. Staged 상태로 만들려면 git add 명령을 실행해야 한다. git add 명령은 파일을 새로 추적할 때도 사용하고 수정한 파일을 Staged 상태로 만들 때도 사용한다. Merge 할 때 충돌난 상태의 파일을 Resolve 상태로 만들때도 사용한다. add의 의미는 프로젝트에 파일을 추가한다기 보다는 다음 커밋에 추가한다고 받아들이는게 좋다.\ngit add _notebooks/2022-06-13-git_cheatsheet.ipynb후 다시 git status를 해 보자.\n\nPS C:\\Users\\limyj0708\\fastpages> git status\nOn branch master\nYour branch is up to date with 'origin/master'.\n\nChanges to be committed:\n  (use \"git restore --staged <file>...\" to unstage)\n        new file:   _notebooks/2022-06-13-git_cheatsheet.ipynb\n\n“Changes to be committed”에 잘 들어갔는데, 여기서 또 수정을 하고 git status를 하면?\n\nPS C:\\Users\\limyj0708\\fastpages> git status\nOn branch master\nYour branch is up to date with 'origin/master'.\n\nChanges to be committed:\n  (use \"git restore --staged <file>...\" to unstage)\n        new file:   _notebooks/2022-06-13-git_cheatsheet.ipynb\n\nChanges not staged for commit:\n  (use \"git add <file>...\" to update what will be committed)\n  (use \"git restore <file>...\" to discard changes in working directory)\n        modified:   _notebooks/2022-06-13-git_cheatsheet.ipynb\n\nChanges to be committed / Changes not staged for commit에 둘 다 2022-06-13-git_cheatsheet.ipynb이 들어있는 이유\n\n지금 이 시점에서 커밋을 하면 git commit 명령을 실행하는 시점의 버전이 커밋되는 것이 아니라 마지막으로 git add 명령을 실행했을 때의 버전이 커밋된다. 그러니까 git add 명령을 실행한 후에 또 파일을 수정하면 git add 명령을 다시 실행해서 최신 버전을 Staged 상태로 만들어야 한다.\n\n\n\n\n2-4. commit\n\nStaged 상태가 된 파일을 저장소에 기록\n커밋 메세지를 첨부하려면 -m을 붙이고 메시지를 기재\n\ngit commit -m \"modify readme\"\n[main c524828] modify readme\n 1 file changed, 23 insertions(+), 24 deletions(-)\n\nmain branch에 기록되었으며, 체크섬은 c524828\n-a 옵션을 붙이면, add를 해서 staging area에 변경된 파일을 추가하는 작업을 자동으로 처리해 줌\n\ngit commit -a -m \"modify readme\"\n\n\n\n\n2-5. 파일 삭제\n\ngit rm [파일명 or 디렉토리명]\n\n파일이 실제로 삭제된다.\n\n파일을 그냥 삭제하면, 파일이 unstaged 상태에 있다고 표시된다.\n\n파일을 그냥 삭제하였다면, git rm을 적용해 주어야 staged 상태가 된다.\n그리고 commit을 하면, 더 이상 파일을 추적하지 않는다.\n\n파일을 수정했는데 지우고 싶거나, staging area에 추가했다면, -f 옵션을 주어서 강제로 삭제해야 한다.\nStaging Area에서만 제거하고 디렉토리에 있는 파일은 지우지 않고 남겨두기\n\n–cached 옵션 사용\ngit rm --cached README\n\n한 번에 여러 파일 삭제하기\n\ngit rm log/\\*.log\n\nlog 폴더 내의, .log 확장자인 파일을 모두 삭제함\n\ngit rm \\*~\n\n이름이 ~로 끝나는 파일을 모두 삭제함\n\n\n\n\n\n2-6 파일 이동, 이름 바꾸기\n\ngit mv README.md README\n\nREADME.md를 README로 이름 변경"
  },
  {
    "objectID": "posts/2022-06-13-git_cheatsheet.html#원격-저장소",
    "href": "posts/2022-06-13-git_cheatsheet.html#원격-저장소",
    "title": "Git CheatSheet",
    "section": "3. 원격 저장소",
    "text": "3. 원격 저장소\n\n3-1 원격 저장소 확인하기\n$ git remote -v\norigin  https://github.com/limyj0708/bigquery_module.git (fetch)\norigin  https://github.com/limyj0708/bigquery_module.git (push)\n\n\n\n3-2. 원격 저장소 추가하기\n\ngit remote add <원격 저장소 이름> <url>\n\nclone 시에는, 단축이름이 자동으로 origin이 된다.\ngit clone https://github.com/limyj0708/fastpages.git : 이런 식으로 할 경우\n\n현재 디렉토리에 추가된 원격 저장소가 있는데, 다른 원격 저장소로 바꾸고 싶을 경우\n\nhttps://shanepark.tistory.com/284 참조\n\n\n\n\n3-3. 원격 저장소에서 Pull, Fetch\n\ngit fetch <원격 저장소 이름>\n\n로컬에는 없는데, 원격 저장소에 있는 내용을 모두 가져온다.\n가져오긴 하지만 branch를 merge 하지는 않으므로, 수동으로 merge 해야 한다.\n\ngit pull <원격 저장소 이름>\n\n원격 저장소에 있는 내용을 모두 가져온 후, branch merge까지 알아서 진행한다.\n최초에 내용을 git clone으로 가져왔을 경우, 자동으로 로컬의 master branch가 리모트 저장소의 master branch를 추적하도록 한다(물론 리모트 저장소에 master 브랜치가 있다는 가정에서).\n\n\n\n\n3-4. 원격 저장소에 Push 하기\n\ngit push <원격 저장소 이름> <브랜치 이름>\n\n최초에 git clone으로 가져왔을 경우, 단축이름은 origin이고 branch 이름은 master이므로 아래와 같이 된다.\ngit push origin master\n\n이 명령은 Clone 한 리모트 저장소에 쓰기 권한이 있고, Clone 하고 난 이후 아무도 Upstream 저장소에 Push 하지 않았을 때만 사용할 수 있다. 다시 말해서 Clone 한 사람이 여러 명 있을 때, 다른 사람이 Push 한 후에 Push 하려고 하면 Push 할 수 없다. 먼저 다른 사람이 작업한 것을 가져와서 Merge 한 후에 Push 할 수 있다.\n\n\n\n3-5. 원격 저장소 정보 보기\n\ngit remote show <원격 저장소 이름>\n\n$ git remote show origin\n* remote origin\n  Fetch URL: https://github.com/schacon/ticgit\n  Push  URL: https://github.com/schacon/ticgit\n  HEAD branch: master\n  Remote branches:\n    master                               tracked\n    dev-branch                           tracked\n  Local branch configured for 'git pull':\n    master merges with remote master\n  Local ref configured for 'git push':\n    master pushes to master (up to date)\n\n원격 저장소의 URL과 추적하는 branch를 출력한다. 이 명령은 git pull 명령을 실행할 때 master branch와 Merge할 branch가 무엇인지 보여준다. git pull 명령은 원격 저장소 branch의 데이터를 모두 가져오고 나서 자동으로 Merge할 것이다.\n\n\n\n3-6. 원격 저장소 이름 바꾸기, 삭제하기\n\ngit remote rename <기존 원격 저장소 이름> <바꿀 원격 저장소 이름>\ngit remote remove <원격 저장소 이름>"
  },
  {
    "objectID": "posts/2022-06-13-git_cheatsheet.html#다중-계정-사용",
    "href": "posts/2022-06-13-git_cheatsheet.html#다중-계정-사용",
    "title": "Git CheatSheet",
    "section": "4. 다중 계정 사용",
    "text": "4. 다중 계정 사용\n\n한 PC에서 업무용 repository 접근계정, 개인용 repository 접근계정을 분리해서 사용하고 싶을 때\n윈도우의 경우, C:{계정명} 에 .gitconfig가 존재한다.\n.gitconfig에 아래 항목 추가\n\n[includeIf \"gitdir/i:C:/Code/limyj0708_code_archive/\"]\n    path = .gitconfig_personal.config\n\nC:/Code/limyj0708_code_archive/ 아래에 있는 repository에 접근 시에는, .gitconfig_personal.config의 정보를 사용하겠다는 의미이다.\nC:{계정명} 에 .gitconfig_personal.config를 만들고, users 항목을 입력한다.\n\n[user]\n    email = limyj0708@gmail.com\n    name = limyj0708\n\n확인해보면, 원하는 대로 잘 된다.\n\n$ git config --show-origin user.email\nfile:C:/Users/limyj0708/.gitconfig_personal.config      limyj0708@gmail.com"
  },
  {
    "objectID": "posts/2022-07-05-Ubuntu에 새 유저 SSH key 추가.html",
    "href": "posts/2022-07-05-Ubuntu에 새 유저 SSH key 추가.html",
    "title": "Ubuntu 새 유저 SSH key 추가",
    "section": "",
    "text": "Oracle Free tier에서 Ubuntu instance를 만들면, 기본 계정명이 ubuntu다.\n\n계정을 새로 만들면, 처음에 생성했던 SSH key로는 접속이 되지 않는다.\n\nkey를 새로 만든 후에, instance에 등록해보자.\n\n\n\n\n\nsudo adduser limyj0708 : limyj0708 계정 생성\n\npassword 설정 진행\n몇 가지 정보 적당히 씀 (Full_name 등)\n\nsudo usermod -aG sudo limyj0708 : sudo 그룹을 limyj0708에 추가하여 sudo 명령어를 사용 가능하게 세팅\n\n\n\n\n ssh-keygen -t rsa -N \"원하는 password\" -b 2048 -C \"원하는 comment\" -f \"원하는 file path\"\n\nbyte는 최소 2048로 진행\nfile path에 지정한 경로, 이름으로 공개키와 비밀키가 생성된다.\n\n\n\n\n\nlocal이 리눅스면 ssh-copy-id를 쓰고, 윈도우에서도 해당 명령어에 대응하는 powershell 명령어가 있던데, 솔직히 잘 안 되었다. 권한 문제 뜨고.\n그래서 수동으로 추가하기로 한다.\n\n\n새로 생성한 계정에서는 처음에 /.ssh 폴더가 없다. 생성해줘야 한다.\nubuntu 계정으로 접속\n\nsudo su : root 계정으로 변경\nmkdir /home/limyj0708/.ssh : .ssh 폴더 생성\nnano /home/limyj0708/.ssh/authorized_keys : authorized_keys 파일도 없어서 새로 생성된다.\n\n이제 위에서 생성했던 공개키의 내용을 붙여넣고 저장한다.\n\n\n\n\n\n\n\nOpenssh 윈도우에서도 잘 되니까, Powershell에서 아래 명령어로 접속한다.\n\nssh -i \"비밀키 경로\" limyj0708@서버ip\n\nProfit!"
  },
  {
    "objectID": "posts/2022-10-25-Bigquery 7일 연속 미접속 시작일 쉽게 추출하기.html",
    "href": "posts/2022-10-25-Bigquery 7일 연속 미접속 시작일 쉽게 추출하기.html",
    "title": "Bigquery_7일 연속 미접속 시작일 쉽게 추출하기",
    "section": "",
    "text": "DECLARE date_array_base ARRAY<DATE>;\nDECLARE power_array_base ARRAY<INT64>;\nSET date_array_base = GENERATE_DATE_ARRAY(DATE('2022-08-16'), DATE('2022-10-23'), INTERVAL 1 DAY);\nSET power_array_base = GENERATE_ARRAY(0, 340000, 20000);\n\nwith abuse_list as ( --어뷰징 유저 목록\n  SELECT accountId\n  FROM `___`\n  WHERE acc_buy_krw IS NULL OR acc_buy_krw < power(10,5)\n\n), raw_sum_sales as ( #누적과금액\n  SELECT accountId, sum(won) as total_sales\n  FROM `___`\n  WHERE b_date < DATE(2022, 10, 23)\n    AND accountId NOT IN (SELECT accountId FROM abuse_list)\n  group by accountId\n\n), max_power_daily as ( --일자별 누적 최고전투력\n  SELECT b_date, accountId, max_power\n  FROM `___`\n  WHERE accountId NOT IN (SELECT accountId FROM abuse_list)\n\n), acc_created_date as ( --계정 생성일\n  SELECT accountId, EXTRACT(DATE FROM created_date) AS c_date\n  FROM `___`\n  WHERE b_date = DATE(2022, 10, 23)\n    AND accountId NOT IN (SELECT accountId FROM abuse_list)\n\n), total_logs_score_connect AS ( --로비 커넥트 로그\n  SELECT * FROM (\n    SELECT EXTRACT(DATE FROM date_time) AS b_date, account_id\n    FROM `___`\n  ) WHERE b_date > DATE(2022, 08, 15)\n    AND account_id NOT IN (SELECT accountId FROM abuse_list)\n\n), account_id_connect_date_array AS ( --유저 별 접속일자 array 만들고, c_date(계정 생성일) 붙임\n  SELECT account_id, date_array, acd.c_date\n  FROM (\n    SELECT account_id, ARRAY_AGG(b_date ORDER BY b_date) AS date_array\n    FROM (\n      SELECT DISTINCT b_date, account_id\n      FROM total_logs_score_connect\n    )\n    GROUP BY account_id\n  ) base\n  LEFT JOIN acc_created_date acd ON acd.accountId = base.account_id\n\n), connect_date_max_power AS ( --접속한 날의 누적 최고전투력\n  SELECT base.account_id, days, mpd.max_power\n  FROM (\n    SELECT account_id, days\n    FROM account_id_connect_date_array, UNNEST(date_array) AS days\n    WHERE ARRAY_LENGTH(date_array) > 14 --14일 초과 접속한 유저 대상\n  ) base\n  LEFT JOIN max_power_daily mpd ON (mpd.accountId = base.account_id AND mpd.b_date = base.days)\n  WHERE days < DATE(2022, 10, 24)\n  ORDER BY account_id, days\n\n), add_gap_day_3_avg_last AS ( --접속일 기준, 3일 전투력 이동평균과 3일 전투력 이동평균의 일자별 변화량 계산\n  SELECT account_id, days, max_power, day_3_avg\n    ,(day_3_avg - LAG(day_3_avg) over(partition by account_id order by days)) AS gap_day_3_avg_last\n  FROM (\n    SELECT account_id, days, max_power\n      ,avg(max_power) over(partition by account_id order by days rows between 3 preceding and current row) as day_3_avg\n    FROM connect_date_max_power\n  )\n  ORDER BY account_id, days\n\n), get_first_stag_start_date AS ( --최초로 4접속일 연속 전투력이 같은 날\n  SELECT account_id, days, max_power, day_3_avg, gap_day_3_avg_last\n  FROM (\n    SELECT account_id, days, max_power, day_3_avg, gap_day_3_avg_last\n      ,ROW_NUMBER() OVER(PARTITION BY account_id ORDER BY days) num\n    FROM (\n      SELECT account_id, days, max_power, day_3_avg, gap_day_3_avg_last\n      FROM add_gap_day_3_avg_last\n      WHERE gap_day_3_avg_last = 0\n    )\n  ) WHERE num = 1\n  ORDER BY account_id, days\n  \n), add_connect_check_array AS (\n  SELECT cda.account_id, ssd.days\n    ,(\n      SELECT ARRAY (\n        SELECT\n          CASE\n            WHEN day < ssd.days THEN \"3\" --최초 4접속일 연속 전투력 같은 날 보다 이전 날짜는 3 \n            WHEN day IN UNNEST(date_array) THEN \"1\" --접속일은 1\n            WHEN day < c_date THEN \"2\" --계정 생성일보다 이전날짜는 2\n            ELSE \"0\" --접속 안했으면 0\n            END AS element\n        FROM UNNEST(date_array_base) as day\n      )\n    ) AS connect_check_array\n    ,date_array\n  FROM account_id_connect_date_array cda\n  LEFT JOIN get_first_stag_start_date ssd ON ssd.account_id = cda.account_id\n  WHERE ARRAY_LENGTH(date_array) > 14 --14일 초과 접속한 유저 대상\n\n), add_leaving_position AS (\n  SELECT account_id, array_string, STRPOS(array_string, '0000000') AS leaving_pos --7일 연속 최초 미접속 확인부분\n  FROM (\n    SELECT account_id, connect_check_array, array_to_string(connect_check_array, '') AS array_string\n    FROM add_connect_check_array\n  )\n\n), get_leaving_date AS (\n  SELECT account_id, DATE_ADD(DATE(2022,08,16), INTERVAL leaving_pos-1 DAY) AS leaving_date --8/16이 위치 1이기 때문에, leaving_pos에서 1을 빼줌\n  FROM add_leaving_position\n  WHERE leaving_pos != 0 --7일 연속 미접속이 없는 계정은 제외\n\n)\n\nSELECT ssd.account_id, days, max_power, day_3_avg, gap_day_3_avg_last, gld.leaving_date, DATE_DIFF(gld.leaving_date, days, DAY) AS gap,\n  CASE\n    WHEN max_power BETWEEN power_array_base[OFFSET(0)] AND power_array_base[OFFSET(1)] THEN \"01. 0 ~ 20000\"\n    WHEN max_power BETWEEN power_array_base[OFFSET(1)]+1 AND power_array_base[OFFSET(2)] THEN \"02. 20001 ~ 40000\"\n    WHEN max_power BETWEEN power_array_base[OFFSET(2)]+1 AND power_array_base[OFFSET(3)] THEN \"03. 40001 ~ 60000\"\n    WHEN max_power BETWEEN power_array_base[OFFSET(3)]+1 AND power_array_base[OFFSET(4)] THEN \"04. 60001 ~ 80000\"\n    WHEN max_power BETWEEN power_array_base[OFFSET(4)]+1 AND power_array_base[OFFSET(5)] THEN \"05. 80001 ~ 100000\"\n    WHEN max_power BETWEEN power_array_base[OFFSET(5)]+1 AND power_array_base[OFFSET(6)] THEN \"06. 100001 ~ 120000\"\n    WHEN max_power BETWEEN power_array_base[OFFSET(6)]+1 AND power_array_base[OFFSET(7)] THEN \"07. 120001 ~ 140000\"\n    WHEN max_power BETWEEN power_array_base[OFFSET(7)]+1 AND power_array_base[OFFSET(8)] THEN \"08. 140001 ~ 160000\"\n    WHEN max_power BETWEEN power_array_base[OFFSET(8)]+1 AND power_array_base[OFFSET(9)] THEN \"09. 160001 ~ 180000\"\n    WHEN max_power BETWEEN power_array_base[OFFSET(9)]+1 AND power_array_base[OFFSET(10)] THEN \"10. 180001 ~ 200000\"\n    WHEN max_power BETWEEN power_array_base[OFFSET(10)]+1 AND power_array_base[OFFSET(11)] THEN \"11. 200001 ~ 220000\"\n    WHEN max_power BETWEEN power_array_base[OFFSET(11)]+1 AND power_array_base[OFFSET(12)] THEN \"12. 220001 ~ 240000\"\n    WHEN max_power BETWEEN power_array_base[OFFSET(12)]+1 AND power_array_base[OFFSET(13)] THEN \"13. 240001 ~ 260000\"\n    WHEN max_power BETWEEN power_array_base[OFFSET(13)]+1 AND power_array_base[OFFSET(14)] THEN \"14. 260001 ~ 280000\"\n    WHEN max_power BETWEEN power_array_base[OFFSET(14)]+1 AND power_array_base[OFFSET(15)] THEN \"15. 280001 ~ 300000\"\n    WHEN max_power BETWEEN power_array_base[OFFSET(15)]+1 AND power_array_base[OFFSET(16)] THEN \"16. 300001 ~ 320000\"\n    WHEN max_power BETWEEN power_array_base[OFFSET(16)]+1 AND power_array_base[OFFSET(17)] THEN \"17. 320001 ~ 340000\"\n    WHEN max_power > power_array_base[OFFSET(17)] THEN \"18. 340001 ~ \"\n    ELSE \"None\"\n  END AS end_power_cate\n  ,CASE\n    WHEN total_sales IS NULL THEN '1. 무과금 (0원)'\n    WHEN total_sales > 0 AND total_sales < 1300 THEN '2. 베이직 (1200원)'\n    WHEN total_sales BETWEEN 1301 AND 22000 THEN '3. 소과금' \n    WHEN total_sales BETWEEN 22001 AND 60000 THEN '4. 중과금'\n    WHEN total_sales BETWEEN 60001 AND 240000 THEN '5. 중고과금'\n    WHEN total_sales BETWEEN 240001 AND 500000 THEN '6. 고과금'\n    WHEN total_sales BETWEEN 500001 AND 1000000 THEN '7. VIP'\n    WHEN total_sales BETWEEN 1000001 AND 10000000 THEN '8. VVIP'\n    WHEN total_sales > 10000000 THEN '9. 고래유저'\n    ELSE \"None\"\n  END AS total_sales_cate\nFROM get_first_stag_start_date ssd\nLEFT JOIN get_leaving_date gld ON gld.account_id = ssd.account_id\nLEFT JOIN raw_sum_sales rss ON rss.accountId = ssd.account_id\nWHERE leaving_date IS NOT NULL"
  },
  {
    "objectID": "posts/2022-10-29-Bigquery_Array, Struct 실전사용.html",
    "href": "posts/2022-10-29-Bigquery_Array, Struct 실전사용.html",
    "title": "Bigquery_Array, Struct 조합 사용과 Cartesian Product",
    "section": "",
    "text": "이런 일을 한 이유\n\n지역 던전 별, 이탈 유무 별, 누적과금대 별 지역 던전 클리어 유저 수를 구해야 함\n던전 로그에서 지역 던전 도달 유저 수를 그냥 구하면 도달하지 못한 지역 던전은 아예 출력이 되질 않음. 도달 로그 자체가 없을 것이므로.\n\n해당 유저 세그먼트에서 아무도 도달하지 못한 지역 던전이 있으면, 테이블이 쥐가 파먹은 것 처럼 중간이 비게 된다.\n아무도 도달하지 못한 지역 던전은 값이 0으로 뜨게 하고 싶다.\n\n그래서 생각한 것이, 미리 모든 카테고리를 곱집합(cartesian product)한 결과물을 기반 테이블로 만들어 두고, 지역 던전 도달 유저 수를 LEFT JOIN으로 기반 테이블에 붙이는 방법임.\n\n\nDECLARE leaving_check ARRAY<INT64>;\nDECLARE acc_sales ARRAY<STRING>;\nDECLARE region_quest ARRAY<STRUCT<map_key INT64, map_name STRING, quest_act INT64, quest_chapter INT64, quest_id INT64, quest_name STRING>>;\n-- STRUCT 정의 할 때 Alias를 붙여주지 않으면 supertype 에러가 발생함\n\nSET leaving_check = [0,1];\nSET acc_sales = [\n  '1. 무과금 (0원)'\n  ,'2. 베이직 (1200원)'\n  ,'3. 소과금' \n  ,'4. 중과금'\n  ,'5. 중고과금'\n  ,'6. 고과금'\n  ,'7. VIP'\n  ,'8. VVIP'\n  ,'9. 고래유저'\n];\nSET region_quest = [ -- SET에서는 alias를 붙여주지 않아도 되지만, 붙여주면 쿼리 속도가 훨씬 빨라졌음.\n    STRUCT(dummy_key_1111 AS map_key, \"dummy_name_1111\" AS map_name, 1 AS quest_act, 13 AS quest_chapter, dummy_id_1111 AS quest_id, \"dummy_q_name_1111\" AS quest_name)\n    ,STRUCT(dummy_key_1112 AS map_key, \"dummy_name_1112\" AS map_name, 1 AS quest_act, 24 AS quest_chapter, dummy_id_1112 AS quest_id, \"dummy_q_name_1112\" AS quest_name)\n    ,STRUCT(dummy_key_1113 AS map_key, \"dummy_name_1113\" AS map_name, 1 AS quest_act, 29 AS quest_chapter, dummy_id_1113 AS quest_id, \"dummy_q_name_1113\" AS quest_name)\n    ,STRUCT(dummy_key_1114 AS map_key, \"dummy_name_1114\" AS map_name, 1 AS quest_act, 37 AS quest_chapter, dummy_id_1114 AS quest_id, \"dummy_q_name_1114\" AS quest_name)\n    ,STRUCT(dummy_key_1115 AS map_key, \"dummy_name_1115\" AS map_name, 1 AS quest_act, 33 AS quest_chapter, dummy_id_1115 AS quest_id, \"dummy_q_name_1115\" AS quest_name)\n    ,STRUCT(dummy_key_1116 AS map_key, \"dummy_name_1116\" AS map_name, 2 AS quest_act, 5 AS quest_chapter, dummy_id_1116 AS quest_id, \"dummy_q_name_1116\" AS quest_name)\n    ,STRUCT(dummy_key_1117 AS map_key, \"dummy_name_1117\" AS map_name, 2 AS quest_act, 11 AS quest_chapter, dummy_id_1117 AS quest_id, \"dummy_q_name_1117\" AS quest_name)\n    ,STRUCT(dummy_key_1118 AS map_key, \"dummy_name_1118\" AS map_name, 2 AS quest_act, 16 AS quest_chapter, dummy_id_1118 AS quest_id, \"dummy_q_name_1118\" AS quest_name)\n    ,STRUCT(dummy_key_1119 AS map_key, \"dummy_name_1119\" AS map_name, 2 AS quest_act, 20 AS quest_chapter, dummy_id_1119 AS quest_id, \"dummy_q_name_1119\" AS quest_name)\n    ,STRUCT(dummy_key_1120 AS map_key, \"dummy_name_1120\" AS map_name, 2 AS quest_act, 23 AS quest_chapter, dummy_id_1120 AS quest_id, \"dummy_q_name_1120\" AS quest_name)\n    ,STRUCT(dummy_key_1121 AS map_key, \"dummy_name_1121\" AS map_name, 3 AS quest_act, 20 AS quest_chapter, dummy_id_1121 AS quest_id, \"dummy_q_name_1121\" AS quest_name)\n    ,STRUCT(dummy_key_1122 AS map_key, \"dummy_name_1122\" AS map_name, 3 AS quest_act, 25 AS quest_chapter, dummy_id_1122 AS quest_id, \"dummy_q_name_1122\" AS quest_name)\n    ,STRUCT(dummy_key_1123 AS map_key, \"dummy_name_1123\" AS map_name, 3 AS quest_act, 5 AS quest_chapter, dummy_id_1123 AS quest_id, \"dummy_q_name_1123\" AS quest_name)\n    ,STRUCT(dummy_key_1124 AS map_key, \"dummy_name_1124\" AS map_name, 3 AS quest_act, 10 AS quest_chapter, dummy_id_1124 AS quest_id, \"dummy_q_name_1124\" AS quest_name)\n    ,STRUCT(dummy_key_1125 AS map_key, \"dummy_name_1125\" AS map_name, 3 AS quest_act, 29 AS quest_chapter, dummy_id_1125 AS quest_id, \"dummy_q_name_1125\" AS quest_name)\n    ,STRUCT(dummy_key_1126 AS map_key, \"dummy_name_1126\" AS map_name, 3 AS quest_act, 32 AS quest_chapter, dummy_id_1126 AS quest_id, \"dummy_q_name_1126\" AS quest_name)\n    ,STRUCT(dummy_key_1127 AS map_key, \"dummy_name_1127\" AS map_name, 3 AS quest_act, 35 AS quest_chapter, dummy_id_1127 AS quest_id, \"dummy_q_name_1127\" AS quest_name)\n    ,STRUCT(dummy_key_1128 AS map_key, \"dummy_name_1128\" AS map_name, 4 AS quest_act, 6 AS quest_chapter, dummy_id_1128 AS quest_id, \"dummy_q_name_1128\" AS quest_name)\n    ,STRUCT(dummy_key_1129 AS map_key, \"dummy_name_1129\" AS map_name, 4 AS quest_act, 12 AS quest_chapter, dummy_id_1129 AS quest_id, \"dummy_q_name_1129\" AS quest_name)\n    ,STRUCT(dummy_key_1130 AS map_key, \"dummy_name_1130\" AS map_name, 4 AS quest_act, 19 AS quest_chapter, dummy_id_1130 AS quest_id, \"dummy_q_name_1130\" AS quest_name)\n    ,STRUCT(dummy_key_1131 AS map_key, \"dummy_name_1131\" AS map_name, 4 AS quest_act, 24 AS quest_chapter, dummy_id_1131 AS quest_id, \"dummy_q_name_1131\" AS quest_name)\n    ,STRUCT(dummy_key_1132 AS map_key, \"dummy_name_1132\" AS map_name, 4 AS quest_act, 31 AS quest_chapter, dummy_id_1132 AS quest_id, \"dummy_q_name_1132\" AS quest_name)\n    ,STRUCT(dummy_key_1133 AS map_key, \"dummy_name_1133\" AS map_name, 4 AS quest_act, 37 AS quest_chapter, dummy_id_1133 AS quest_id, \"dummy_q_name_1133\" AS quest_name)\n    ,STRUCT(dummy_key_1134 AS map_key, \"dummy_name_1134\" AS map_name, 4 AS quest_act, 42 AS quest_chapter, dummy_id_1134 AS quest_id, \"dummy_q_name_1134\" AS quest_name)\n    ,STRUCT(dummy_key_1135 AS map_key, \"dummy_name_1135\" AS map_name, 4 AS quest_act, 45 AS quest_chapter, dummy_id_1135 AS quest_id, \"dummy_q_name_1135\" AS quest_name)\n    ,STRUCT(dummy_key_1136 AS map_key, \"dummy_name_1136\" AS map_name, 4 AS quest_act, 48 AS quest_chapter, dummy_id_1136 AS quest_id, \"dummy_q_name_1136\" AS quest_name)\n    ,STRUCT(dummy_key_1137 AS map_key, \"dummy_name_1137\" AS map_name, 5 AS quest_act, 6 AS quest_chapter, dummy_id_1137 AS quest_id, \"dummy_q_name_1137\" AS quest_name)\n    ,STRUCT(dummy_key_1138 AS map_key, \"dummy_name_1138\" AS map_name, 5 AS quest_act, 10 AS quest_chapter, dummy_id_1138 AS quest_id, \"dummy_q_name_1138\" AS quest_name)\n    ,STRUCT(dummy_key_1139 AS map_key, \"dummy_name_1139\" AS map_name, 5 AS quest_act, 15 AS quest_chapter, dummy_id_1139 AS quest_id, \"dummy_q_name_1139\" AS quest_name)\n    ,STRUCT(dummy_key_1140 AS map_key, \"dummy_name_1140\" AS map_name, 5 AS quest_act, 19 AS quest_chapter, dummy_id_1140 AS quest_id, \"dummy_q_name_1140\" AS quest_name)\n    ,STRUCT(dummy_key_1141 AS map_key, \"dummy_name_1141\" AS map_name, 5 AS quest_act, 25 AS quest_chapter, dummy_id_1141 AS quest_id, \"dummy_q_name_1141\" AS quest_name)\n    ,STRUCT(dummy_key_1142 AS map_key, \"dummy_name_1142\" AS map_name, 5 AS quest_act, 30 AS quest_chapter, dummy_id_1142 AS quest_id, \"dummy_q_name_1142\" AS quest_name)\n    ,STRUCT(dummy_key_1143 AS map_key, \"dummy_name_1143\" AS map_name, 5 AS quest_act, 35 AS quest_chapter, dummy_id_1143 AS quest_id, \"dummy_q_name_1143\" AS quest_name)\n    ,STRUCT(dummy_key_1144 AS map_key, \"dummy_name_1144\" AS map_name, 5 AS quest_act, 38 AS quest_chapter, dummy_id_1144 AS quest_id, \"dummy_q_name_1144\" AS quest_name)\n    ,STRUCT(dummy_key_1145 AS map_key, \"dummy_name_1145\" AS map_name, 5 AS quest_act, 41 AS quest_chapter, dummy_id_1145 AS quest_id, \"dummy_q_name_1145\" AS quest_name)\n  ];\n\nWITH base_df AS ( -- 이탈유저유무, 누적과금액별 지역던전 베이스 테이블\n  SELECT map.*, leaving, acc_sales_cate\n  FROM UNNEST(region_quest) AS map, UNNEST(leaving_check) AS leaving, UNNEST(acc_sales) AS acc_sales_cate\n)\n\nSELECT * FROM base_df\n\n아래와 같이 합집합 테이블이 출력된다.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRow\nmap_key\nmap_name\nquest_act\nquest_chapter\nquest_id\nquest_name\nleaving\nacc_sales_cate\n\n\n\n\n1\ndummy_key_1111\ndummy_name_1111\n1\n13\ndummy_id_1111\ndummy_q_name_1111\n0\n1. 무과금 (0원)\n\n\n2\ndummy_key_1111\ndummy_name_1111\n1\n13\ndummy_id_1111\ndummy_q_name_1111\n0\n2. 베이직 (1200원)\n\n\n3\ndummy_key_1111\ndummy_name_1111\n1\n13\ndummy_id_1111\ndummy_q_name_1111\n0\n3. 소과금\n\n\n4\ndummy_key_1111\ndummy_name_1111\n1\n13\ndummy_id_1111\ndummy_q_name_1111\n0\n4. 중과금\n\n\n5\ndummy_key_1111\ndummy_name_1111\n1\n13\ndummy_id_1111\ndummy_q_name_1111\n0\n5. 중고과금\n\n\n6\ndummy_key_1111\ndummy_name_1111\n1\n13\ndummy_id_1111\ndummy_q_name_1111\n0\n6. 고과금\n\n\n7\ndummy_key_1111\ndummy_name_1111\n1\n13\ndummy_id_1111\ndummy_q_name_1111\n0\n7. VIP\n\n\n8\ndummy_key_1111\ndummy_name_1111\n1\n13\ndummy_id_1111\ndummy_q_name_1111\n0\n8. VVIP\n\n\n9\ndummy_key_1111\ndummy_name_1111\n1\n13\ndummy_id_1111\ndummy_q_name_1111\n0\n9. 고래유저\n\n\n10\ndummy_key_1111\ndummy_name_1111\n1\n13\ndummy_id_1111\ndummy_q_name_1111\n1\n1. 무과금 (0원)\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n\n\n잘 join해서 사용하면 된다.\n\nSELECT bd.map_key, bd.map_name, bd.quest_act, bd.quest_chapter, bd.quest_id, bd.quest_name, bd.not_leaving\n  ,bd.acc_sales_cate, IFNULL(ar.total_cnt,0) AS total_cnt, IFNULL(ar.success_cnt,0) AS success_cnt\nFROM base_df bd\nLEFT JOIN agg_result ar ON \n  (ar.map_key = bd.map_key)\n  AND (ar.last_7_day_login = bd.not_leaving)\n  AND (ar.total_sales_cate = bd.acc_sales_cate)\nORDER BY quest_act, quest_chapter"
  },
  {
    "objectID": "posts/2022-10-29-Bigquery_사분위수 구하기.html",
    "href": "posts/2022-10-29-Bigquery_사분위수 구하기.html",
    "title": "Bigquery_사분위수 구하기",
    "section": "",
    "text": "사분위수를 Bigquery에서 미리 구해서 시각화하고 싶을 떄가 있다.\n두 가지 방법이 있는데, 아래와 같다."
  },
  {
    "objectID": "posts/2022-10-29-Bigquery_사분위수 구하기.html#approx_quantiles",
    "href": "posts/2022-10-29-Bigquery_사분위수 구하기.html#approx_quantiles",
    "title": "Bigquery_사분위수 구하기",
    "section": "APPROX_QUANTILES",
    "text": "APPROX_QUANTILES\n\n근사치 집계 함수\n\n근사치 집계 함수는 메모리 사용량과 시간 면에서 확장 가능하지만 정확한 결과가 아닌 근사치 결과를 산출합니다. 이러한 함수는 일반적으로 COUNT(DISTINCT …)와 같은 정확한 집계 함수보다 적은 메모리를 사용하지만 통계상의 불확실성 또한 존재합니다. 따라서 근사치 집계는 선형 메모리 사용량이 비효율적이거나 데이터가 이미 근사치인 대용량 데이터 스트림에 적합합니다.\n\n예를 들어, skill_score라는 수치 컬럼의 사분위수를 구한다고 해 보자.\n\nmin(skill_enhance_score_sum) as score_min\n,APPROX_QUANTILES(skill_enhance_score_sum, 100)[OFFSET(25)] AS q_1 -- 100조각 내서 25번째, Q1\n,APPROX_QUANTILES(skill_enhance_score_sum, 100)[OFFSET(50)] AS median\n,APPROX_QUANTILES(skill_enhance_score_sum, 100)[OFFSET(75)] AS q_3\n,max(skill_enhance_score_sum) as score_max"
  },
  {
    "objectID": "posts/2022-10-29-Bigquery_사분위수 구하기.html#percentile_cont",
    "href": "posts/2022-10-29-Bigquery_사분위수 구하기.html#percentile_cont",
    "title": "Bigquery_사분위수 구하기",
    "section": "PERCENTILE_CONT",
    "text": "PERCENTILE_CONT\n\n탐색 함수\nvalue_expression의 지정된 백분위수 값을 선형 보간으로 계산합니다.\n예를 들어, skill_score라는 수치 컬럼의 사분위수를 구한다고 해 보자.\n\nPERCENTILE_CONT(skill_enhance_score_sum, 0) OVER() as score_min\n,PERCENTILE_CONT(skill_enhance_score_sum, 0.25) OVER() AS q_1\n,PERCENTILE_CONT(skill_enhance_score_sum, 0.5) OVER() AS median\n,PERCENTILE_CONT(skill_enhance_score_sum, 0.75) OVER() AS q_3\n,PERCENTILE_CONT(skill_enhance_score_sum, 1) OVER() as score_max\n\nOVER 절은 윈도우 함수 사용법을 참고하자."
  },
  {
    "objectID": "posts/2022-10-29-Bigquery_사분위수 구하기.html#데이터에-null이-있는-경우에는",
    "href": "posts/2022-10-29-Bigquery_사분위수 구하기.html#데이터에-null이-있는-경우에는",
    "title": "Bigquery_사분위수 구하기",
    "section": "데이터에 NULL이 있는 경우에는?",
    "text": "데이터에 NULL이 있는 경우에는?\n\n각 함수 설명에 NULL이 있는 경우 어떻게 되는지 예시가 나와 있으므로, 참고하여 사용하자.\n근사치 집계 함수\n탐색 함수"
  },
  {
    "objectID": "posts/2022-10-29-Bigquery_사용자 정의 함수(UDF).html",
    "href": "posts/2022-10-29-Bigquery_사용자 정의 함수(UDF).html",
    "title": "Bigquery_사용자 정의 함수(UDF)",
    "section": "",
    "text": "다른 언어에서 함수를 정의, 사용하는 과정과 동일하다.\n엄청 긴 CASE WHEN 구문 같은 것을 등록해 두면, 두고두고 편하게 사용할 수 있다."
  },
  {
    "objectID": "posts/2022-10-29-Bigquery_사용자 정의 함수(UDF).html#함수-등록하기",
    "href": "posts/2022-10-29-Bigquery_사용자 정의 함수(UDF).html#함수-등록하기",
    "title": "Bigquery_사용자 정의 함수(UDF)",
    "section": "함수 등록하기",
    "text": "함수 등록하기\nCREATE OR REPLACE FUNCTION `project_id.dataset.function_name` -- 이 위치에 만들 것\n(money_enum INT64) RETURNS STRING -- 정수 parameter를 받아서 문자열을 반환함\nOPTIONS (description=\"money_enum을 인자로 받아서 money_name 값을 반환하는 함수\") AS (\n    CASE money_enum\n      WHEN 1 THEN \"재화 1\"\n      WHEN 2 THEN \"재화 2\"\n      WHEN 3 THEN \"재화 3\"\n      WHEN 4 THEN \"재화 4\"\n      WHEN 5 THEN \"재화 5\"\n      WHEN 6 THEN \"재화 6\"\n      WHEN 7 THEN \"재화 7\"\n      WHEN 8 THEN \"재화 8\"\n      WHEN 9 THEN \"재화 9\"\n      WHEN 10 THEN \"재화 10\"\n      WHEN 11 THEN \"재화 11\"\n      WHEN 12 THEN \"재화 12\"\n      WHEN 13 THEN \"재화 13\"\n      WHEN 14 THEN \"재화 14\"\n      WHEN 15 THEN \"재화 15\"\n      WHEN 16 THEN \"재화 16\"\n      WHEN 17 THEN \"재화 17\"\n      WHEN 18 THEN \"재화 18\"\n      WHEN 19 THEN \"재화 19\"\n      WHEN 20 THEN \"재화 20\"\n      ELSE \"대응되는 money enum 없음\"\n    END\n);"
  },
  {
    "objectID": "posts/2022-10-29-Bigquery_사용자 정의 함수(UDF).html#함수-사용하기",
    "href": "posts/2022-10-29-Bigquery_사용자 정의 함수(UDF).html#함수-사용하기",
    "title": "Bigquery_사용자 정의 함수(UDF)",
    "section": "함수 사용하기",
    "text": "함수 사용하기\n SELECT b_date, accountId, money_type, amount,\n    `project_id.dataset.function_name`(money_type) AS money_name -- money_type을 넣으면 money_name을 반환하는 UDF\n    FROM `-` \n    where b_date = start_date\n    and money_type NOT IN (3,4,5)\n위와 같이 함수를 호출하여 불러오면 된다."
  },
  {
    "objectID": "posts/2022-11-08-Python 스크립트 Console 유저 입력 받기.html",
    "href": "posts/2022-11-08-Python 스크립트 Console 유저 입력 받기.html",
    "title": "Python 스크립트 Console 유저 입력 받기",
    "section": "",
    "text": "Python 스크립트를 실행 시, Console 창에서 유저의 입력을 받으려면?\n\n\ntext = input(\"아무거나 입력하세요 : \")\nprint(f\"메아리 : {text}\")\n\n아무거나 입력하세요 :  맛있는 거 먹고 싶다\n\n\n메아리 : 맛있는 거 먹고 싶다\n\n\n\n입력되는 값은 기본적으로 string이다.\n\n\ntext1 = input(\"아무거나 입력하세요1 : \")\ntext2 = input(\"아무거나 입력하세요2 : \")\nprint(f\"메아리 : {text1 + text2}\")\nprint(type(text1))\n\n아무거나 입력하세요1 :  1\n아무거나 입력하세요2 :  2\n\n\n메아리 : 12\n<class 'str'>\n\n\n\n다른 자료형으로 쓰려면 형변환을 해야 함\n\n\nint1 = int(input(\"아무거나 입력하세요1 : \"))\nint2 = int(input(\"아무거나 입력하세요2 : \"))\nprint(f\"메아리 : {int1 + int2}\")\n\n아무거나 입력하세요1 :  1\n아무거나 입력하세요2 :  2\n\n\n메아리 : 3\n\n\n\n유저가 잘못된 값을 입력할 때를 대비한 예외처리\n\n\ntry:\n    num = int(input('숫자를 입력하세요: '))\n    print('입력하신 숫자는 : ', num)\n\nexcept ValueError:\n    print('숫자를 넣으라니까?')\n\n숫자를 입력하세요:  커피\n\n\n숫자를 넣으라니까?\n\n\n\n올바른 값을 입력할 때까지 작동하는 예외처리 루프\n\n\nwhile True:\n    try:\n        num = int(input('숫자를 입력하세요: '))\n        print('입력하신 숫자는 : ', num)\n        break\n\n    except ValueError:\n        print('숫자를 넣으라니까?')\n\n숫자를 입력하세요:  커피\n\n\n숫자를 넣으라니까?\n\n\n숫자를 입력하세요:  라떼\n\n\n숫자를 넣으라니까?\n\n\n숫자를 입력하세요:  오미자\n\n\n숫자를 넣으라니까?\n\n\n숫자를 입력하세요:  11\n\n\n입력하신 숫자는 :  11\n\n\n\n한 줄에 여러 값 입력받기\n\n\nname, age, position = input(\"이름, 나이, 직급을 입력하세요.\").split() \n    # 입력값을 쪼갬, 입력값은 스페이스로 구분되어야 함\nprint(\"이름 :\", name)\nprint(\"나이 :\", age)\nprint(\"직급 :\", position)\n\n이름, 나이, 직급을 입력하세요. 홍길돌 35 과장\n\n\n이름 : 홍길돌\n나이 : 35\n직급 : 과장\n\n\n\n리스트를 입력받는다면\n\n\nentered_list = input(\"직원들의 나이를 입력하세요 : \").split()\nprint('직원들이 나이 리스트_문자열 : ',entered_list)\n\nnum_list = list(map(int,entered_list))\n    # map 함수로 리스트의 모든 원소에 대해 int 형변환 시행\nprint('직원들의 나이 리스트_숫자변환: ',num_list)\nprint('평균 나이:', sum(num_list)/len(num_list))\n\n직원들의 나이를 입력하세요 :  24 45 34 37 33 29\n\n\n직원들이 나이 리스트_문자열 :  ['24', '45', '34', '37', '33', '29']\n직원들의 나이 리스트_숫자변환:  [24, 45, 34, 37, 33, 29]\n평균 나이: 33.666666666666664\n\n\n\n여러 줄로 입력받기\n\n\ntotal_input = []\nprint(\"직원들의 이름을 쓰세요 : \")\n\nwhile True:\n    name = input()\n    if name:\n        total_input.append(name)\n    else:\n        break\n        # 아무것도 입력하지 않고 엔터를 누르면 if문에서 false로 처리되어\n        # break를 만나게 됨\n\nprint('입력된 직원들의 이름 목록 :')\nprint(total_input)\n\n직원들의 이름을 쓰세요 : \n\n\n 홍길동\n 둘리\n 마이콜\n \n\n\n입력된 직원들의 이름 목록 :\n['홍길동', '둘리', '마이콜']"
  },
  {
    "objectID": "posts/2022-11-17-리눅스 Shell 명령어 Python 스크립트에서 실행하기 + Crontab.html",
    "href": "posts/2022-11-17-리눅스 Shell 명령어 Python 스크립트에서 실행하기 + Crontab.html",
    "title": "리눅스 Shell 명령어 Python 스크립트에서 실행하기 + Crontab",
    "section": "",
    "text": "“그리고 Crontab으로도 실행해보기”\n\n\n\n\n회사에 업무를 위한 소형 개인 서버로 쓰는 NUC가 있다.\n\nssh로 연결하여 사용\n\n사내 와이파이에 연결되어 있는데, 아주 가끔씩 할당된 IP가 바뀐다.\n이럴 때마다 모니터를 연결해서 ifconfig로 ip 주소를 확인할 수는 없는 노릇이다.\n일주일에 한 번씩, 서버가 나에게 현재 자신의 ip가 뭔지 보내줬으면 좋겠다.\n\n\n\n\n\n\nifconfig [원하는 네트워크 인터페이스명] | grep -Eo '([0-9]{1,3}[\\.]){3}[0-9]{1,3}'\n\ngrep\n\n-E : 표현을 확장 정규 표현식으로 해석\n-o : 매칭되는 문자열만 표시\n\n\n\n\n\nimport subprocess\nimport requests\n\nregex_ipv4 = '([0-9]{1,3}[\\.]){3}[0-9]{1,3}' #ipv4를 추출하는 정규식\nps = subprocess.Popen((\"ifconfig\", \"원하는 네트워크 인터페이스명\"), stdout=subprocess.PIPE)\noutput = subprocess.check_output((\"grep\", \"-Eo\", regex_ipv4), stdin=ps.stdout)\nps.wait()\nipv4_internal = str(output).split('\\\\n')[0][2:]\n# 사내에서 접근 가능한 IP주소만 추출함\n\nTARGET_URL = 'https://notify-api.line.me/api/notify'\nTOKEN = '라인 Notify에서 발급받은 토큰 입력'\n# 요청합니다.\nresponse = requests.post(\n    TARGET_URL,\n    headers={\n    'Authorization': 'Bearer ' + TOKEN\n    },\n    data={\n    'message': f'NUC IP : {ipv4_internal}'\n    }\n)\n\nShell에서처럼 Pipe(|)를 쓸 수 없다.\n\nPopen에 shell=True를 넘겨주면 되긴 하는데, 일반적으로 shell에서 명령을 내리는 것 처럼 별도의 유효성 검사 없이 실행이 되기 때문에 shell injection에 취약하게 된다.\n\n그래서 쪼개서 실행시켜야 한다. Popen으로 ifconfig를 실행시키고, 그 출력값을 check_output에 연결하여 최종 출력값을 만든다.\n추출한 IP를 Line Notify를 통해 라인으로 받는다.\n\n\n\n\n PATH=/usr/bin:/usr/sbin:/sbin:/usr/local/bin\n # ifconfig, grep을 잘 실행시키기 위한 환경 지정\n \n # ssh 접속용 사내 Wifi ipv4 전송용\n # 매주 월요일 오전 10시에 전송 \n00 10 * * 1 /usr/bin/python3.9 /home/limyj0708/Code/ipv4_internal_alarm/ipv4_internal_alarm.py >> /home/limyj0708/Code/ipv4_internal_alarm/cron_log.log 2>&1\n\n\n\n\n지정한 Line Notify 봇을 통해 IP가 잘 날아온다.\n\n\n\n\n\n\nShell=True는 Shell Injection에 취약\nPopen 클래스 개괄\nSubprocess 모듈 사용법"
  }
]