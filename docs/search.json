[
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/2019-10-18-lets-start-docker-through-installing-oracle11g-on-mac.html",
    "href": "posts/2019-10-18-lets-start-docker-through-installing-oracle11g-on-mac.html",
    "title": " Mac에 Oracle DB 설치하기로 Docker 시작하기",
    "section": "",
    "text": "새로운 도구의 필요성은 언제나 갑자기 찾아온다. “오너라, 오라클! 난 Docker 마스터다! 널 맥에 바로 설치해주마!” 같은 상황은 살면서 별로 일어나지 않는다.\n그렇다고 구글링을 해서 나온 명령어들을 그저 복사&붙여넣기 하여 설치하기만 하면, 응용도 안 되고 단지 시간을 쓴 것에 지나지 않게 된다.\n단순한 따라하기를 넘어서, Docker로 Oracle DB를 Mac에 설치하는 과정을 통해 Docker의 개념과 기본 명령어들을 공부해보자. Docker Desktop for mac"
  },
  {
    "objectID": "posts/2019-10-18-lets-start-docker-through-installing-oracle11g-on-mac.html#docker가-뭐요",
    "href": "posts/2019-10-18-lets-start-docker-through-installing-oracle11g-on-mac.html#docker가-뭐요",
    "title": " Mac에 Oracle DB 설치하기로 Docker 시작하기",
    "section": "1. Docker가 뭐요?",
    "text": "1. Docker가 뭐요?\n일단 Nomad Coders의 영상을 본 다음에, 좋은 소개 자료들을 읽어보자. * Docker 공식 문서 / Docker Overview * Docker 공식 문서 / Container 개념 소개 페이지 * 초보를 위한 도커 안내서 - 도커란 무엇인가?\nDocker Overview 페이지와 Wikipedia의 설명을 요약하면, 아래와 같이 정리할 수 있지 않을까?\n’Container’라 불리는 단위로 어플리케이션을 묶고, 인프라로부터 분리하여, 인프라에 종속되지 않는 어플리케이션 실행과 빠른 배포를 가능하게 하는 플랫폼"
  },
  {
    "objectID": "posts/2019-10-18-lets-start-docker-through-installing-oracle11g-on-mac.html#docker의-구조",
    "href": "posts/2019-10-18-lets-start-docker-through-installing-oracle11g-on-mac.html#docker의-구조",
    "title": " Mac에 Oracle DB 설치하기로 Docker 시작하기",
    "section": "2. Docker의 구조",
    "text": "2. Docker의 구조\n\n2-1. Docker Engine\n다음은 Docker Engine의 구조이다. 서버-클라이언트 형식으로 되어 있다.이 Docker Engine 위에서 Container가 돌아가고, 이런저런 관리를 하게 된다.\n\n\n\n\n\nOops, Some network ants ate this pic!\n\n\n\nDaemon 프로세스인 서버가 돌아가고 있다.\nREST API : 프로그램들이 deamon과 통신하고 뭘 해야 할 지 명령할 때, 사용할 수 있는 인터페이스를 명시하는 REST API.\nCLI Client : AWS CLI 처럼 HTTP API WRAPPER로서 기능함. 아래 설명을 잘 읽어 보자. > When you use docker run command to start up a Container, your docker client will translate that command into http API call, sends it to docker daemon, Docker daemon then evaluates the request, talks to underlying os and provisions your Container.\n\n\n\n2-2. Docker Architecture\n\n\n\n\n\nOops, Some network ants ate this pic!\n\n\n\n\nClient가 Daemon에게 요청을 보낸다.\n\nClient와 Daemon은 같은 시스템에서 작동할 수도 있고,\nClient가 별도 서버의 Daemon에 접속할 수도 있다.\n\nClient와 Daemon은 UNIX Socket(로컬에서 돌아갈 때)이나 네트워크 인터페이스를 통해, REST API로 통신한다.\n\n\nThe Docker daemon\n\nDocker daemon(dockerd)는 Docker API의 요청을 받아서, Docker Object(image, Container, network, volume…)들을 관리한다.\nDocker service들을 관리하기 위해 다른 daemon들과 통신할 수도 있음.\n\n\n\nThe Docker client\n\nDocker client(docker) : Docker와 상호작용 하기 위한 가장 주요한 방법! e.g : docker run명령어를 실행하면, client는 dockerd에 이 명령어를 보낸다. docker명령은 Docker API를 사용한다.\n여러 개의 daemon과 통신할 수도 있다.\n\n\n\nDocker registries\n\nDocker image들의 저장장소. Docker Hub라는 Public registy가 제공된다. (image를 찾을 때, 찾는 장소는 Docker Hub가 기본값이다!) Private registry도 사용 가능."
  },
  {
    "objectID": "posts/2019-10-18-lets-start-docker-through-installing-oracle11g-on-mac.html#docker-objects-container-image",
    "href": "posts/2019-10-18-lets-start-docker-through-installing-oracle11g-on-mac.html#docker-objects-container-image",
    "title": " Mac에 Oracle DB 설치하기로 Docker 시작하기",
    "section": "3. Docker objects : Container? image?",
    "text": "3. Docker objects : Container? image?\n\n3-1. Image\n\nImage는 Container를 만들기 위한 명령들이 들어 있는 읽기 전용 템플릿이다.\nImage를 만들기 위해서는 Dockerfile을 만들어야 한다.\n\nDockerfile은 image를 만들고 실행하는데 필요한 단계들을 정의하는 구문들을 담고 있는 파일이다.\nDockerfile의 각 명령들은 image의 각 layer들을 생성한다. Dockerfile을 수정하고 다시 빌드하면, 바꾼 layer만 변경된다. 그래서 다른 가상화 기술에 비해 가볍고 작고 빠르다.\n\nImage는 상태(state)를 가지지 않으며, 변하지 않는다.(Container의 설계도가 상황에 따라 이리저리 변하면 곤란할 것이다.)\nLayer로 구성된 특성 덕분에, 특정 이미지를 약간 수정한 다른 이미지를 쉽게 만들 수 있다.\n\ne.g. Ubuntu 이미지를 기반으로, Ubuntu에 Node.js를 설치한 다른 이미지 생성.\n\n\n위의 설명을 시각화하면 아래처럼 된다. \n\n\n3-2. Container (is a runnable instance of image)\n\n제목에 써 있는 대로, image의 인스턴스. image가 실행된 상태.\nDocker 공식 Container 소개 페이지에 따르면, 아래와 같은 컨셉으로 작동한다. VM은 비교를 위해 언급되어 있다.\n\n\n\n\n\n\nOops, Some network ants ate this pic!\n\n\n\nContainers Containers are an abstraction at the app layer that packages code and dependencies together. Multiple Containers can run on the same machine and share the OS kernel with other Containers, each running as isolated processes in user space. Containers take up less space than VMs (Container images are typically tens of MBs in size), can handle more applications and require fewer VMs and Operating systems.\n\n\nVIRTUAL MACHINES Virtual machines (VMs) are an abstraction of physical hardware turning one server into many servers. The hypervisor allows multiple VMs to run on a single machine. Each VM includes a full copy of an operating system, the application, necessary binaries and libraries - taking up tens of GBs. VMs can also be slow to boot.\n\nContainer의 특징들 * Docker API, CLI로 만들고(create) 시작하고(start) 멈추고(stop) 옮기고(move) 할 수 있다. * 하나 혹은 여러 네트워크에 연결하고, 저장소를 추가하고, Container의 현재 상태를 기반으로 새 image도 만들 수 있다. * 기본적으로는, Container는 다른 Container, 자신의 Host와 잘 분리되어 있으며, 이 격리 상태도 조정할 수 있다. * Container는 (image) + (생성, 실행 시 받는 구성 옵션 값)에 의해 정의된다.(configuration option) * Container가 삭제될 때에는, 영구 저장소에 저장되지 않은 상태 변경 값은 사라진다. 그렇다. 모든 변경 사항은 Container의 R/W Layer에 저장되며, 이 Layer는 Container가 삭제되면 같이 사라진다.\ndocker run 명령어 예시로 알아보는 Container 생성과정 % docker run -i -t ubuntu /bin/bash 위의 명령어를 실행하면, ubuntu Container를 만들고 실행해서 로컬 command-line 세션에 붙이고, Ubuntu의 Bash Shell을 실행한다. 어떤 일이 일어나는걸까? 1. ubuntu image가 로컬에 없으면, Docker가 image를 내가 설정해 둔 registry에서 pull한다. docker pull ubuntu를 직접 입력한 것처럼. (기본 registry는 Docker Hub) 1. Docker가 새 Container를 만든다. docker Container create를 직접 입력한 것처럼. 1. Docker가 읽고 쓰기가 가능한 파일시스템을 Container의 최종 레이어로써 할당한다.(R/W Layer) 이는 Container가 로컬 파일시스템에서 파일과 디렉토리를 만들고 수정할 수 있게 해 준다. 1. 네트워크 옵션을 하나도 주지 않았기 때문에, Docker가 Container를 기본 네트워크에 연결하기 위해 네트워크 인터페이스를 만든다. 이 과정은 IP 주소를 Container에 할당하는 과정이 포함된다. 자연스럽게, Container는 host의 네트워크 연결을 사용하여 외부 네트워크에 접속할 수 있게 된다. 1. Docker가 Container를 켠 후 /bin/bash를 실행한다. Container는 상호작용이 가능하게(interactively) 동작하고 있고, 터미널에 붙어 있기 때문에 (-i,-t 플래그를 넣어서), 키보드로 명령을 입력할 수 있고, 출력을 터미널로 받아볼 수 있다. 1. /bin/bash명령을 끄기 위해 exit를 입력하면, Container는 멈추지만 제거되지는 않는다. 다시 시작하거나 제거할 수 있다.\n\n\n3-3. Services\n(이 내용은 당장 활용할 일이 없다. 이후 사용의 필요가 느껴지면 심도있게 알아보자.) (Swarm에 대해 소개하는 아주 좋은 글) Service는 여러 개의 Docker daemon위에서 Container를 스케일링 할 수 있게 해준다. 이 여러 개의 Docker daemon들은 Swarm이라는, 여러 개의 manager, worker를 가지고 있는 단위로 뭉쳐서 작동한다. Swarm의 각 멤버들은 Docker daemon이고, daemon들은 Docker API를 사용하여 통신한다. Service는 원하는 상태를 정의할 수 있게 해 주는데, 특정 시간에 반드시 접근 가능해야 하는 Service 복제체의 숫자라던가 하는 것이다. 기본적으로, Service는 모든 worker node들에 부하 분산처리된다.(load-balanced) 사용자에게는 Docker service가 하나의 어플리케이션으로 보인다. Docker 1.12버전 이상부터 지원됨!"
  },
  {
    "objectID": "posts/2019-10-18-lets-start-docker-through-installing-oracle11g-on-mac.html#여기까지-읽었을-때의-궁금한-점들",
    "href": "posts/2019-10-18-lets-start-docker-through-installing-oracle11g-on-mac.html#여기까지-읽었을-때의-궁금한-점들",
    "title": " Mac에 Oracle DB 설치하기로 Docker 시작하기",
    "section": "4. 여기까지 읽었을 때의 궁금한 점들",
    "text": "4. 여기까지 읽었을 때의 궁금한 점들\n\n4-1. Daemon이 뭐지?\n\nhttps://en.wikipedia.org/wiki/Daemon_(computing) In multitasking computer operating systems, a daemon (/ˈdiːmən/ or /ˈdeɪmən/)[1] is a computer program that runs as a background process, rather than being under the direct control of an interactive user. 악마는 당신의 등 뒤에서 조용히 돌아가고 있다… :) \n\n\n\n4-2. Rest API가 뭐지?\nREpresentational State Transfer 이 영상 이상으로 잘 설명한 자료가 있을까?내용이 방대하기 때문에, 추후 별도의 정리를 진행해야겠다.  > youtube: https://www.youtube.com/watch?v=RP_f5dMoHFc \n\n\n4-3. Socket이 뭐지?\n프로세스 간 데이터 교환을 위한, 소프트웨어로 작성된 통신 접속점 아래는 Unix Socket(Unix Domain Socket)과 IP Socket에 대한 설명이다. > https://serverfault.com/questions/124517/whats-the-difference-between-unix-socket-and-tcp-ip-socket > A UNIX socket is an inter-process communication mechanism that allows bidirectional data exchange between processes running on the same machine. > IP sockets (especially TCP/IP sockets) are a mechanism allowing communication between processes over the network. In some cases, you can use TCP/IP sockets to talk with processes running on the same computer (by using the loopback interface). > UNIX domain sockets know that they’re executing on the same system, so they can avoid some checks and operations (like routing); which makes them faster and lighter than IP sockets. So if you plan to communicate with processes on the same host, this is a better option than IP sockets.\n아래 글들도 읽어보도록 하자. * 소켓과 포트 * 소켓 프로그래밍 * 소켓 프로그래밍 기초 * 번외 : 서버,클라이언트,호스트"
  },
  {
    "objectID": "posts/2019-10-18-lets-start-docker-through-installing-oracle11g-on-mac.html#그래서-선생-이-명령어가-도대체-뭐요",
    "href": "posts/2019-10-18-lets-start-docker-through-installing-oracle11g-on-mac.html#그래서-선생-이-명령어가-도대체-뭐요",
    "title": " Mac에 Oracle DB 설치하기로 Docker 시작하기",
    "section": "5. 그래서 선생, 이 명령어가 도대체 뭐요?",
    "text": "5. 그래서 선생, 이 명령어가 도대체 뭐요?\n\n5-1. 어떤 명령어들을 사용하였는가\nOracle DB 설치와 실행을 위해 어떤 명령어들을 사용하였는가? (클릭하면 각 명령어 설명 공식문서로 이동함)\n\ndocker search oracle : 기본 registry인 docker hub에서, oracle이라는 단어를 포함한 image를 찾는다. 추천수 순으로 정렬되어 나온다. 그런데 솔직히 Docker Hub에서 검색하는 것이 더 좋다.\ndocker pull oracleinanutshell/oracle-xe-11g : registry에서 image나 repository를 가져온다. 여기서는 추천수가 가장 높았던 oracleinanutshell/oracle-xe-11g 라는 image를 가져왔다. Docker Hub : oracleinanutshell/oracle-xe-11g 설명을 보니 Ubuntu 18.04 LTS에 Oracle xe 11g를 올린 image다.\ndocker run --name oracle-xe-11g -d -p 8080:8080 -p 1521:1521 oracleinanutshell/oracle-xe-11g : docker run [OPTIONS] image [COMMAND] [ARG...] 이런 구조로 되어있다. Container를 생성하여 실행한다. 여기서 사용한 옵션값부터 살펴보자.\n\n--name : Container의 이름을 설정한다. 중복 이름은 허용하지 않음.\n-d : Container를 백그라운드에서 실행하고, Container ID를 출력한다. Background와 Foreground의 차이\n-p : 특정 범위의 포트, 혹은 포트 하나를 host에 publish한다. 여기서는 hostPort:ContainerPort 구조로 사용했다. 즉, Container의 8080포트를 Host의 8080포트에 매핑하는 방화벽 규칙을 만든다. (하필이면 8080, 1521인 이유는, Oracle Listener가 사용하는 포트가 1521이고, XML DB가 8080포트를 사용하기 때문이다.) docker port oracle-xe-11g로 연결된 포트를 확인해보면, 다음과 같이 뜬다.\n\n1521/tcp -> 0.0.0.0:1521\n8080/tcp -> 0.0.0.0:8080\n포트들이 0.0.0.0 (all IPv4 addresses on the local machine)의 동일 포트에 매핑되었다. 이제 Container가 생성된 후 실행되었는데, 그럼 다음엔 뭘 해야 할까? * 참고 공식 문서 : Container networking * 참고 공식 문서 : Docker run reference\ndocker exec -it oracle-xe-11g bash : 외부에서, 실행 중인 Container 안의 명령을 실행한다. oracle-xe-11g Container는 Ubuntu 18.04위에서 Oracle DB를 구동하는 구조이기 때문에, bash shell을 열라는 명령어를 보내 보았다.(bash) 옵션 -it는 뭘까?\n\n-i : 키보드, 화면을 통해 STDIN, STDOUT(표준입력, 표준출력)[설명][설명의 번역]을 열고 유지한다. 명령어 입력, 결과 출력을 위해서 넣어주어야 하는 값이다.\n-t : pseudo-TTY를 할당한다. 터미널 환경을 에뮬레이션 해 주는데, 이 옵션을 입력하지 않으면 터미널 환경이 보이지 않는다.(i와 t값을 넣지 않으면 각각 어떻게 되는가? 는 이 블로그를 참고하자.) 보기만 해서는 잘 기억나지 않을테니, 직접 해보자.\n\n% docker exec oracle-xe-11g bash\n%\n하나도 안 쓰면 바로 종료된다. 입출력도, tty도 활성화되지 않았으니 당연한 결과. % docker exec -t oracle-xe-11g bash   root@a702ae5d7f10:/# sqlplus -t만 쓰면, 첫 번째 명령어 입력까지는 가능하나 그 후의 결과가 출력되지 않는다. STDIN을 열지 않았으니 당연한 결과. % docker exec -i oracle-xe-11g bash   sqlplus   bash: line 1: sqlplus: command not found   ls   bin   boot   dev   ...(이하생략) -i만 쓰면, 터미널 환경이 조성되지 않는다. 그냥 명령어를 입력하면 sqlplus는 command not found 에러가 뜨고, ls는 제대로 출력이 되긴 한다. 응용 프로그램(sqlplus) 실행은 안 되고, 기본 shell command는 제대로 실행되는 건 terminal의 부재 때문이 아닌가 추측해본다. 자세한 이유는 다음에 알아보기로 하자. ``` % docker exec -it oracle-xe-11g bash\nroot@a702ae5d7f10:/# sqlplus\nSQL*Plus: Release 11.2.0.2.0 Production on Sun Oct 27 02:33:24 2019\nCopyright (c) 1982, 2011, Oracle. All rights reserved.\nEnter user-name: system Enter password:\nConnected to: Oracle Database 11g Express Edition Release 11.2.0.2.0 - 64bit Production\nSQL> ``` 제대로 모두 입력하여 sqlplus에 로그인까지 진행하면 이렇게 된다.\n\n도대체 pseudo-TTY가 뭐지? 에 대한 글은 다음 링크를 참고하자. * Bash Shell에 대한 엄청난 gitbook : TTY * 콘솔? 터미널? 쉘?\n- oracleinanutshell/oracle-xe-11g image로부터 Container를 실행하였고, Container에서 sqlplus를 실행해서 로그인도 해 봤다. - 설정을 다 했다. 그런데 컴퓨터 재부팅을 하거나, Docker를 종료했다가 Container를 또 실행하고 싶으면 어떻게 해야 할까?\n-docker start oracle-xe-11g : 하나, 혹은 여러 개의 멈춘 Container를 실행한다. 여기서는 docker run으로 생성한 ‘oracle-xe-11g’ 라는 이름을 붙인 Container를 실행한다.\n\n\n5-2. 뭐 하나 잊어버린 것 같은데..? : Volume\n그런데 여기서 빼먹은 것이 하나 있다. 위에 위험한 설명이 하나 있었던 것 같은데? > Container가 삭제될 때에는, 영구 저장소에 저장되지 않은 상태 변경 값은 사라진다. 그렇다. 모든 변경 사항은 Container의 R/W Layer에 저장되며, 이 Layer는 Container가 삭제되면 같이 사라진다.\nContainer를 영영 없애지 않을 생각이거나, 한 Container의 데이터를 다른 Container와 공유하지 않을 생각이라면 지금까지 입력한 명령어들만으로도 충분하다. 하지만 아니라면? Volume을 사용, 데이터를 Host에 저장하여 안전하게 유지, 공유해보자. 아래 두 링크를 꼭 읽어보자. 초반 부분만 읽어봐도 된다. * Docker 공식 페이지 / Use volumes * Docker 공식 페이지 / About storage drivers\n\nVolumes are the preferred mechanism for persisting data generated by and used by Docker Containers.\n\n\nThe major difference between a Container and an image is the top writable layer. All writes to the Container that add new or modify existing data are stored in this writable layer. When the Container is deleted, the writable layer is also deleted. The underlying image remains unchanged. Because each Container has its own writable Container layer, and all changes are stored in this Container layer, multiple Containers can share access to the same underlying image and yet have their own data state.\n\n그럼, oracle-xe-11g Container 안의 어떤 폴더를 Host의 폴더와 연결해 주어야 할까? Container 안에서 oracle이란 이름을 가진 폴더를 검색해보자.\n$ find / -name oracle -type d  # 전체 폴더에서 oracle 이름을 가진 폴더 검색\n/u01/app/oracle\n오호라, /u01/app/oracle를 연결하면 될 것 같다.\ndocker run문서에 따르면, Volume을 할당하기 위해 다음과 같은 옵션이 필요하다. -v [Host directory]:[Container directory] 이 옵션을 추가하여, Container를 다시 생성해 보자.\ndocker run --name oracle-xe-11g -d -p 8080:8080 -p 1521:1521 -v /Users/youngjinlim/Coding/BigData_Study/SQL/Docker_volume:/u01/app/oracle oracleinanutshell/oracle-xe-11g\n과연 Volume이 잘 Mount 되었을까?\n% docker inspect --format='{{.Mounts}}' oracle-xe-11g\n[{bind  /Users/youngjinlim/Coding/BigData_Study/SQL/Docker_volume /u01/app/oracle   true rprivate}]\n오, 잘 연결된 것 같다. 그런데… DB에 연결할 수가 없었다! 왜지?\nroot@141d12eb18ac:/u01/app/oracle# ls -l\ntotal 0\n어엉? Container 쪽 폴더가 텅 비어버렸다! 아무래도 Host쪽의 폴더로 덮어씌워진 것 같다. 아.. 너무 고통스럽다.. 이에 대한 원인과 해결방법은 이 블로그에서 찾을 수 있었다. * docker volume의 사용방법과 차이점 : !!주의!! 이 블로그 포스팅에선 docker create volume volume_name 라고 썼는데, 이러면 안된다. 왜냐면..\n\nhttps://docs.docker.com/engine/reference/commandline/create/ Description : Create a new Container Usage : docker create [OPTIONS] image [COMMAND] [ARG…] docker create는 Container를 만들 때 쓰는 명령어이기 때문이다. docker volume을 써야 한다.\n\nHost쪽 폴더가 텅 비어있을 때, Container쪽 폴더를 남기려면 아래와 같은 방법을 사용해야 한다.\ndocker volume create volume_name\ndocker run --name oracle-xe-11g -d -v volume_name:/Container/some/where ...(이하생략)\n그럼 실행해 보자.\n% docker volume create oracle-xe-11g_study    # Volume 생성\n% docker volume ls    # 잘 생성되었는지 확인\nDRIVER              VOLUME NAME\nlocal               oracle-xe-11g_study\n% docker run --name oracle-xe-11g_study -d -v oracle-xe-11g_study:/u01/app/oracle -p 8080:8080 -p 1521:1521 oracleinanutshell/oracle-xe-11g\n6a7d5728c9084c9f2c25931a8a7bf1120594776380d5cd8e17d6d15b48604eb6    # 새 Container 생성\n% docker exec -it oracle-xe-11g_study bash    # /u01/app/oracle 폴더 무사한지 확인하러 들어감\nroot@6a7d5728c908:/# cd /u01/app/oracle\nroot@6a7d5728c908:/u01/app/oracle# ls -l    # 결과를 보면 무사함을 알 수 있음\ntotal 24\ndrwxr-x--- 4 oracle dba  4096 Oct 27 03:39 admin\ndrwxrwxr-x 4 oracle dba  4096 Oct 27 03:39 diag\ndrwxr-x--- 3 oracle dba  4096 Oct 27 03:39 fast_recovery_area\ndrwxr-x--- 3 oracle dba  4096 Oct 27 03:39 oradata\ndrwxr-xr-x 3 oracle dba  4096 Oct 27 03:39 oradiag_oracle\ndrwxr-xr-x 3 root   root 4096 Oct 27 03:39 product\nroot@6a7d5728c908:/u01/app/oracle# exit\nexit\n% docker inspect --format='{{.Mounts}}' oracle-xe-11g_study    # Volume 잘 연결 되었는지 확인\n[{volume oracle-xe-11g_study /var/lib/docker/volumes/oracle-xe-11g_study/_data /u01/app/oracle local z true }]\n연결도 잘 됐고, Container 쪽 폴더도 무사하다. ‘CUSTOMERS’ 라는 이름의 테이블을 추가한 후, Container를 새로 생성해서 같은 Volume에 연결했을 경우, 새로 생성한 Container에서도 CUSTOMERS 테이블이 잘 보이는지 확인해 보자.\n\n\n\n\n\nOops, Some network ants ate this pic!\n\n\n% docker run --name oracle-xe-11g_volumetest -d -v oracle-xe-11g_study:/u01/app/oracle -p 8080:8080 -p 1521:1521 oracleinanutshell/oracle-xe-11g\n08ba1459d196d6094b7adc2232d843e430574551cb1ba4c823fae7f85aa8fe36\nyoungjinlim@Youngui-MacBookPro ~ % docker ps\n...중략  NAMES\n        oracle-xe-11g_volumetest\n새로 생성한 Container 하나만 실행 중이다.(oracle-xe-11g_volumetest) 이제 추가했던 테이블이 그대로 있는지 확인해 보자.\n\n\n\n\n\nOops, Some network ants ate this pic!\n\n\n잘 있다.\n일단 Mac에서 Oracle DB를 사용하기 위한 여정은 여기서 끝이다."
  },
  {
    "objectID": "posts/2019-10-18-lets-start-docker-through-installing-oracle11g-on-mac.html#이번에-알아본-것",
    "href": "posts/2019-10-18-lets-start-docker-through-installing-oracle11g-on-mac.html#이번에-알아본-것",
    "title": " Mac에 Oracle DB 설치하기로 Docker 시작하기",
    "section": "이번에 알아본 것",
    "text": "이번에 알아본 것\n\nDocker의 기본적인 개념\nDocker를 개념을 알아보다가 궁금해진 것들\n\n궁금해진 것들 중 너무 큰 주제들이 많았는데, 별도로 정리를 할 필요가 있어 보인다.\n\nOracle Database 11g 설치 중 사용한 명령어들의 의미\n\nDocker 공식문서가 최고다. 공식문서를 보시오"
  },
  {
    "objectID": "posts/2021-09-06-Linux_비밀번호 만료 안 되게 하기.html",
    "href": "posts/2021-09-06-Linux_비밀번호 만료 안 되게 하기.html",
    "title": "Linux 비밀번호 만료 안 되게 하기",
    "section": "",
    "text": "2. 각 명령어 구성품의 의미\n\nchage : 사용자의 패스워드 정보를 관리하는 명령어\n\n-E : 계정의 만료일 설정\n-l : 지정한 계정의 정보를 보여 줌\n-M : 패스워드 최종 변경일로부터 패스워드 변경 없이 사용할 수 있는 최대 일수를 설정\n\n-E에는 -1을 할당 : 영원히 계정을 만료시키지 않음\n-M에는 99999를 할당 : 패스워드 변경 이후 99999일 동안 변경 없이 사용 가능\n위의 명령어 입력 후, sudo chage -l 계정명 으로 계정/패스워드 정보를 확인해 보면 아래와 같다.\nLast password change              : Aug 23, 2021\nPassword expires                  : never\nPassword inactive                 : never\nAccount expires                       : never\nMinimum number of days between password change        : 5\nMaximum number of days between password change        : 99999\nNumber of days of warning before password expires : 7\n\n\n\n3. Reference\n리눅스 패스워드 만료 안되게 하기 - 제타위키 (zetawiki.com) [Linux] chage 명령어 (사용자 패스워드 만기 정보 관리) (tistory.com)"
  },
  {
    "objectID": "posts/2021-09-07-Crontab으로 Python 스크립트 주기적으로 실행하기.html",
    "href": "posts/2021-09-07-Crontab으로 Python 스크립트 주기적으로 실행하기.html",
    "title": "Crontab으로 Python 스크립트 주기적으로 실행하기",
    "section": "",
    "text": "sudo crontab -e : crontab 설정 오픈. 자동으로 root가 작업하는 것으로 인지됨\n설정\n\n경로는 절대경로를 입력해야 제대로 작동\n\nPython 경로도 절대경로로 입력해 줘야 함\n\n시간설정은 아래 링크에서 직관적으로 확인 가능\n\nCrontab.guru - The cron schedule expression editor\n\n\n\n30 8 * * * /usr/local/bin/python3.9 /home/limyj0708/cw_daily_bigquery/cw_daily.py\n\ncron 재시작\n\n재시작해야 적용됨\nservice cron restart\nCentOS일 경우, service crond restart"
  },
  {
    "objectID": "posts/2021-11-08-Python_스크립트_콘솔_유저_입력_받기.html",
    "href": "posts/2021-11-08-Python_스크립트_콘솔_유저_입력_받기.html",
    "title": "Lim's Code Archive",
    "section": "",
    "text": "badges: False\ncomments: true\nauthor: limyj0708\ncategories: [Python]\nPython 스크립트를 실행 시, Console 창에서 유저의 입력을 받으려면?\n\n\ntext = input(\"아무거나 입력하세요 : \")\nprint(f\"메아리 : {text}\")\n\n아무거나 입력하세요 :  맛있는 거 먹고 싶다\n\n\n메아리 : 맛있는 거 먹고 싶다\n\n\n\n입력되는 값은 기본적으로 string이다.\n\n\ntext1 = input(\"아무거나 입력하세요1 : \")\ntext2 = input(\"아무거나 입력하세요2 : \")\nprint(f\"메아리 : {text1 + text2}\")\nprint(type(text1))\n\n아무거나 입력하세요1 :  1\n아무거나 입력하세요2 :  2\n\n\n메아리 : 12\n<class 'str'>\n\n\n\n다른 자료형으로 쓰려면 형변환을 해야 함\n\n\nint1 = int(input(\"아무거나 입력하세요1 : \"))\nint2 = int(input(\"아무거나 입력하세요2 : \"))\nprint(f\"메아리 : {int1 + int2}\")\n\n아무거나 입력하세요1 :  1\n아무거나 입력하세요2 :  2\n\n\n메아리 : 3\n\n\n\n유저가 잘못된 값을 입력할 때를 대비한 예외처리\n\n\ntry:\n    num = int(input('숫자를 입력하세요: '))\n    print('입력하신 숫자는 : ', num)\n\nexcept ValueError:\n    print('숫자를 넣으라니까?')\n\n숫자를 입력하세요:  커피\n\n\n숫자를 넣으라니까?\n\n\n\n올바른 값을 입력할 때까지 작동하는 예외처리 루프\n\n\nwhile True:\n    try:\n        num = int(input('숫자를 입력하세요: '))\n        print('입력하신 숫자는 : ', num)\n        break\n\n    except ValueError:\n        print('숫자를 넣으라니까?')\n\n숫자를 입력하세요:  커피\n\n\n숫자를 넣으라니까?\n\n\n숫자를 입력하세요:  라떼\n\n\n숫자를 넣으라니까?\n\n\n숫자를 입력하세요:  오미자\n\n\n숫자를 넣으라니까?\n\n\n숫자를 입력하세요:  11\n\n\n입력하신 숫자는 :  11\n\n\n\n한 줄에 여러 값 입력받기\n\n\nname, age, position = input(\"이름, 나이, 직급을 입력하세요.\").split() \n    # 입력값을 쪼갬, 입력값은 스페이스로 구분되어야 함\nprint(\"이름 :\", name)\nprint(\"나이 :\", age)\nprint(\"직급 :\", position)\n\n이름, 나이, 직급을 입력하세요. 홍길돌 35 과장\n\n\n이름 : 홍길돌\n나이 : 35\n직급 : 과장\n\n\n\n리스트를 입력받는다면\n\n\nentered_list = input(\"직원들의 나이를 입력하세요 : \").split()\nprint('직원들이 나이 리스트_문자열 : ',entered_list)\n\nnum_list = list(map(int,entered_list))\n    # map 함수로 리스트의 모든 원소에 대해 int 형변환 시행\nprint('직원들의 나이 리스트_숫자변환: ',num_list)\nprint('평균 나이:', sum(num_list)/len(num_list))\n\n직원들의 나이를 입력하세요 :  24 45 34 37 33 29\n\n\n직원들이 나이 리스트_문자열 :  ['24', '45', '34', '37', '33', '29']\n직원들의 나이 리스트_숫자변환:  [24, 45, 34, 37, 33, 29]\n평균 나이: 33.666666666666664\n\n\n\n여러 줄로 입력받기\n\n\ntotal_input = []\nprint(\"직원들의 이름을 쓰세요 : \")\n\nwhile True:\n    name = input()\n    if name:\n        total_input.append(name)\n    else:\n        break\n        # 아무것도 입력하지 않고 엔터를 누르면 if문에서 false로 처리되어\n        # break를 만나게 됨\n\nprint('입력된 직원들의 이름 목록 :')\nprint(total_input)\n\n직원들의 이름을 쓰세요 : \n\n\n 홍길동\n 둘리\n 마이콜\n \n\n\n입력된 직원들의 이름 목록 :\n['홍길동', '둘리', '마이콜']"
  },
  {
    "objectID": "posts/2019-11-11-Asynchronous, Synchronous, Blocking, Non-Blocking.html",
    "href": "posts/2019-11-11-Asynchronous, Synchronous, Blocking, Non-Blocking.html",
    "title": "Lim's Code Archive",
    "section": "",
    "text": "badges: false\ncomments: true\nauthor: limyj0708\ncategories: [Python]\ntoc: true\n\n많은 자료의 산 중에서, 가장 알기 쉽고 직관적으로 설명한 자료는 https://stackoverflow.com/questions/2625493/asynchronous-vs-non-blocking 이 질문의 세 번째 답변이라는 결론을 내렸다. 이 답변의 번역 + 보충 설명을 위한 다른 자료들 + 사족을 섞어서 정리하였다.\n\nsynchronous / asynchronous : 두 모듈 사이의 관계에 대한 표현\nblocking / non-blocking : 모듈 하나의 상태에 대한 표현\n예를 들어,\n\n모듈 X : 나\n모듈 Y : 서점\nX가 Y에게 질문 : C++ primer 책 있나요?\n\n\n1. Blocking * Y가 X에게 답하기 전까지, X는 기다린다. X는 Blocking 상태에 빠진 것이다.\n2. Non-Blocking * Y가 X에게 답하기 전에, X는 다른 일을 할 수 있다. * X가 2분마다 Y가 일을 끝냈는지 확인할까?(Synchronous라면 이렇게 될 것 같다.) 아니면 Y가 다 됐다고 부르면 확인할까?(Asynchronous라면 이렇게 될 것 같다.) 모른다.(= 상관이 없다.) * 우리가 아는 건 X가 Y가 일을 끝내기 전에 다른 일을 할 수 있다는 것 뿐이다. X는 Non-Blocking이다.\n3. Synchronous * Y가 X에게 답하기 전에는, X는 다른 일을 진행하지 않는다 - 라고 설명하고 있는데, 이러면 Blocking과 정의가 같다. 좀 다르게 생각해 보자. * Synchronous에는 중요한 두 가지 키워드가 있다. * 작업의 순서를 맞추는 것. 왜 순서를 맞추냐고? 여러 작업이 동시에(Concurrently) Critical section에 진입하는것을 막기 위해서이기도 하고, 특정 순서에 맞게 작업들을 실행해야 할 필요가 있기(표를 사지도 않고 비행기에 탈 수는 없다) 때문이기도 하다. 작업 순서를 맞출 때 Blocking으로 처리하면 편하기 때문에 Blocking의 개념이 섞여서 등장하는 것 뿐이다. * 작업 순서의 관점에서 설명하는 글 두 개 * https://jins-dev.tistory.com/entry/동기Synchronous-작업과-비동기Asynchronous-작업-그리고-블락Blocking-과-넌블락NonBlocking-의-개념 (이 글의 경우 blocking 부분은 보면 더 헷갈리니 위에만 보자.) * https://medium.com/from-the-scratch/wtf-is-synchronous-and-asynchronous-1a75afd039df * Caller가 Callee의 완료 상태를 확인하는 것. Callee의 완료 여부가 Caller의 다음 작업에 영향을 미치기 때문으로, 작업의 순서를 맞추는 것의 하위 개념이다. * 좋은 예시 : 상사가 와서 어떤 일을 처리하라고 말한다. 그리고 내 등 뒤에서 시체를 노리는 독수리마냥 나를 쳐다보고 있다. “자네가 일을 다 끝낼 때까지 여기서 기다릴 걸세.” * 상태 확인의 관점에서 설명하는 글 : https://homoefficio.github.io/2017/02/19/Blocking-NonBlocking-Synchronous-Asynchronous/ * 이 X,Y 예에서는, Synchronous는 X가 Y에게 책 찾았냐고 물어보고,(caller가 callee의 상태 확인) Y가 책이 있는지 없는지 X에게 알려 준 이후에야, X가 Y에게 “그래서 이 책 얼마죠?” 라고 물어보던가, “그 책 주문 좀 해주세요” 라고 요청할 수 있는 상황인 것이다.(작업 순서) 책이 있는지 확인한 다음에 가격을 물어보던지 주문을 요청하던지 할 수 있으니까. X가 Y가 책 찾는 동안 다른 무언가를 하는 건 상관이 없다. 서점 앞에서 줄넘기를 할 수도 있지 않은가? 책에 관련된 다음 일을 못하는 것 뿐. * 이렇게 봐도 상당히 헷갈리기 때문에, 코드를 보자. 아래의 코드는 Synchronous & Non-Blocking 한 간단한 코드이다.\n# thread X\nwhile (true)\n{\n    msg = recv(Y, NON_BLOCKING_FLAG);\n    if (msg is not empty)\n    {\n        break;\n    }\n    # 이 루프 안에서 다른 작업을 할 수 있다.\n    sleep(2000); // 2 sec\n}\n\n# thread Y\n# prepare the book for X\nsend(X, book);\n\nX가 2초마다 Y가 답을 줬는지 아닌지 확인한다.\nY가 결과를 반환하기 위해 준비 중이어도, X의 while 루프는 계속 돌아가고 그 안에서 다른 작업을 진행할 수 있다. 그래서 Non-Blocking이다.\n하지만, while문을 빠져나가서 다른 작업을 할 수는 없다. 그래서 Synchronous다.\n코드의 예시까지 보면, X가 서점을 떠나지 못한다고 해석할 수 있다. Y가 책을 찾아줘서 서점과 관련된 일을 마치기 전에는, 서점을 떠나서 다른 걸 할 수 없다. 서점 앞에서 줄넘기는 가능해도.(while문 안에서 뭔가 다른 작업)\nBlocking이었다면, X는 아무것도 못하고 기다려야 했겠지만.\n\n4. Asychronous * Y가 X에게 답하기 전에, X는 다른 곳에 가서 다른 일을 할 수 있다. X는 Y가 부르기 전까지 돌아오지 않는다. 이 때 X와 Y는 Asychronous 하다고 말한다. * 여기서도 Synchronous와 같은 두 가지 키워드로 살펴보자. * 작업의 순서가 보장되지 않음 : Asynchronous는 엄밀히 말하면, 작업들이 공통적으로 사용하는 global clock이 없고, 신호나 메세지의 도착 시간이 작업의 신뢰성에 영향을 미치지 않음을 뜻한다. 즉 작업의 순서가 보장되지 않는다.(A,B,C 순서로 실행되었으나 완료도 A,B,C 순서일 것이라 보장할 수 없음) * 신호나 메세지의 도착 시간이 작업의 신뢰성에 영향을 미치지 않음, 즉, 각 작업이 서로 연관되지 않아서 분리될 수 있으며, 작업 지연시간이 큰 경우에 잘 활용될 수 있다. (DB 접근, Http 요청, File I/O 등) * Callee가 자신의 완료 상태를 확인하며, callback으로 Caller에게 자신의 완료를 알림 * 근데 사실 완료 통보를 해도 되고 안해도 된다. 완료 통보가 caller에게 의미가 있느냐 없느냐의 문제이다.(내가 한 질문이다 :D) * 좋은 예시 : 상사가 와서 어떤 일을 처리하라고 말한다. 그리고 다른 일 하러 가버림. 일을 다 끝내면, 나는 상사에게 “나 다함!” 이라고 말한다. * Y에게 책 있냐고 물어본 후에 X가 카페에 가서 커피를 마시기 시작했지만, 책 찾기보다 커피 마시기가 더 빨리 끝날 수도 있다. 이 둘은 완전히 별개의 작업이며, 작업의 순서가 보장되지 않는다.(Asynchronous & Non-Blocking이라면.) 그리고 X는 Y가 X를 부를 때 서점으로 돌아간다.\n각 2개씩의 개념이 있으니, 총 4개의 조합이 나올 것이다. * Synchronous - Blocking * Asynchronous - Blocking * Synchronous - Non-Blocking * Asynchronous - Non-Blocking 이 조합들에 대해서는 여기를 참고하자. 아래 사진이 핵심인데, 출처의 글에서 가져온 사진이다.\n 출처 : https://homoefficio.github.io/2017/02/19/Blocking-NonBlocking-Synchronous-Asynchronous/\n\n번외 내용 : 정리하다 보니 핵심 개념을 직관적으로 알기에는 너무 응용에 가깝다고 생각되었던 내용. 지우기는 아까워서 넘겨두었다.\n\nBlocking I/O : application이 kernal에 I/O 해줘~ 라고 system call을 날린다. kernal이 I/O를 수행하는 동안, application은 아무것도 못 하고 기다린다. I/O가 완료되면 call에 대한 return값으로 원하던 데이터를 받는다. \nNon-Blocking I/O : application이 kernal에 I/O 해줘~ 라고 system call을 날린다. 그림의 recvfrom 함수는, 바로 결과를 return 하는데, 아직 I/O가 완료되지 않았으므로 에러인 EWOULDBLOCK을 return한다. 프로세스는 계속 recvfrom을 call 하게 되고, 데이터가 완료되었으면 그 때 데이터가 return된다. 이렇게 계속 요청하는 걸 polling 이라 한다. \n\ncall에 대한 return을 바로 받아서, application이 제어권을 넘겨받고 다른 일을 진행할 수 있는 것이 중요하다. Blocking I/O와는 정 반대로.\n\nNon-Blocking Algorithm : 어떤 쓰레드의 실패(failure)나 멈춤(suspension)이 다른 쓰레드에 영향을 미치지 않게 하는 알고리즘. 몇몇 상황에서는 이런 알고리즘이 전통적인 Blocking 적용(Lock)의 유용한 대안이 된다.\n\n\n\n\n정확한 용어의 정의를 알아보려고 했다. 하지만 일반적으로는 아래의 대략적인 의미로 사용되는 듯하다. 나는 정확하게 쓰도록 노력해야겠다. * Asynchronous Programming(비동기 프로그래밍): 하나의 요청을 시작한 후, 완료를 기다리지 않고 제어권을 다음 요청으로 넘기는 방식.(Non-Blocking의 의미를 포함) * Synchronous Programming(동기 프로그래밍): 하나의 요청이 처리되는 동안 다른 요청이 처리되지 못하는 방식. 전 요청이 완료되어야 다음 요청 처리가 가능함.(Blocking의 의미를 포함) 참고\n\n\n\n\nhttps://en.wikipedia.org/wiki/Synchronization_(computer_science)#Thread_or_process_synchronization\nhttps://en.wikipedia.org/wiki/Synchronous_circuit\nhttps://en.wikipedia.org/wiki/Asynchronous_system\nhttps://jins-dev.tistory.com/entry/동기Synchronous-작업과-비동기Asynchronous-작업-그리고-블락Blocking-과-넌블락NonBlocking-의-개념\nhttps://medium.com/from-the-scratch/wtf-is-synchronous-and-asynchronous-1a75afd039df\nhttps://homoefficio.github.io/2017/02/19/Blocking-NonBlocking-Synchronous-Asynchronous/\nhttps://ozt88.tistory.com/20\nhttp://www.masterraghu.com/subjects/np/introduction/unix_network_programming_v1.3/ch06lev1sec2.html\nhttps://en.wikipedia.org/wiki/Blocking_(computing)\nhttps://developer.ibm.com/articles/l-async/"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Since this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/2020-02-11-Redshift에 데이터를 적재하는 과정에서 얻은 교훈 & psycopg2.html",
    "href": "posts/2020-02-11-Redshift에 데이터를 적재하는 과정에서 얻은 교훈 & psycopg2.html",
    "title": "Redshift에 데이터를 적재하는 과정에서 얻은 교훈 & psycopg2",
    "section": "",
    "text": "차라리 로컬 DB에서 연산한 후, S3에 올리고 COPY로 집어넣는 것이 훨씬 빠르다.\nRedshift에서 직접 SELECT, INSERT 처리를 하면 : 300만행 추가에 예상 완료시간 4일\n로컬 DB에서 연산 후 COPY로 업로드하면 : 300만 행 연산시간 3.5시간, COPY 업로드 시간 5분\nAWS DW 설명 문서의 말을 들었어야 했는데.\n\n# 같은 코드를 로컬 DB에서 돌리면 3.5시간, Redshift에 연결해서 돌리면 4일\n\nwith psycopg2.connect(**connect_param_local) as con:\n    cur_2 = con.cursor()\n    # fetchall을 사용해서 커서를 재활용하지 않고 커서를 두 개 두는 이유는, redshift의 single-node cluster에서는 fetchall이 지원되지 않기 떄문이다.\n        # InternalError_: Fetch ALL is not supported on single-node clusters.\n        # Please specify the fetch size (maximum 1000 for single-node clusters) \n        # or upgrade to a multi node installation.\n    # 로컬 머신에서 연산하면 fetchall을 사용할 수 있으므로 이렇게 안 해도 되지만,\n    # 코드 수정이 더 번거로웠으므로 그냥 사용하였다.\n    \n    get_companylist_sql = '''select * from target_company_list;'''\n    target_company_list = sqlio.read_sql_query(get_companylist_sql, con)\n    # 1mb도 안되는 작은 테이블이라서 데이터프레임으로 한 번에 받아옴\n\n    for each_code in target_company_list['stock_code']:\n        cur_1 = con.cursor('ss_cursor') # server side cursor\n        # cur_1.itersize = 1000 # redshift single-node cluster에서의 server side cursor의 최대 제한값\n        \n        print(each_code,'_start')\n        predict_start = target_company_list[target_company_list['stock_code'] == each_code]['pre_6m'].values[0] - datetime.timedelta(days=1)\n        predict_end = predict_start - datetime.timedelta(days=1096)\n\n        cur_1.execute(\n            \"\"\"\n            select * from stock_data_2000_2020_raw\n            where (date between %(predict_end)s and %(predict_start)s) and (stock_code = %(stock_code)s);\n            \"\"\",\n            {'predict_end':predict_end.strftime(\"%Y-%m-%d\"),'predict_start':predict_start.strftime(\"%Y-%m-%d\"),'stock_code': each_code}\n        )\n\n        for each_row in cur_1: \n            # next(cur_1)로 한줄씩 불러와서 insert\n            cur_2.execute(\"\"\"insert into stock_data_3years_raw_6m values %s\"\"\", [each_row])\n            # each_row는 tuple이다.\n        con.commit()\n        print(each_code,'_commit complete')\n    cur_2.close()"
  },
  {
    "objectID": "posts/2020-02-11-Redshift에 데이터를 적재하는 과정에서 얻은 교훈 & psycopg2.html#psycopg2",
    "href": "posts/2020-02-11-Redshift에 데이터를 적재하는 과정에서 얻은 교훈 & psycopg2.html#psycopg2",
    "title": "Redshift에 데이터를 적재하는 과정에서 얻은 교훈 & psycopg2",
    "section": "2. psycopg2",
    "text": "2. psycopg2\n\n2-1. Query parameter 전달 시의 유의점\n\npsycopg2 document에는, 빨간색으로 엄청 잘 보이게 써 있는 경고문이 있다.\n\n\nWarning: Never, never, NEVER use Python string concatenation (+) or string parameters interpolation (%) to pass variables to a SQL query string. Not even at gunpoint.\n\n\nSQL Injection의 위험이 있기 때문인데, 어떻게 위험한지는 여기를 참고하자.\n그럼 어떻게 하라는 걸까?\n\n최종적으로 사용한 형식은 아래와 같다.\n\nimport psycopg2\nfrom psycopg2 import sql\n\ncredentials = 'aws_access_key_id=**************;aws_secret_access_key=**************'\ns3_bucket_param = 's3://BUCKET-NAME/FILE-NAME'\n\ncopy_query = sql.SQL(\"\"\"\n        copy {table_name}\n        from %(s3_bucket_param)s\n        credentials %(credentials)s\n        IGNOREHEADER 1\n        CSV;\n    \"\"\").format(table_name = sql.Identifier('TABLE-NAME'))\n\ncur.execute(copy_query, {'s3_bucket_param':s3_bucket_param, 'credentials':credentials})\n\n{} : 테이블 이름 등의 identifier를 받는다. %s 형식으로 identifier를 받으려고 하면, 제대로 인식이 안 되기 때문에 번거롭지만 .foramt(table_name = sql.Identifier('TABLE-NAME'))형식으로 인자를 넘겨야 한다. keyword parameter로 안 해도 되지만, 어떤 자리에 무엇이 들어가는 지 명확하게 정의하는 것을 좋아하므로 몽땅 keyword parameter로 진행하였다.\n\nsql 모듈 설명\n같은 문제로 고통받던 사람의 이슈제기\n\n%(keyword)s : value를 받는다. execute 함수 내부에서 인자로 전달하면 된다. keyword parameter로 정의했을 경우 dictionary로 전달하자.\n\n\n\n\n2-2. Server Side Cursor\n\nClient Side Cursor를 사용하면, 일단 데이터를 클라이언트의 메모리에 저장한 후 거기서 결과값을 계산하게 된다.\n엄청 큰 테이블의 일부를 select 하려고 하면 반드시 메모리 부족으로 문제가 생기게 된다.\nServer Side Cursor를 사용하면, 서버에서 연산 처리 후 결과값만 반환해주기 때문에, 클라이언트 메모리 문제에서 좀 자유로워진다.\n서버 리소스는 더 쓰게 되고, 네트워크 부하는 줄어들게 된다.\n\n# 편리하게도 커서에 이름만 지정해주면 된다!\ncur_1 = con.cursor('ss_cursor')"
  },
  {
    "objectID": "posts/2020-02-11-Redshift에 데이터를 적재하는 과정에서 얻은 교훈 & psycopg2.html#로컬-db-postgresql",
    "href": "posts/2020-02-11-Redshift에 데이터를 적재하는 과정에서 얻은 교훈 & psycopg2.html#로컬-db-postgresql",
    "title": "Redshift에 데이터를 적재하는 과정에서 얻은 교훈 & psycopg2",
    "section": "3. 로컬 DB : postgreSQL",
    "text": "3. 로컬 DB : postgreSQL\n\n3-1. COPY 시의 권한 문제\n\n작업 폴더 내에 있는 CSV 파일을 그대로 COPY하려고 하면, 무조건 permission error가 발생한다.\nDB 서버 사용자가 해당 파일에 접근할 권한이 없기 때문에 발생하는 문제로, 파일이나 폴더의 권한설정을 만져주면 해결된다. 그런데 권한 설정하는 것 보다는 DB 서버 사용자가 접근할 수 있는 폴더에 CSV 파일을 옮기는 것이 더 빠르지 않을까?\n\npostgreSQL을 Mac에서 Homebrew로 설치했다면, /usr/local/var/postgres\n예시 Python       with psycopg2.connect(**connect_param_local) as con:           with con.cursor() as cur:               cur.execute(                   \"\"\"                   COPY stock_data_2000_2020_raw                   from '/usr/local/var/postgres/stock_data_raw_2.csv'                   DELIMITER ','                   CSV HEADER;                   \"\"\")       con.commit()"
  },
  {
    "objectID": "posts/2022-06-13-git_cheatsheet.html",
    "href": "posts/2022-06-13-git_cheatsheet.html",
    "title": "Lim's Code Archive",
    "section": "",
    "text": "안 쓰면 잊어버리는, git 주요 조작법들을 정리\n\n\nbadges: false\ncomments: true\nauthor: limyj0708\ncategories: [git]\nsticky_rank: 2\ntoc: true\n\n교과서 : https://git-scm.com/book/ko/v2\n\n\n\n원하는 폴더로 이동 후 git init\n\n\n\n\n\n\n\nlifecycle.png\n\n\n워킹 디렉토리의 모든 파일은 크게 Tracked(관리대상임)와 Untracked(관리대상이 아님)로 나눈다. Tracked 파일은 이미 스냅샷에 포함돼 있던 파일이다. Tracked 파일은 또 Unmodified(수정하지 않음)와 Modified(수정함) 그리고 Staged(커밋으로 저장소에 기록할) 상태 중 하나이다. 간단히 말하자면 Git이 알고 있는 파일이라는 것이다.\n그리고 나머지 파일은 모두 Untracked 파일이다. Untracked 파일은 워킹 디렉토리에 있는 파일 중 스냅샷에도 Staging Area에도 포함되지 않은 파일이다. 처음 저장소를 Clone 하면 모든 파일은 Tracked이면서 Unmodified 상태이다. 파일을 Checkout 하고 나서 아무것도 수정하지 않았기 때문에 그렇다.\n마지막 커밋 이후 아직 아무것도 수정하지 않은 상태에서 어떤 파일을 수정하면 Git은 그 파일을 Modified 상태로 인식한다. 실제로 커밋을 하기 위해서는 이 수정한 파일을 Staged 상태로 만들고, Staged 상태의 파일을 커밋한다. 이런 라이프사이클을 계속 반복한다.\n\n\n\ngit status\n\n  PS C:\\Users\\limyj0708\\fastpages> git status\nOn branch master\nYour branch is up to date with 'origin/master'.\n\nUntracked files:\n  (use \"git add <file>...\" to include in what will be committed)\n        _notebooks/2022-06-13-git_cheatsheet.ipynb\n\nnothing added to commit but untracked files present (use \"git add\" to track)\n\n2022-06-13-git_cheatsheet.ipynb 파일이 untracked 상태\nGit은 Untracked 파일을 아직 스냅샷(커밋)에 넣어지지 않은 파일이라고 본다. 파일이 Tracked 상태가 되기 전까지는 Git은 절대 그 파일을 커밋하지 않는다. 그래서 일하면서 생성하는 바이너리 파일 같은 것을 커밋하는 실수는 하지 않게 된다.\n\n\n\n\n\ngit add _notebooks/2022-06-13-git_cheatsheet.ipynb\n이후 다시 status를 보면\n\nPS C:\\Users\\limyj0708\\fastpages> git status\nOn branch master\nYour branch is up to date with 'origin/master'.\n\nChanges to be committed:\n  (use \"git restore --staged <file>...\" to unstage)\n        new file:   _notebooks/2022-06-13-git_cheatsheet.ipynb\n\n“Changes to be committed” 에 들어 있는 파일은 Staged 상태라는 것을 의미한다. 커밋하면 git add 를 실행한 시점의 파일이 커밋되어 저장소 히스토리에 남는다.\ngit add 명령은 파일 또는 디렉토리의 경로를 argument로 받는다. 디렉토리면 아래에 있는 모든 파일들까지 재귀적으로 추가한다.\ngit add . 의 경우, .은 현재 디렉토리를 나타내므로, 현재 디렉토리와 하위 디렉토리의 모든 파일들을 Staged 상태로 만든다.\n\n\n\n\n\n2022-06-13-git_cheatsheet.ipynb를 수정한 후에 git status를 해 보면?\n\nOn branch master\nYour branch is up to date with 'origin/master'.\n\nChanges to be committed:\n  (use \"git restore --staged <file>...\" to unstage)\n        new file:   _notebooks/2022-06-13-git_cheatsheet.ipynb\n\nChanges not staged for commit:\n  (use \"git add <file>...\" to update what will be committed)\n  (use \"git restore <file>...\" to discard changes in working directory)\n        modified:   _notebooks/2022-06-13-git_cheatsheet.ipynb\n\n“Changes not staged for commit” 에 있다. 이것은 수정한 파일이 Tracked 상태이지만 아직 Staged 상태는 아니라는 것이다. Staged 상태로 만들려면 git add 명령을 실행해야 한다. git add 명령은 파일을 새로 추적할 때도 사용하고 수정한 파일을 Staged 상태로 만들 때도 사용한다. Merge 할 때 충돌난 상태의 파일을 Resolve 상태로 만들때도 사용한다. add의 의미는 프로젝트에 파일을 추가한다기 보다는 다음 커밋에 추가한다고 받아들이는게 좋다.\ngit add _notebooks/2022-06-13-git_cheatsheet.ipynb후 다시 git status를 해 보자.\n\nPS C:\\Users\\limyj0708\\fastpages> git status\nOn branch master\nYour branch is up to date with 'origin/master'.\n\nChanges to be committed:\n  (use \"git restore --staged <file>...\" to unstage)\n        new file:   _notebooks/2022-06-13-git_cheatsheet.ipynb\n\n“Changes to be committed”에 잘 들어갔는데, 여기서 또 수정을 하고 git status를 하면?\n\nPS C:\\Users\\limyj0708\\fastpages> git status\nOn branch master\nYour branch is up to date with 'origin/master'.\n\nChanges to be committed:\n  (use \"git restore --staged <file>...\" to unstage)\n        new file:   _notebooks/2022-06-13-git_cheatsheet.ipynb\n\nChanges not staged for commit:\n  (use \"git add <file>...\" to update what will be committed)\n  (use \"git restore <file>...\" to discard changes in working directory)\n        modified:   _notebooks/2022-06-13-git_cheatsheet.ipynb\n\nChanges to be committed / Changes not staged for commit에 둘 다 2022-06-13-git_cheatsheet.ipynb이 들어있는 이유\n\n지금 이 시점에서 커밋을 하면 git commit 명령을 실행하는 시점의 버전이 커밋되는 것이 아니라 마지막으로 git add 명령을 실행했을 때의 버전이 커밋된다. 그러니까 git add 명령을 실행한 후에 또 파일을 수정하면 git add 명령을 다시 실행해서 최신 버전을 Staged 상태로 만들어야 한다.\n\n\n\n\n\n\nStaged 상태가 된 파일을 저장소에 기록\n커밋 메세지를 첨부하려면 -m을 붙이고 메시지를 기재\n\ngit commit -m \"modify readme\"\n[main c524828] modify readme\n 1 file changed, 23 insertions(+), 24 deletions(-)\n\nmain branch에 기록되었으며, 체크섬은 c524828\n-a 옵션을 붙이면, add를 해서 staging area에 변경된 파일을 추가하는 작업을 자동으로 처리해 줌\n\ngit commit -a -m \"modify readme\"\n\n\n\n\n\n\ngit rm [파일명 or 디렉토리명]\n\n파일이 실제로 삭제된다.\n\n파일을 그냥 삭제하면, 파일이 unstaged 상태에 있다고 표시된다.\n\n파일을 그냥 삭제하였다면, git rm을 적용해 주어야 staged 상태가 된다.\n그리고 commit을 하면, 더 이상 파일을 추적하지 않는다.\n\n파일을 수정했는데 지우고 싶거나, staging area에 추가했다면, -f 옵션을 주어서 강제로 삭제해야 한다.\nStaging Area에서만 제거하고 디렉토리에 있는 파일은 지우지 않고 남겨두기\n\n–cached 옵션 사용\ngit rm --cached README\n\n한 번에 여러 파일 삭제하기\n\ngit rm log/\\*.log\n\nlog 폴더 내의, .log 확장자인 파일을 모두 삭제함\n\ngit rm \\*~\n\n이름이 ~로 끝나는 파일을 모두 삭제함\n\n\n\n\n\n\n\ngit mv README.md README\n\nREADME.md를 README로 이름 변경\n\n\n\n\n\n\n\n\n$ git remote -v\norigin  https://github.com/limyj0708/bigquery_module.git (fetch)\norigin  https://github.com/limyj0708/bigquery_module.git (push)\n\n\n\n\n\ngit remote add <원격 저장소 이름> <url>\n\nclone 시에는, 단축이름이 자동으로 origin이 된다.\ngit clone https://github.com/limyj0708/fastpages.git : 이런 식으로 할 경우\n\n현재 디렉토리에 추가된 원격 저장소가 있는데, 다른 원격 저장소로 바꾸고 싶을 경우\n\nhttps://shanepark.tistory.com/284 참조\n\n\n\n\n\n\ngit fetch <원격 저장소 이름>\n\n로컬에는 없는데, 원격 저장소에 있는 내용을 모두 가져온다.\n가져오긴 하지만 branch를 merge 하지는 않으므로, 수동으로 merge 해야 한다.\n\ngit pull <원격 저장소 이름>\n\n원격 저장소에 있는 내용을 모두 가져온 후, branch merge까지 알아서 진행한다.\n최초에 내용을 git clone으로 가져왔을 경우, 자동으로 로컬의 master branch가 리모트 저장소의 master branch를 추적하도록 한다(물론 리모트 저장소에 master 브랜치가 있다는 가정에서).\n\n\n\n\n\n\ngit push <원격 저장소 이름> <브랜치 이름>\n\n최초에 git clone으로 가져왔을 경우, 단축이름은 origin이고 branch 이름은 master이므로 아래와 같이 된다.\ngit push origin master\n\n이 명령은 Clone 한 리모트 저장소에 쓰기 권한이 있고, Clone 하고 난 이후 아무도 Upstream 저장소에 Push 하지 않았을 때만 사용할 수 있다. 다시 말해서 Clone 한 사람이 여러 명 있을 때, 다른 사람이 Push 한 후에 Push 하려고 하면 Push 할 수 없다. 먼저 다른 사람이 작업한 것을 가져와서 Merge 한 후에 Push 할 수 있다.\n\n\n\n\n\ngit remote show <원격 저장소 이름>\n\n$ git remote show origin\n* remote origin\n  Fetch URL: https://github.com/schacon/ticgit\n  Push  URL: https://github.com/schacon/ticgit\n  HEAD branch: master\n  Remote branches:\n    master                               tracked\n    dev-branch                           tracked\n  Local branch configured for 'git pull':\n    master merges with remote master\n  Local ref configured for 'git push':\n    master pushes to master (up to date)\n\n원격 저장소의 URL과 추적하는 branch를 출력한다. 이 명령은 git pull 명령을 실행할 때 master branch와 Merge할 branch가 무엇인지 보여준다. git pull 명령은 원격 저장소 branch의 데이터를 모두 가져오고 나서 자동으로 Merge할 것이다.\n\n\n\n\n\ngit remote rename <기존 원격 저장소 이름> <바꿀 원격 저장소 이름>\ngit remote remove <원격 저장소 이름>"
  },
  {
    "objectID": "posts/2020-02-06-로컬 머신에서 AWS Redshift에 접근하기.html",
    "href": "posts/2020-02-06-로컬 머신에서 AWS Redshift에 접근하기.html",
    "title": "로컬 머신에서 AWS Redshift에 접근하기",
    "section": "",
    "text": "Redshift 공식 메뉴얼에서 명료하게 설명하지 않았거나, 없는 내용에 대한 정리이다."
  },
  {
    "objectID": "posts/2020-02-06-로컬 머신에서 AWS Redshift에 접근하기.html#virtual-private-cloudvpc-보안-그룹-설정",
    "href": "posts/2020-02-06-로컬 머신에서 AWS Redshift에 접근하기.html#virtual-private-cloudvpc-보안-그룹-설정",
    "title": "로컬 머신에서 AWS Redshift에 접근하기",
    "section": "1. Virtual Private Cloud(VPC) 보안 그룹 설정",
    "text": "1. Virtual Private Cloud(VPC) 보안 그룹 설정\n\nVPC의 보안 그룹 설정이 필요하다.\n클러스터 선택 -> 속성 -> 네트워크 및 보안 -> VPC 보안 그룹 메뉴로\nInbound 설정에서, 유형 Redshift, 연결을 원하는 머신의 IP를 Source로 설정하자."
  },
  {
    "objectID": "posts/2020-02-06-로컬 머신에서 AWS Redshift에 접근하기.html#공개적으로-액세스-할-수-있음-설정",
    "href": "posts/2020-02-06-로컬 머신에서 AWS Redshift에 접근하기.html#공개적으로-액세스-할-수-있음-설정",
    "title": "로컬 머신에서 AWS Redshift에 접근하기",
    "section": "2. “공개적으로 액세스 할 수 있음” 설정",
    "text": "2. “공개적으로 액세스 할 수 있음” 설정\n\n너무 간단한 내용이지만, 메뉴얼에 없었다…\n클러스터 선택 -> 속성 -> 네트워크 및 보안 -> 공개적으로 액세스 할 수 있음 메뉴에서 ’예’로 바꿔주면 된다."
  },
  {
    "objectID": "posts/2020-02-06-로컬 머신에서 AWS Redshift에 접근하기.html#tableplus에서-연결하기",
    "href": "posts/2020-02-06-로컬 머신에서 AWS Redshift에 접근하기.html#tableplus에서-연결하기",
    "title": "로컬 머신에서 AWS Redshift에 접근하기",
    "section": "3. TablePlus에서 연결하기",
    "text": "3. TablePlus에서 연결하기\n\nMac용 SQL 클라이언트 중에서는 TablePlus가 제일 좋은 것 같다.\n클러스터 Endpoint는 이런 구조다.\n\nCLUSTER-NAME.CLUSTER-KEY.CLUSTER-REGION.redshift.amazonaws.com:PORT/DATABASE-NAME\n\nHost에 입력할 값\n\nCLUSTER-NAME.CLUSTER-KEY.CLUSTER-REGION.redshift.amazonaws.com\n\nPort, User, Password, Database는 설정했던 값을 입력\n나머지 옵션은 조절하지 않아도 됨"
  },
  {
    "objectID": "posts/2020-02-06-로컬 머신에서 AWS Redshift에 접근하기.html#psycopg2에서-연결하기",
    "href": "posts/2020-02-06-로컬 머신에서 AWS Redshift에 접근하기.html#psycopg2에서-연결하기",
    "title": "로컬 머신에서 AWS Redshift에 접근하기",
    "section": "4. psycopg2에서 연결하기",
    "text": "4. psycopg2에서 연결하기\n\n간단해서 코드로 대신함\n\nimport psycopg2\ndbname='YOUR-DB-NAME' # 최초의 기본 db명은 dev\nhost='CLUSTER-NAME.CLUESTER-KEY.CLUESTER-REGION.redshift.amazonaws.com'\nport=5439\nuser='USER-NAME'\npassword='********'\ncon=psycopg2.connect(dbname=dbname, host=host, port=port, user=user, password=password)\ncur = con.cursor()\n\n매번 connect, cursor를 닫기 번거로우니 with 구문을 사용하자.\n\ndbname='YOUR-DB-NAME' # 최초의 기본 db명은 dev\nhost='CLUSTER-NAME.CLUESTER-KEY.CLUESTER-REGION.redshift.amazonaws.com'\nport=5439\nuser='USER-NAME'\npassword='********'\nconnect_param = dict({'dbname':dbname, 'host':host, 'port':port, 'user':user, 'password':password})\n\nwith psycopg2.connect(**connect_param) as con:\n    with con.cursor() as cur:\n        do something"
  },
  {
    "objectID": "posts/2022-03-19-맥 OS pyenv 세팅 101.html",
    "href": "posts/2022-03-19-맥 OS pyenv 세팅 101.html",
    "title": "Lim's Code Archive",
    "section": "",
    "text": "badges: False\ncomments: true\nauthor: limyj0708\ncategories: [Python]\n\n\n\n\nhttps://brew.sh 확인하여 설치\n설치 후 ~/.zprofile에 eval \"$(/opt/homebrew/bin/brew shellenv)\" 추가\n\n다 설치하면 맨 마지막에 안내문으로 추가하라고 나오니 따라하기만 하자.\n\n\n\n\n\n\nbrew install pyenv : 설치가 끝났다면…\n\n.zshrc에 eval \"$(pyenv init -)\" 추가\n.zprofile에 eval \"$(pyenv init --path)\" 추가\n\n\n\n\n\n\npyenv install -list : 설치가능한 파이썬 버전 목록 확인\npyenv install 3.10.3 : 예) 3.10.3 버전 설치\n\n\n\n\n\nbrew install pyenv-virtualenv\n\n.zshrc에 eval \"$(pyenv virtualenv-init -)\" 추가\n\n\n\n\n\n\npyenv virtualenv [파이썬 버전] [가상환경 이름]\n\n예) pyenv virtualenv 3.10.3 requests-3.10.3\n\npyenv versions : 생성한 가상환경이 추가되었음을 알 수 있음\n\n\n\n\n\n직접 on/off\n\npyenv activate requests-3.10.3\n\n실행하면 아래와 같은 메세지가 출력된다.\npyenv-virtualenv: prompt changing will be removed from future release. configure “export PYENV_VIRTUALENV_DISABLE_PROMPT=1” to simulate the behavior\n곧 이 기능은 사라질 모양이다.\n\npyenv deactivate\n\nshell의 세션이 유지되는 동안 가상환경 유지\n\npyenv shell requests-3.10.3\n\n원하는 폴더에 가서 실행하면, 이후 shell에서 해당 폴더로 가면 자동으로 원하는 가상환경이 켜지게 됨 (.python-version 파일이 해당 폴더에 생성)\n\npyenv local requests-3.10.3\npyenv local system : 다시 기본 시스템 버전으로 돌리고 싶을 때\n해당 폴더에서 나가면 자동으로 기본 환경으로 돌아가게 된다. 편리하네!\n\n전체 적용\n\npyenv global requests-3.10.3\npyenv global system : 다시 기본 시스템 버전으로 돌리고 싶을 때"
  },
  {
    "objectID": "posts/2021-11-17-리눅스 shell 명령어 Python 스크립트에서 실행 + Crontab.html",
    "href": "posts/2021-11-17-리눅스 shell 명령어 Python 스크립트에서 실행 + Crontab.html",
    "title": "Lim's Code Archive",
    "section": "",
    "text": "“그리고 Crontab으로도 실행해보기”\n\n\nbadges: false\ncomments: true\nauthor: limyj0708\ncategories: [Python, Linux]\n\n\n\n\n회사에 업무를 위한 소형 개인 서버로 쓰는 NUC가 있다.\n\nssh로 연결하여 사용\n\n사내 와이파이에 연결되어 있는데, 아주 가끔씩 할당된 IP가 바뀐다.\n이럴 때마다 모니터를 연결해서 ifconfig로 ip 주소를 확인할 수는 없는 노릇이다.\n일주일에 한 번씩, 서버가 나에게 현재 자신의 ip가 뭔지 보내줬으면 좋겠다.\n\n\n\n\n\n\nifconfig [원하는 네트워크 인터페이스명] | grep -Eo '([0-9]{1,3}[\\.]){3}[0-9]{1,3}'\n\ngrep\n\n-E : 표현을 확장 정규 표현식으로 해석\n-o : 매칭되는 문자열만 표시\n\n\n\n\n\nimport subprocess\nimport requests\n\nregex_ipv4 = '([0-9]{1,3}[\\.]){3}[0-9]{1,3}' #ipv4를 추출하는 정규식\nps = subprocess.Popen((\"ifconfig\", \"원하는 네트워크 인터페이스명\"), stdout=subprocess.PIPE)\noutput = subprocess.check_output((\"grep\", \"-Eo\", regex_ipv4), stdin=ps.stdout)\nps.wait()\nipv4_internal = str(output).split('\\\\n')[0][2:]\n# 사내에서 접근 가능한 IP주소만 추출함\n\nTARGET_URL = 'https://notify-api.line.me/api/notify'\nTOKEN = '라인 Notify에서 발급받은 토큰 입력'\n# 요청합니다.\nresponse = requests.post(\n    TARGET_URL,\n    headers={\n    'Authorization': 'Bearer ' + TOKEN\n    },\n    data={\n    'message': f'NUC IP : {ipv4_internal}'\n    }\n)\n\nShell에서처럼 Pipe(|)를 쓸 수 없다.\n\nPopen에 shell=True를 넘겨주면 되긴 하는데, 일반적으로 shell에서 명령을 내리는 것 처럼 별도의 유효성 검사 없이 실행이 되기 때문에 shell injection에 취약하게 된다.\n\n그래서 쪼개서 실행시켜야 한다. Popen으로 ifconfig를 실행시키고, 그 출력값을 check_output에 연결하여 최종 출력값을 만든다.\n추출한 IP를 Line Notify를 통해 라인으로 받는다.\n\n### 3. Crontab에서\n PATH=/usr/bin:/usr/sbin:/sbin:/usr/local/bin\n # ifconfig, grep을 잘 실행시키기 위한 환경 지정\n \n # ssh 접속용 사내 Wifi ipv4 전송용\n # 매주 월요일 오전 10시에 전송 \n00 10 * * 1 /usr/bin/python3.9 /home/limyj0708/Code/ipv4_internal_alarm/ipv4_internal_alarm.py >> /home/limyj0708/Code/ipv4_internal_alarm/cron_log.log 2>&1\n\n\n\n\n지정한 Line Notify 봇을 통해 IP가 잘 날아온다.\n\n\n\n\n\nShell=True는 Shell Injection에 취약\nPopen 클래스 개괄\nSubprocess 모듈 사용법"
  },
  {
    "objectID": "posts/2022-05-26-Jupyter Lab Server 세팅.html",
    "href": "posts/2022-05-26-Jupyter Lab Server 세팅.html",
    "title": "Lim's Code Archive",
    "section": "",
    "text": "badges: False\ncomments: true\nauthor: limyj0708\ncategories: [JupyterLab]\ntoc: true\n\n\n\nCentOS에 Jupyter Lab 설치가 완료되었다고 가정하자.\n\n구글에 Jupyter lab server라고 검색하면, 아래 페이지가 가장 먼저 뜨게 되는데, 이 페이지 말고\n\nhttps://jupyter-notebook.readthedocs.io/en/stable/public_server.html\n\n이 페이지를 확인하는 것이 좋다.\n\nhttps://jupyter-server.readthedocs.io/en/latest/operators/public-server.html\n\n\n\n\n\n\njupyter server --generate-config를 하면, /home/“유저이름”/.jupyter/jupyter_server_config.py가 생성된다. 여기에서 세팅을 해야 한다.\n\nc.ServerApp.open_browser = False (브라우저 띄우지 않음)\nc.ServerApp.password = ‘argon2…’\n\nfrom jupyter_server.auth import passwd; passwd()를 실행하여 생성하는, 암호화된 비밀번호를 입력한다.\n\nc.ServerApp.port = 원하는 포트\nc.ServerApp.certfile = openssl로 만든 certfile 등록 (예: mycert.pem)\nc.ServerApp.keyfile = openssl로 만든 keyfile 등록 (예: mykey.key)\nc.ServerApp.ip = ’*’, 혹은 접근 가능하게 하고싶은 ip\nc.ServerApp.root_dir = 원하는 경로\n\nnotebook_dir is deprecated, use root_dir\n\nc.ServerApp.allow_origin = ’*’\n\nUse ’*’ to allow any origin to access your server.\n\n\n\n\n\n\n\nself-signed certificate 오류 메세지\n\nopenssl req -x509 -nodes -days 999 -newkey rsa:2048 -keyout mykey.key -out mycert.pem\n\n이런 식으로 만든 self-signed certificate를 쓰면 jupyter lab 실행 시, 뭘 하기만 하면 SSL Error를 띄운다.\n\n이런 식으로 : SSL Error on 13 (‘ip’, 13786): [SSL: SSLV3_ALERT_CERTIFICATE_UNKNOWN] sslv3 alert certificate unknown (_ssl.c:997)\nSafari에서는 안 뜨고, Edge에서는 뜨는 걸로 봐서 크로미움 기반 브라우저에서 접속하면 뜨는 것 같다.\n\n\nLet’s Encrypt 같은 서비스를 이용해서 인증서를 받아도 되는데, 도메인 네임도 없는, 혼자 쓰는 무료 클라우드 서버에서 그렇게까지 해야 하나 싶다.\ntmux에서 새 pane을 만들고, jupyter lab > /dev/null 2>&1 &으로 jupyter lab을 실행하여 콘솔 output을 없애고 백그라운드에서 jupyter lab을 실행하자."
  },
  {
    "objectID": "posts/2022-05-26-Python Google Drive API v3로 파일 업로드.html",
    "href": "posts/2022-05-26-Python Google Drive API v3로 파일 업로드.html",
    "title": "Lim's Code Archive",
    "section": "",
    "text": "badges: False\ncomments: true\nauthor: limyj0708\ncategories: [Python]\ntoc: true\n\n\n\n\n!pip3 install --upgrade google-api-python-client google-auth-httplib2 google-auth-oauthlib\n\n\n\n\n\nhttps://console.cloud.google.com/ 접속\n원하는 프로젝트 선택\nAPI & Services\n\nEnabled APIs & Services에서 Google Drive API 활성화\nCredentials\n\nCreate Credentials -> OAuth client ID 생성\n\nservice account를 사용하고 싶었으나, 대상 폴더가 회사 조직 내 계정이 아니면 공유가 되지 않는 폴더여서 service account 사용이 불가능\n가능한 상황이면, service account를 대상 폴더의 편집자로 추가하는 편이, 더 보안상 좋다.\n\nDownload OAuth Client\n\nclinet-secret JSON 파일이 받아진다.\n\n\n\nhttps://developers.google.com/drive/api/quickstart/python\n\nquickstart 스크립트를 적절하게 바꾸어서 실행한다.\n최초로 실행하면 로그인 과정 후에 token.json이 생성되고, 이후에는 token.json을 읽어서 실행된다.\n아래 스크립트는 xlsx 파일 하나를 원하는 폴더에 업로드 하는 스크립트이다.\n\n\n\nimport os.path\n\nfrom google.auth.transport.requests import Request\nfrom google.oauth2.credentials import Credentials\nfrom google_auth_oauthlib.flow import InstalledAppFlow\nfrom googleapiclient.discovery import build\nfrom googleapiclient.errors import HttpError\n\n# If modifying these scopes, delete the file token.json.\n# \nSCOPES = ['https://www.googleapis.com/auth/drive.file']\n\n\ndef main():\n    \"\"\"Shows basic usage of the Drive v3 API.\n    Prints the names and ids of the first 10 files the user has access to.\n    \"\"\"\n    creds = None\n    # The file token.json stores the user's access and refresh tokens, and is\n    # created automatically when the authorization flow completes for the first\n    # time.\n    if os.path.exists('token.json'):\n        creds = Credentials.from_authorized_user_file('token.json', SCOPES)\n    # If there are no (valid) credentials available, let the user log in.\n    if not creds or not creds.valid:\n        if creds and creds.expired and creds.refresh_token:\n            creds.refresh(Request())\n        else:\n            flow = InstalledAppFlow.from_client_secrets_file('Download OAuth Clinet에서 받은 client-secret JSON 파일', SCOPES)\n            creds = flow.run_local_server(port=0)\n        # Save the credentials for the next run\n        with open('token.json', 'w') as token:\n            token.write(creds.to_json())\n\n    try:\n        # 원하는 작업 코드 작성\n        # 이 경우에는, xlsx 파일 하나를 원하는 폴더에 업로드        \n        folder_id = '원하는 폴더 ID'\n        service = build('drive', 'v3', credentials=creds)\n        file_metadata = {'name': 'quest_main_join_string_name.xlsx','parents': [folder_id]}\n        media = MediaFileUpload('quest_main_join_string_name.xlsx',\n                            mimetype=None, resumable=True)\n        # 파일이 커질 것 같으면 resumable을 켜 주는 것이 좋다.\n        file = service.files().create(body=file_metadata,media_body=media,fields='id').execute\n        \n        except HttpError as error:\n            # TODO(developer) - Handle errors from drive API.\n            print(f'An error occurred: {error}')\n\nif __name__ == '__main__':\n    main()"
  },
  {
    "objectID": "posts/2021-11-05-pandas_cheatsheet.html",
    "href": "posts/2021-11-05-pandas_cheatsheet.html",
    "title": "Lim's Code Archive",
    "section": "",
    "text": "” 안 쓰면 잊어버리는, pandas에서의 주요 Dataframe 조작 방법들을 정리”\n\n\nimport pandas as pd\nimport numpy as np\nfrom IPython.display import display_html \n\n\n\n\n\n\ndata = {'col_1': [3, 2, 1, 0], 'col_2': ['a', 'b', 'c', 'd']}\npd.DataFrame.from_dict(data)\n# key가 컬럼, value로 들어간 리스트가 컬럼의 row 하나하나가 된다.\n\n\n\n\n\n  \n    \n      \n      col_1\n      col_2\n    \n  \n  \n    \n      0\n      3\n      a\n    \n    \n      1\n      2\n      b\n    \n    \n      2\n      1\n      c\n    \n    \n      3\n      0\n      d\n    \n  \n\n\n\n\n\ndict_list = [\n    { \"id\" : 1001001, \"address\" : \"AABCC\"}\n    ,{ \"id\" : 2101001, \"address\" : \"BBBDD\"}\n    ,{ \"id\" : 3201001, \"address\" : \"백두산\"}\n    ,{ \"id\" : 4301001, \"address\" : \"한라산\"}\n    ,{ \"id\" : 5401001, \"address\" : \"몰디브\"}\n] # 같은 key들을 가진 딕셔너리들이 담긴 리스트\npd.DataFrame.from_dict(dict_list) # 이렇게 넣어도, key들이 컬럼이 되어 데이터프레임이 만들어진다.\n# 실무적으로는 이 형태를 더 많이 쓰게 된다.\n\n\n\n\n\n  \n    \n      \n      id\n      address\n    \n  \n  \n    \n      0\n      1001001\n      AABCC\n    \n    \n      1\n      2101001\n      BBBDD\n    \n    \n      2\n      3201001\n      백두산\n    \n    \n      3\n      4301001\n      한라산\n    \n    \n      4\n      5401001\n      몰디브\n    \n  \n\n\n\n\n\n\n\n\ndf = pd.DataFrame(columns=['A','B','BB','C','D'])\n# 컬럼들이 될 리스트를 columns parameter에 argument로 넘김\ndf\n\n\n\n\n\n  \n    \n      \n      A\n      B\n      BB\n      C\n      D\n    \n  \n  \n  \n\n\n\n\n\ndf['A'] = [1,3,1]\ndf\n\n\n\n\n\n  \n    \n      \n      A\n      B\n      BB\n      C\n      D\n    \n  \n  \n    \n      0\n      1\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      1\n      3\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2\n      1\n      NaN\n      NaN\n      NaN\n      NaN\n    \n  \n\n\n\n\n\ndf['B'] = [4,4,6]\ndf\n\n\n\n\n\n  \n    \n      \n      A\n      B\n      BB\n      C\n      D\n    \n  \n  \n    \n      0\n      1\n      4\n      NaN\n      NaN\n      NaN\n    \n    \n      1\n      3\n      4\n      NaN\n      NaN\n      NaN\n    \n    \n      2\n      1\n      6\n      NaN\n      NaN\n      NaN\n    \n  \n\n\n\n\n\ndf.loc[((df['A'] == 1) & (df['B'] == 4)), 'C'] = 444\ndf\n# 컬럼 값 조건을 걸고 값을 변경\n\n\n\n\n\n  \n    \n      \n      A\n      B\n      BB\n      C\n      D\n    \n  \n  \n    \n      0\n      1\n      4\n      NaN\n      444\n      NaN\n    \n    \n      1\n      3\n      4\n      NaN\n      NaN\n      NaN\n    \n    \n      2\n      1\n      6\n      NaN\n      NaN\n      NaN\n    \n  \n\n\n\n\n\ndf.loc[(df['B'] == 4), 'C'] = 0\ndf\n# 컬럼 값 조건을 걸고 값을 변경 2\n\n\n\n\n\n  \n    \n      \n      A\n      B\n      BB\n      C\n      D\n    \n  \n  \n    \n      0\n      1\n      4\n      NaN\n      0\n      NaN\n    \n    \n      1\n      3\n      4\n      NaN\n      0\n      NaN\n    \n    \n      2\n      1\n      6\n      NaN\n      NaN\n      NaN\n    \n  \n\n\n\n\n\nsample_list = [1,2,3,4,5]\n# 해당 데이터프레임 가장 아래에 리스트를 row로 넣음\ndf.loc[len(df)] = sample_list\n# 이 방식은 좀 느린 편이며, 데이터프레임에 행을 추가해야 한다면\n# 자료를 dictionary로 관리하다가 모든 데이터 추가가 다 끝나고 데이터프레임으로 변환하는 것이 빠름\ndf\n\n\n\n\n\n  \n    \n      \n      A\n      B\n      BB\n      C\n      D\n    \n  \n  \n    \n      0\n      1\n      4\n      NaN\n      0\n      NaN\n    \n    \n      1\n      3\n      4\n      NaN\n      0\n      NaN\n    \n    \n      2\n      1\n      6\n      NaN\n      NaN\n      NaN\n    \n    \n      3\n      1\n      2\n      3\n      4\n      5\n    \n  \n\n\n\n\n\n\n\n\n\n\n\nprint(type(df.loc[0]))\ndf.loc[0]\n# loc의 첫 번째 인자는 '행 라벨' 이다.\n# 그래서 0을 넣으면, index가 0인 행을 series로 반환하고 있다.\n\n<class 'pandas.core.series.Series'>\n\n\nA       1\nB       4\nBB    NaN\nC       0\nD     NaN\nName: 0, dtype: object\n\n\n\nprint(type(df.loc[0, 'A']))\ndf.loc[0, 'A']\n# 두 번째 인자는 컬럼명이다.\n\n<class 'numpy.int64'>\n\n\n1\n\n\n\ndf.loc[[0,1,2,3], ['A','B']]\n# 이런 식으로 접근하면, 다중 컬럼과 행을 데이터프레임으로 가져올 수 있다.\n\n\n\n\n\n  \n    \n      \n      A\n      B\n    \n  \n  \n    \n      0\n      1\n      4\n    \n    \n      1\n      3\n      4\n    \n    \n      2\n      1\n      6\n    \n    \n      3\n      1\n      2\n    \n  \n\n\n\n\n\ndf.loc[df.index[0:3], ['A','B']]\n# df.index로도 접근 가능\n\n\n\n\n\n  \n    \n      \n      A\n      B\n    \n  \n  \n    \n      0\n      1\n      4\n    \n    \n      1\n      3\n      4\n    \n    \n      2\n      1\n      6\n    \n  \n\n\n\n\n\ndf.loc[df['B'] == 4]\n# row에 값 조건을 걸 수도 있다.\n\n\n\n\n\n  \n    \n      \n      A\n      B\n      BB\n      C\n      D\n    \n  \n  \n    \n      0\n      1\n      4\n      NaN\n      0\n      NaN\n    \n    \n      1\n      3\n      4\n      NaN\n      0\n      NaN\n    \n  \n\n\n\n\n\ndf.loc[df['B'] == 4, df.columns.str.contains('B')]\n# 컬럼 이름에도 조건을 걸 수 있다. 위의 경우, 컬럼 이름에 B를 포함하는 컬럼만 가져옴.\n\n\n\n\n\n  \n    \n      \n      B\n      BB\n    \n  \n  \n    \n      0\n      4\n      NaN\n    \n    \n      1\n      4\n      NaN\n    \n  \n\n\n\n\n\ndf.loc[:,df.columns.str.contains('B')]\n# 행 조건 자리에 :를 넣으면, 행에 대해서는 전체를 다 가져오라는 뜻이다.\n\n\n\n\n\n  \n    \n      \n      B\n      BB\n    \n  \n  \n    \n      0\n      4\n      NaN\n    \n    \n      1\n      4\n      NaN\n    \n    \n      2\n      6\n      NaN\n    \n    \n      3\n      2\n      3\n    \n  \n\n\n\n\n\nprint(df.columns) # 컬럼명을 가져옴\nprint(df.columns.str)\nprint(df.columns.str.contains('B')) # boolean indexing이 가능한 형태가 된다.\nprint(type(df.columns.str.contains('B'))) # 결과물은 false와 true가 들어간 ndarray\nprint(df.columns.str.startswith('A')) # 이렇게 하면 A로 시작하는 컬럼을 가져올 수 있음\n# 결론은, 다른 외부 함수를 사용해서 어쩄든 boolean 타입 값이 담긴 리스트를 만들면, loc에 넣어서 boolean indexing이 가능하다는 것.\n\nIndex(['A', 'B', 'BB', 'C', 'D'], dtype='object')\n<pandas.core.strings.StringMethods object at 0x000001E1FEC0B250>\n[False  True  True False False]\n<class 'numpy.ndarray'>\n[ True False False False False]\n\n\n\ndf.loc[:,'new'] = 3\ndf\n# loc으로도 기존에 없던 새 컬럼을 추가할 수 있음\n\n\n\n\n\n  \n    \n      \n      A\n      B\n      BB\n      C\n      D\n      new\n    \n  \n  \n    \n      0\n      1\n      4\n      NaN\n      0\n      NaN\n      3\n    \n    \n      1\n      3\n      4\n      NaN\n      0\n      NaN\n      3\n    \n    \n      2\n      1\n      6\n      NaN\n      NaN\n      NaN\n      3\n    \n    \n      3\n      1\n      2\n      3\n      4\n      5\n      3\n    \n  \n\n\n\n\n\ndf.loc[4] = [1] * len(df.columns)\ndf.loc[99] = [1] * len(df.columns)\ndf.loc['cool'] = [22] * len(df.columns)\n# dataframe에 행을 추가함. index가 늘어난다.\ndf\n\n\n\n\n\n  \n    \n      \n      A\n      B\n      BB\n      C\n      D\n      new\n    \n  \n  \n    \n      0\n      1\n      4\n      NaN\n      0\n      NaN\n      3\n    \n    \n      1\n      3\n      4\n      NaN\n      0\n      NaN\n      3\n    \n    \n      2\n      1\n      6\n      NaN\n      NaN\n      NaN\n      3\n    \n    \n      3\n      1\n      2\n      3\n      4\n      5\n      3\n    \n    \n      4\n      1\n      1\n      1\n      1\n      1\n      1\n    \n    \n      99\n      1\n      1\n      1\n      1\n      1\n      1\n    \n    \n      cool\n      22\n      22\n      22\n      22\n      22\n      22\n    \n  \n\n\n\n\n\n\n\n\ndf.iloc[0:2,0:4]\n# 기본적 동작은 loc과 동일하나, 받는 인자가 라벨이 아니고 '위치'다.\n\n\n\n\n\n  \n    \n      \n      A\n      B\n      BB\n      C\n    \n  \n  \n    \n      0\n      1\n      4\n      NaN\n      0\n    \n    \n      1\n      3\n      4\n      NaN\n      0\n    \n  \n\n\n\n\n\ndf.iloc[4:7,0:4]\n# 위치를 받기 때문에, index는 99여도 5번째 줄로 인식됨\n\n\n\n\n\n  \n    \n      \n      A\n      B\n      BB\n      C\n    \n  \n  \n    \n      4\n      1\n      1\n      1\n      1\n    \n    \n      99\n      1\n      1\n      1\n      1\n    \n    \n      cool\n      22\n      22\n      22\n      22\n    \n  \n\n\n\n\n\ndf.iloc[7] = [2] * len(df.columns)\n# IndexError: iloc cannot enlarge its target object\n# 위치를 인자로 받기 때문에, 새로운 컬럼, 행을 만든다거나 하는 행위는 불가능하다.\n\nIndexError: iloc cannot enlarge its target object\n\n\n\n\n\n\ndf.at[1,'A']\n# 한 번에 1개의 스칼라값에만 접근 가능\n# 여러 개의 값에 접근하려고 범위를 지정하면, 에러를 출력한다.\n# 단일 값에 접근하는 목적이라면 loc보다 훨씬 빠름\n\n3\n\n\n\ndf.at[1,'A'] = 100\ndf\n# 값을 딱 하나만 바꾸고 싶다! 라고 하면 at을 활용해보자.\n\n\n\n\n\n  \n    \n      \n      A\n      B\n      BB\n      C\n      D\n      new\n    \n  \n  \n    \n      0\n      1\n      4\n      NaN\n      0\n      NaN\n      3\n    \n    \n      1\n      100\n      4\n      NaN\n      0\n      NaN\n      3\n    \n    \n      2\n      1\n      6\n      NaN\n      NaN\n      NaN\n      3\n    \n    \n      3\n      1\n      2\n      3\n      4\n      5\n      3\n    \n    \n      4\n      1\n      1\n      1\n      1\n      1\n      1\n    \n    \n      99\n      1\n      1\n      1\n      1\n      1\n      1\n    \n    \n      cool\n      22\n      22\n      22\n      22\n      22\n      22\n    \n  \n\n\n\n\n\ndf.at[99, 'new']\n# 그 이외에는 label base인 것이 loc과 똑같음\n\n1\n\n\n\n\n\n\ndf.iat[4,2]\n# iloc의 스칼라 버전.\n# 이외의 동작은 at과 같다.\n\n1\n\n\n\ndf.iat[df.index.get_loc('cool'),df.columns.get_loc('new')]\n# get_loc을 쓰면, 해당 인덱스와 컬럼의 위치를 반환받을 수 있음.\n# 그럼 인덱스와 컬럼의 이름으로도 iat, iloc을 이용 가능\n\n22\n\n\n\n\n\n\nmap함수는 DataFrame 타입이 아니라, 반드시 Series 타입에서만 사용해야 한다.\nSeries를 한마디로 정의하면 딱 이거다.\n\n값(value) + 인덱스(index) = 시리즈 클래스(Series)\n\nSeries는 NumPy에서 제공하는 1차원 배열과 비슷하지만 각 데이터의 의미를 표시하는 인덱스(index)를 붙일 수 있다. 하지만 데이터 자체는 그냥 값(value)의 1차원 배열이다.\nmap함수는 Series의 이러한 값 하나하나에 접근하면서 해당 함수를 수행한다.\n\n\nimport math as m # sqrt 함수 사용을 위해 부름\n# http://www.leejungmin.org/post/2018/04/21/pandas_apply_and_map/\ndf[\"map_b\"] = df[\"B\"].map(lambda x : m.sqrt(x)) \n# B컬럼의 값 하나하나에 sqrt 함수를 적용한 결과를 map_b 컬럼으로 추가\ndf\n\n\n\n\n\n  \n    \n      \n      A\n      B\n      BB\n      C\n      D\n      new\n      map_b\n    \n  \n  \n    \n      0\n      1\n      4\n      NaN\n      0\n      NaN\n      3\n      2.000000\n    \n    \n      1\n      100\n      4\n      NaN\n      0\n      NaN\n      3\n      2.000000\n    \n    \n      2\n      1\n      6\n      NaN\n      NaN\n      NaN\n      3\n      2.449490\n    \n    \n      3\n      1\n      2\n      3\n      4\n      5\n      3\n      1.414214\n    \n    \n      4\n      1\n      1\n      1\n      1\n      1\n      1\n      1.000000\n    \n    \n      99\n      1\n      1\n      1\n      1\n      1\n      1\n      1.000000\n    \n    \n      cool\n      22\n      22\n      22\n      22\n      22\n      22\n      4.690416\n    \n  \n\n\n\n\n\n\n\n\n커스텀 함수를 사용하기 위해 DataFrame에서 복수 개의 컬럼이 필요하다면, apply 함수를 사용해야 한다.\n\n\nimport math as m # sqrt 함수 사용을 위해 부름\n# 두 컬럼의 제곱근의 값을 각각 곱하는 함수\ndef sqrt_multi(x,y):\n    return m.sqrt(x) * m.sqrt(y)\n\n\ndf.loc[:,'new'] = df.apply(lambda x : sqrt_multi(x['A'], x['B']), axis=1) # axis=1 이면 각 열의 원소에 대해 연산 수행\ndf\n\n\n\n\n\n  \n    \n      \n      A\n      B\n      BB\n      C\n      D\n      new\n      map_b\n    \n  \n  \n    \n      0\n      1\n      4\n      NaN\n      0\n      NaN\n      2.000000\n      2.000000\n    \n    \n      1\n      100\n      4\n      NaN\n      0\n      NaN\n      20.000000\n      2.000000\n    \n    \n      2\n      1\n      6\n      NaN\n      NaN\n      NaN\n      2.449490\n      2.449490\n    \n    \n      3\n      1\n      2\n      3\n      4\n      5\n      1.414214\n      1.414214\n    \n    \n      4\n      1\n      1\n      1\n      1\n      1\n      1.000000\n      1.000000\n    \n    \n      99\n      1\n      1\n      1\n      1\n      1\n      1.000000\n      1.000000\n    \n    \n      cool\n      22\n      22\n      22\n      22\n      22\n      22.000000\n      4.690416\n    \n  \n\n\n\n\n\ndf[\"apply_bb_d\"] = df.apply(lambda x : sqrt_multi(x['BB'], x['B']), axis=1) # axis=1 이면 각 열의 원소에 대해 연산 수행\ndf # NaN과의 연산은 NaN이 됨을 참고하자.\n\n\n\n\n\n  \n    \n      \n      A\n      B\n      BB\n      C\n      D\n      new\n      map_b\n      apply_bb_d\n    \n  \n  \n    \n      0\n      1\n      4\n      NaN\n      0\n      NaN\n      2.000000\n      2.000000\n      NaN\n    \n    \n      1\n      100\n      4\n      NaN\n      0\n      NaN\n      20.000000\n      2.000000\n      NaN\n    \n    \n      2\n      1\n      6\n      NaN\n      NaN\n      NaN\n      2.449490\n      2.449490\n      NaN\n    \n    \n      3\n      1\n      2\n      3\n      4\n      5\n      1.414214\n      1.414214\n      2.44949\n    \n    \n      4\n      1\n      1\n      1\n      1\n      1\n      1.000000\n      1.000000\n      1.00000\n    \n    \n      99\n      1\n      1\n      1\n      1\n      1\n      1.000000\n      1.000000\n      1.00000\n    \n    \n      cool\n      22\n      22\n      22\n      22\n      22\n      22.000000\n      4.690416\n      22.00000\n    \n  \n\n\n\n\n\n\n\n\nprint(df.columns)\nprint(type(df.columns))\nprint(df.index)\nprint(type(df.index))\nprint(df.columns[2]) # 위치값으로 개별 요소에 접근 가능\nprint(df.index[6])\n\nIndex(['A', 'B', 'BB', 'C', 'D', 'new', 'map_b', 'apply_bb_d'], dtype='object')\n<class 'pandas.core.indexes.base.Index'>\nIndex([0, 1, 2, 3, 4, 99, 'cool'], dtype='object')\n<class 'pandas.core.indexes.base.Index'>\nBB\ncool\n\n\n\ndf.columns = ['가', '나', '다', '라', '마', '바', '사', '아'] \n# df.columns에 직접 컬럼명 리스트를 할당하여 컬럼명 변경 가능\n# 기존 컬럼 수와 같은 길이의 리스트를 넣지 않으면 오류가 발생함\nprint(df.columns)\nprint(type(df.columns))\n\nIndex(['가', '나', '다', '라', '마', '바', '사', '아'], dtype='object')\n<class 'pandas.core.indexes.base.Index'>\n\n\n\ndf.index = [1,2,3,4,5,6,7] \n# df.columns에 직접 컬럼명 리스트를 할당하여 컬럼명 변경 가능\nprint(df.index)\nprint(type(df.index))\n\nInt64Index([1, 2, 3, 4, 5, 6, 7], dtype='int64')\n<class 'pandas.core.indexes.numeric.Int64Index'>\n\n\n\n\n\nDataFrame.set_index(keys, drop=True, append=False, inplace=False)\n\nkeys에는 index로 할당하고자 하는 열의 레이블을 입력한다.\n\nmulti-index를 하고 싶으면, [‘가’, ‘나’] 이렇게 열 레이블 배열을 입력한다.\n\ndrop : index로 할당한 열을 삭제할까요?\nappend : 기존에 존재하던 index를 삭제할까요?\ninplace : 원본 데이터프레임을 변경할까요?\n\n\ndf\n\n\n\n\n\n  \n    \n      \n      가\n      나\n      다\n      라\n      마\n      바\n      사\n      아\n    \n  \n  \n    \n      1\n      1\n      4\n      NaN\n      0\n      NaN\n      2.000000\n      2.000000\n      NaN\n    \n    \n      2\n      100\n      4\n      NaN\n      0\n      NaN\n      20.000000\n      2.000000\n      NaN\n    \n    \n      3\n      1\n      6\n      NaN\n      NaN\n      NaN\n      2.449490\n      2.449490\n      NaN\n    \n    \n      4\n      1\n      2\n      3\n      4\n      5\n      1.414214\n      1.414214\n      2.44949\n    \n    \n      5\n      1\n      1\n      1\n      1\n      1\n      1.000000\n      1.000000\n      1.00000\n    \n    \n      6\n      1\n      1\n      1\n      1\n      1\n      1.000000\n      1.000000\n      1.00000\n    \n    \n      7\n      22\n      22\n      22\n      22\n      22\n      22.000000\n      4.690416\n      22.00000\n    \n  \n\n\n\n\n\ndf.set_index('가') # 기본값\n\n\n\n\n\n  \n    \n      \n      나\n      다\n      라\n      마\n      바\n      사\n      아\n    \n    \n      가\n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1\n      4\n      NaN\n      0\n      NaN\n      2.000000\n      2.000000\n      NaN\n    \n    \n      100\n      4\n      NaN\n      0\n      NaN\n      20.000000\n      2.000000\n      NaN\n    \n    \n      1\n      6\n      NaN\n      NaN\n      NaN\n      2.449490\n      2.449490\n      NaN\n    \n    \n      1\n      2\n      3\n      4\n      5\n      1.414214\n      1.414214\n      2.44949\n    \n    \n      1\n      1\n      1\n      1\n      1\n      1.000000\n      1.000000\n      1.00000\n    \n    \n      1\n      1\n      1\n      1\n      1\n      1.000000\n      1.000000\n      1.00000\n    \n    \n      22\n      22\n      22\n      22\n      22\n      22.000000\n      4.690416\n      22.00000\n    \n  \n\n\n\n\n\ndf.set_index('가', drop=False) # index로 선택된 열 삭제 안 함\n\n\n\n\n\n  \n    \n      \n      가\n      나\n      다\n      라\n      마\n      바\n      사\n      아\n    \n    \n      가\n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1\n      1\n      4\n      NaN\n      0\n      NaN\n      2.000000\n      2.000000\n      NaN\n    \n    \n      100\n      100\n      4\n      NaN\n      0\n      NaN\n      20.000000\n      2.000000\n      NaN\n    \n    \n      1\n      1\n      6\n      NaN\n      NaN\n      NaN\n      2.449490\n      2.449490\n      NaN\n    \n    \n      1\n      1\n      2\n      3\n      4\n      5\n      1.414214\n      1.414214\n      2.44949\n    \n    \n      1\n      1\n      1\n      1\n      1\n      1\n      1.000000\n      1.000000\n      1.00000\n    \n    \n      1\n      1\n      1\n      1\n      1\n      1\n      1.000000\n      1.000000\n      1.00000\n    \n    \n      22\n      22\n      22\n      22\n      22\n      22\n      22.000000\n      4.690416\n      22.00000\n    \n  \n\n\n\n\n\ndf.set_index('가', append=True) # 기존 index 삭제 안 함\n\n\n\n\n\n  \n    \n      \n      \n      나\n      다\n      라\n      마\n      바\n      사\n      아\n    \n    \n      \n      가\n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1\n      1\n      4\n      NaN\n      0\n      NaN\n      2.000000\n      2.000000\n      NaN\n    \n    \n      2\n      100\n      4\n      NaN\n      0\n      NaN\n      20.000000\n      2.000000\n      NaN\n    \n    \n      3\n      1\n      6\n      NaN\n      NaN\n      NaN\n      2.449490\n      2.449490\n      NaN\n    \n    \n      4\n      1\n      2\n      3\n      4\n      5\n      1.414214\n      1.414214\n      2.44949\n    \n    \n      5\n      1\n      1\n      1\n      1\n      1\n      1.000000\n      1.000000\n      1.00000\n    \n    \n      6\n      1\n      1\n      1\n      1\n      1\n      1.000000\n      1.000000\n      1.00000\n    \n    \n      7\n      22\n      22\n      22\n      22\n      22\n      22.000000\n      4.690416\n      22.00000\n    \n  \n\n\n\n\n\ndf.set_index(['가','나']) # 동시에 여러 열을 index로 설정하기\n\n\n\n\n\n  \n    \n      \n      \n      다\n      라\n      마\n      바\n      사\n      아\n    \n    \n      가\n      나\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1\n      4\n      NaN\n      0\n      NaN\n      2.000000\n      2.000000\n      NaN\n    \n    \n      100\n      4\n      NaN\n      0\n      NaN\n      20.000000\n      2.000000\n      NaN\n    \n    \n      1\n      6\n      NaN\n      NaN\n      NaN\n      2.449490\n      2.449490\n      NaN\n    \n    \n      2\n      3\n      4\n      5\n      1.414214\n      1.414214\n      2.44949\n    \n    \n      1\n      1\n      1\n      1\n      1.000000\n      1.000000\n      1.00000\n    \n    \n      1\n      1\n      1\n      1\n      1.000000\n      1.000000\n      1.00000\n    \n    \n      22\n      22\n      22\n      22\n      22\n      22.000000\n      4.690416\n      22.00000\n    \n  \n\n\n\n\n\n\n\nDataFrame.reset_index(drop=False, inplace=False)\n\n기존에 있던 index 대신에, 0부터 시작하여 1씩 늘어나는 정수 index를 추가한다.\ndrop : 기존에 index였던 열을 삭제할까요?\ninplace : 원본 데이터프레임을 변경할까요?\n\n\ndf.reset_index()\n\n\n\n\n\n  \n    \n      \n      index\n      가\n      나\n      다\n      라\n      마\n      바\n      사\n      아\n    \n  \n  \n    \n      0\n      1\n      1\n      4\n      NaN\n      0\n      NaN\n      2.000000\n      2.000000\n      NaN\n    \n    \n      1\n      2\n      100\n      4\n      NaN\n      0\n      NaN\n      20.000000\n      2.000000\n      NaN\n    \n    \n      2\n      3\n      1\n      6\n      NaN\n      NaN\n      NaN\n      2.449490\n      2.449490\n      NaN\n    \n    \n      3\n      4\n      1\n      2\n      3\n      4\n      5\n      1.414214\n      1.414214\n      2.44949\n    \n    \n      4\n      5\n      1\n      1\n      1\n      1\n      1\n      1.000000\n      1.000000\n      1.00000\n    \n    \n      5\n      6\n      1\n      1\n      1\n      1\n      1\n      1.000000\n      1.000000\n      1.00000\n    \n    \n      6\n      7\n      22\n      22\n      22\n      22\n      22\n      22.000000\n      4.690416\n      22.00000\n    \n  \n\n\n\n\n\ndf.reset_index(drop=True)\n\n\n\n\n\n  \n    \n      \n      가\n      나\n      다\n      라\n      마\n      바\n      사\n      아\n    \n  \n  \n    \n      0\n      1\n      4\n      NaN\n      0\n      NaN\n      2.000000\n      2.000000\n      NaN\n    \n    \n      1\n      100\n      4\n      NaN\n      0\n      NaN\n      20.000000\n      2.000000\n      NaN\n    \n    \n      2\n      1\n      6\n      NaN\n      NaN\n      NaN\n      2.449490\n      2.449490\n      NaN\n    \n    \n      3\n      1\n      2\n      3\n      4\n      5\n      1.414214\n      1.414214\n      2.44949\n    \n    \n      4\n      1\n      1\n      1\n      1\n      1\n      1.000000\n      1.000000\n      1.00000\n    \n    \n      5\n      1\n      1\n      1\n      1\n      1\n      1.000000\n      1.000000\n      1.00000\n    \n    \n      6\n      22\n      22\n      22\n      22\n      22\n      22.000000\n      4.690416\n      22.00000\n    \n  \n\n\n\n\n\n\n\n\n\n특정 조건의 값을 삭제하고 싶은 경우에는, 해당 조건의 반대 조건을 걸어서 반환 결과를 사용하는 식으로 처리한다.\n\n\n\nDataFrame.drop(labels=None, axis=0, index=None, columns=None, level=None, inplace=False, errors='raise')\n\n특정 레이블의 행이나 열을 제거한다.\nlabels : 제거할 index, 레이블 하나 혹은 리스트 (list-like)\naxis : 0이면 행, 1이면 컬럼 대상\nindex : labels, axis=0 대신 사용가능\ncolumns : labels, axis=1 대신 사용가능\nlevel : MultiIndex일 경우, 어떤 레벨을 제거할 것인지\ninplace : 원본 변경 할 건가요?\nerrors : ’ignore’로 세팅하면, 에러 출력 안 하고 존재하는 레이블만 제거한다.\n\n\ndf\n\n\n\n\n\n  \n    \n      \n      가\n      나\n      다\n      라\n      마\n      바\n      사\n      아\n    \n  \n  \n    \n      1\n      1\n      4\n      NaN\n      0\n      NaN\n      2.000000\n      2.000000\n      NaN\n    \n    \n      2\n      100\n      4\n      NaN\n      0\n      NaN\n      20.000000\n      2.000000\n      NaN\n    \n    \n      3\n      1\n      6\n      NaN\n      NaN\n      NaN\n      2.449490\n      2.449490\n      NaN\n    \n    \n      4\n      1\n      2\n      3\n      4\n      5\n      1.414214\n      1.414214\n      2.44949\n    \n    \n      5\n      1\n      1\n      1\n      1\n      1\n      1.000000\n      1.000000\n      1.00000\n    \n    \n      6\n      1\n      1\n      1\n      1\n      1\n      1.000000\n      1.000000\n      1.00000\n    \n    \n      7\n      22\n      22\n      22\n      22\n      22\n      22.000000\n      4.690416\n      22.00000\n    \n  \n\n\n\n\n\ndf.drop(labels=['가','아'], axis=1)\n\n\n\n\n\n  \n    \n      \n      나\n      다\n      라\n      마\n      바\n      사\n    \n  \n  \n    \n      1\n      4\n      NaN\n      0\n      NaN\n      2.000000\n      2.000000\n    \n    \n      2\n      4\n      NaN\n      0\n      NaN\n      20.000000\n      2.000000\n    \n    \n      3\n      6\n      NaN\n      NaN\n      NaN\n      2.449490\n      2.449490\n    \n    \n      4\n      2\n      3\n      4\n      5\n      1.414214\n      1.414214\n    \n    \n      5\n      1\n      1\n      1\n      1\n      1.000000\n      1.000000\n    \n    \n      6\n      1\n      1\n      1\n      1\n      1.000000\n      1.000000\n    \n    \n      7\n      22\n      22\n      22\n      22\n      22.000000\n      4.690416\n    \n  \n\n\n\n\n\ndf.drop(labels=[1,7], axis=0)\n\n\n\n\n\n  \n    \n      \n      가\n      나\n      다\n      라\n      마\n      바\n      사\n      아\n    \n  \n  \n    \n      2\n      100\n      4\n      NaN\n      0\n      NaN\n      20.000000\n      2.000000\n      NaN\n    \n    \n      3\n      1\n      6\n      NaN\n      NaN\n      NaN\n      2.449490\n      2.449490\n      NaN\n    \n    \n      4\n      1\n      2\n      3\n      4\n      5\n      1.414214\n      1.414214\n      2.44949\n    \n    \n      5\n      1\n      1\n      1\n      1\n      1\n      1.000000\n      1.000000\n      1.00000\n    \n    \n      6\n      1\n      1\n      1\n      1\n      1\n      1.000000\n      1.000000\n      1.00000\n    \n  \n\n\n\n\n\ndf.drop(columns=['가','아'])\n# df.drop(labels=['가','아'], axis=1)와 같은 결과\n\n\n\n\n\n  \n    \n      \n      나\n      다\n      라\n      마\n      바\n      사\n    \n  \n  \n    \n      1\n      4\n      NaN\n      0\n      NaN\n      2.000000\n      2.000000\n    \n    \n      2\n      4\n      NaN\n      0\n      NaN\n      20.000000\n      2.000000\n    \n    \n      3\n      6\n      NaN\n      NaN\n      NaN\n      2.449490\n      2.449490\n    \n    \n      4\n      2\n      3\n      4\n      5\n      1.414214\n      1.414214\n    \n    \n      5\n      1\n      1\n      1\n      1\n      1.000000\n      1.000000\n    \n    \n      6\n      1\n      1\n      1\n      1\n      1.000000\n      1.000000\n    \n    \n      7\n      22\n      22\n      22\n      22\n      22.000000\n      4.690416\n    \n  \n\n\n\n\n\n\n\n\n\n\nNaN인지 각 값에 대해 확인하여 boolean으로 표현\nisnull() 도 완전히 같은 기능을 한다.\n왜 같은 기능을 하는 함수가 두 개나 있는지는 아래 링크를 참조\n\nhttps://datascience.stackexchange.com/questions/37878/difference-between-isna-and-isnull-in-pandas\nThis is because pandas’ DataFrames are based on R’s DataFrames. In R na and null are two separate things. Read this post for more information. However, in python, pandas is built on top of numpy, which has neither na nor null values. Instead numpy has NaN values (which stands for “Not a Number”). Consequently, pandas also uses NaN values.\n\n\n\ndf.isna()\n# 특정 컬럼, 행에 대해서도 사용 가능\n\n\n\n\n\n  \n    \n      \n      가\n      나\n      다\n      라\n      마\n      바\n      사\n      아\n    \n  \n  \n    \n      1\n      False\n      False\n      True\n      False\n      True\n      False\n      False\n      True\n    \n    \n      2\n      False\n      False\n      True\n      False\n      True\n      False\n      False\n      True\n    \n    \n      3\n      False\n      False\n      True\n      True\n      True\n      False\n      False\n      True\n    \n    \n      4\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n    \n    \n      5\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n    \n    \n      6\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n    \n    \n      7\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n    \n  \n\n\n\n\n\n\n\nDataFrame.dropna(axis=0, how='any', thresh=None, subset=None, inplace=False)\n\naxis\n\n0 혹은 ‘index’ : missing value가 있는 행을 드랍\n1 혹은 ‘columns’ : missing value가 있는 열을 드랍\n\nhow\n\nany : missing value가 하나라도 있으면 드랍\nall : 전체 값이 다 missing value여야 드랍\n\nthresh : 문턱값. 정수를 입력 시, 정상값이 해당 정수 갯수만큼은 있어야 제거 안 함\nsubset : list-like 오브젝트를 넣으면, 해당 index나 컬럼에서만 missing value 체크\ninplace : 원본 변경 할 건가요?\n\n\ndf\n\n\n\n\n\n  \n    \n      \n      가\n      나\n      다\n      라\n      마\n      바\n      사\n      아\n    \n  \n  \n    \n      1\n      1\n      4\n      NaN\n      0\n      NaN\n      2.000000\n      2.000000\n      NaN\n    \n    \n      2\n      100\n      4\n      NaN\n      0\n      NaN\n      20.000000\n      2.000000\n      NaN\n    \n    \n      3\n      1\n      6\n      NaN\n      NaN\n      NaN\n      2.449490\n      2.449490\n      NaN\n    \n    \n      4\n      1\n      2\n      3\n      4\n      5\n      1.414214\n      1.414214\n      2.44949\n    \n    \n      5\n      1\n      1\n      1\n      1\n      1\n      1.000000\n      1.000000\n      1.00000\n    \n    \n      6\n      1\n      1\n      1\n      1\n      1\n      1.000000\n      1.000000\n      1.00000\n    \n    \n      7\n      22\n      22\n      22\n      22\n      22\n      22.000000\n      4.690416\n      22.00000\n    \n  \n\n\n\n\n\ndf.dropna() # 기본적으로 행 드랍\n\n\n\n\n\n  \n    \n      \n      가\n      나\n      다\n      라\n      마\n      바\n      사\n      아\n    \n  \n  \n    \n      4\n      1\n      2\n      3\n      4\n      5\n      1.414214\n      1.414214\n      2.44949\n    \n    \n      5\n      1\n      1\n      1\n      1\n      1\n      1.000000\n      1.000000\n      1.00000\n    \n    \n      6\n      1\n      1\n      1\n      1\n      1\n      1.000000\n      1.000000\n      1.00000\n    \n    \n      7\n      22\n      22\n      22\n      22\n      22\n      22.000000\n      4.690416\n      22.00000\n    \n  \n\n\n\n\n\ndf.dropna(axis=1) # 열 드랍\n\n\n\n\n\n  \n    \n      \n      가\n      나\n      바\n      사\n    \n  \n  \n    \n      1\n      1\n      4\n      2.000000\n      2.000000\n    \n    \n      2\n      100\n      4\n      20.000000\n      2.000000\n    \n    \n      3\n      1\n      6\n      2.449490\n      2.449490\n    \n    \n      4\n      1\n      2\n      1.414214\n      1.414214\n    \n    \n      5\n      1\n      1\n      1.000000\n      1.000000\n    \n    \n      6\n      1\n      1\n      1.000000\n      1.000000\n    \n    \n      7\n      22\n      22\n      22.000000\n      4.690416\n    \n  \n\n\n\n\n\ndf.dropna(thresh=5) # index 3인 행은 정상값이 4개였음\n\n\n\n\n\n  \n    \n      \n      가\n      나\n      다\n      라\n      마\n      바\n      사\n      아\n    \n  \n  \n    \n      1\n      1\n      4\n      NaN\n      0\n      NaN\n      2.000000\n      2.000000\n      NaN\n    \n    \n      2\n      100\n      4\n      NaN\n      0\n      NaN\n      20.000000\n      2.000000\n      NaN\n    \n    \n      4\n      1\n      2\n      3\n      4\n      5\n      1.414214\n      1.414214\n      2.44949\n    \n    \n      5\n      1\n      1\n      1\n      1\n      1\n      1.000000\n      1.000000\n      1.00000\n    \n    \n      6\n      1\n      1\n      1\n      1\n      1\n      1.000000\n      1.000000\n      1.00000\n    \n    \n      7\n      22\n      22\n      22\n      22\n      22\n      22.000000\n      4.690416\n      22.00000\n    \n  \n\n\n\n\n\ndf.dropna(axis=0, subset=['라']) # '라'열만 검사해서 NaN이 있는 행을 제거함\n\n\n\n\n\n  \n    \n      \n      가\n      나\n      다\n      라\n      마\n      바\n      사\n      아\n    \n  \n  \n    \n      1\n      1\n      4\n      NaN\n      0\n      NaN\n      2.000000\n      2.000000\n      NaN\n    \n    \n      2\n      100\n      4\n      NaN\n      0\n      NaN\n      20.000000\n      2.000000\n      NaN\n    \n    \n      4\n      1\n      2\n      3\n      4\n      5\n      1.414214\n      1.414214\n      2.44949\n    \n    \n      5\n      1\n      1\n      1\n      1\n      1\n      1.000000\n      1.000000\n      1.00000\n    \n    \n      6\n      1\n      1\n      1\n      1\n      1\n      1.000000\n      1.000000\n      1.00000\n    \n    \n      7\n      22\n      22\n      22\n      22\n      22\n      22.000000\n      4.690416\n      22.00000\n    \n  \n\n\n\n\n\n\n\nDataFrame.fillna(value=None, method=None, axis=None, inplace=False, limit=None, downcast=None)\n\nvalue : NaN을 무엇으로 채울 것인가?\n\nscalar : 0, 1 따위의 값을 넣음\ndict : {“A”: 0, “B”: 1, “C”: 2, “D”: 3}\n\n컬럼 A의 NaN은 0으로, 컬럼 B의 NaN은 1로, 컬럼 C의 NaN은 2로, 컬럼 D의 NaN은 3으로 대체\n\ndataframe : 대체 대상 dataframe와 같은 크기의 dataframe을 준비한 후, value에 dataframe을 넣으면 NaN 값만 넣은 dataframe의 값으로 대체된다. 컬럼명이나 인덱스는 원본 dataframe의 것이 유지된다.\n\nmethod : 어떤 방법으로 채울까? (value와 같이 사용할 수 없음)\n\nbackfill, bfill : NaN의 다음 값으로 NaN 채우기.\nffill, pad : NaN의 직전 값으로 NaN 채우기.\n\naxis\n\n0 혹은 ‘index’\n1 혹은 ‘columns’\n\ninplace : 원본 변경 할 건가요?\nlimit : 위에서부터 NaN 몇 개만 바꿀래? 기본값 None이면 모든 NaN을 바꾸는 것.\n\n\ndf = pd.DataFrame([[np.nan, 2, np.nan, 0],\n                   [3, 4, np.nan, 1],\n                   [np.nan, np.nan, np.nan, 5],\n                   [np.nan, 3, np.nan, 4]],\n                  columns=list(\"ABCD\"))\ndf\n\n\n\n\n\n  \n    \n      \n      A\n      B\n      C\n      D\n    \n  \n  \n    \n      0\n      NaN\n      2.0\n      NaN\n      0\n    \n    \n      1\n      3.0\n      4.0\n      NaN\n      1\n    \n    \n      2\n      NaN\n      NaN\n      NaN\n      5\n    \n    \n      3\n      NaN\n      3.0\n      NaN\n      4\n    \n  \n\n\n\n\n\ndf.fillna(value=0) # 0으로 NaN 채우기\n\n\n\n\n\n  \n    \n      \n      A\n      B\n      C\n      D\n    \n  \n  \n    \n      0\n      0.0\n      2.0\n      0.0\n      0\n    \n    \n      1\n      3.0\n      4.0\n      0.0\n      1\n    \n    \n      2\n      0.0\n      0.0\n      0.0\n      5\n    \n    \n      3\n      0.0\n      3.0\n      0.0\n      4\n    \n  \n\n\n\n\n\ndf.fillna(method='ffill') # NaN의 직전 값으로 NaN 채우기. 'pad'를 써도 마찬가지\n\n\n\n\n\n  \n    \n      \n      A\n      B\n      C\n      D\n    \n  \n  \n    \n      0\n      NaN\n      2.0\n      NaN\n      0\n    \n    \n      1\n      3.0\n      4.0\n      NaN\n      1\n    \n    \n      2\n      3.0\n      4.0\n      NaN\n      5\n    \n    \n      3\n      3.0\n      3.0\n      NaN\n      4\n    \n  \n\n\n\n\n\ndf.fillna(method='bfill') # NaN의 다음 값으로 NaN 채우기. 'backfill'을 써도 마찬가지\n\n\n\n\n\n  \n    \n      \n      A\n      B\n      C\n      D\n    \n  \n  \n    \n      0\n      3.0\n      2.0\n      NaN\n      0\n    \n    \n      1\n      3.0\n      4.0\n      NaN\n      1\n    \n    \n      2\n      NaN\n      3.0\n      NaN\n      5\n    \n    \n      3\n      NaN\n      3.0\n      NaN\n      4\n    \n  \n\n\n\n\n\nvalues = {\"A\": 0, \"B\": 1, \"C\": 2, \"D\": 3}\ndf.fillna(value=values) # values에 dictionary를 넣어서 컬럼마다 NaN을 다른 값으로 대체\n\n\n\n\n\n  \n    \n      \n      A\n      B\n      C\n      D\n    \n  \n  \n    \n      0\n      0.0\n      2.0\n      2.0\n      0\n    \n    \n      1\n      3.0\n      4.0\n      2.0\n      1\n    \n    \n      2\n      0.0\n      1.0\n      2.0\n      5\n    \n    \n      3\n      0.0\n      3.0\n      2.0\n      4\n    \n  \n\n\n\n\n\ndf.fillna(value=values, limit=1) # limit=1이어서, 최초의 NaN 하나만 대체\n\n\n\n\n\n  \n    \n      \n      A\n      B\n      C\n      D\n    \n  \n  \n    \n      0\n      0.0\n      2.0\n      2.0\n      0\n    \n    \n      1\n      3.0\n      4.0\n      NaN\n      1\n    \n    \n      2\n      NaN\n      1.0\n      NaN\n      5\n    \n    \n      3\n      NaN\n      3.0\n      NaN\n      4\n    \n  \n\n\n\n\n\ndf2 = pd.DataFrame(np.zeros((4, 4)), columns=list(\"ABCE\"))\ndf2 # 4 by 4 영행렬을 만들어 보자\n\n\n\n\n\n  \n    \n      \n      A\n      B\n      C\n      E\n    \n  \n  \n    \n      0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      1\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      2\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      3\n      0.0\n      0.0\n      0.0\n      0.0\n    \n  \n\n\n\n\n\ndf.fillna(df2) #원본 df의 컬럼이 유지됨\n\n\n\n\n\n  \n    \n      \n      A\n      B\n      C\n      D\n    \n  \n  \n    \n      0\n      0.0\n      2.0\n      0.0\n      0\n    \n    \n      1\n      3.0\n      4.0\n      0.0\n      1\n    \n    \n      2\n      0.0\n      0.0\n      0.0\n      5\n    \n    \n      3\n      0.0\n      3.0\n      0.0\n      4\n    \n  \n\n\n\n\n\ndf.loc[:,'A'].fillna(df.loc[:,'A'].mean()) # A열의 NaN 값을 A열의 평균으로 채움\n\n0    3.0\n1    3.0\n2    3.0\n3    3.0\nName: A, dtype: float64\n\n\n\n\n\n\nDataFrame.drop_duplicates(subset=None, keep='first', inplace=False, ignore_index=False)\n\nsubset : 컬럼 라벨, 혹은 컬럼 라벨 리스트\n\n넣은 특정 컬럼만 중복값을 체크함. 기본으로는 전체 컬럼의 값이 다 같아야 제거\n\nkeep\n\nfirst : 첫 번째 등장한 것을 제외하면 다 제거\nlast : 마지막에 등장한 것을 제외하면 다 제거\nFalse : 몽땅 다 제거\n\ninplace : 원본 변경 할 건가요?\nignore_index : True 값을 넣으면, 결과값의 인덱스를 0, 1, … n-1로 라벨링함\n\n\ndf = pd.DataFrame({\n    'brand': ['Yum Yum', 'Yum Yum', 'Indomie', 'Indomie', 'Indomie'],\n    'style': ['cup', 'cup', 'cup', 'pack', 'pack'],\n    'rating': [4, 4, 3.5, 15, 5]\n})\ndf\n\n\n\n\n\n  \n    \n      \n      brand\n      style\n      rating\n    \n  \n  \n    \n      0\n      Yum Yum\n      cup\n      4.0\n    \n    \n      1\n      Yum Yum\n      cup\n      4.0\n    \n    \n      2\n      Indomie\n      cup\n      3.5\n    \n    \n      3\n      Indomie\n      pack\n      15.0\n    \n    \n      4\n      Indomie\n      pack\n      5.0\n    \n  \n\n\n\n\n\ndf.drop_duplicates() # 모든 열의 값이 다 같으면 제거\n\n\n\n\n\n  \n    \n      \n      brand\n      style\n      rating\n    \n  \n  \n    \n      0\n      Yum Yum\n      cup\n      4.0\n    \n    \n      2\n      Indomie\n      cup\n      3.5\n    \n    \n      3\n      Indomie\n      pack\n      15.0\n    \n    \n      4\n      Indomie\n      pack\n      5.0\n    \n  \n\n\n\n\n\ndf.drop_duplicates(subset=['brand']) # brand 컬럼 하나에서만 값이 같아도 제거\n\n\n\n\n\n  \n    \n      \n      brand\n      style\n      rating\n    \n  \n  \n    \n      0\n      Yum Yum\n      cup\n      4.0\n    \n    \n      2\n      Indomie\n      cup\n      3.5\n    \n  \n\n\n\n\n\ndf.drop_duplicates(subset=['brand', 'style'], keep='last')\n# brand, style 모두 같으면, 마지막 값만 남김\n\n\n\n\n\n  \n    \n      \n      brand\n      style\n      rating\n    \n  \n  \n    \n      1\n      Yum Yum\n      cup\n      4.0\n    \n    \n      2\n      Indomie\n      cup\n      3.5\n    \n    \n      4\n      Indomie\n      pack\n      5.0\n    \n  \n\n\n\n\n\n\n\n\n\n\n\n결론부터 말하자면 데이터를 Dictionary의 리스트로 관리하다가 마지막에 Dataframe으로 만드는 것이 가장 빠르다.\n\nhttps://stackoverflow.com/questions/57000903/what-is-the-fastest-and-most-efficient-way-to-append-rows-to-a-dataframe\n\n  start_time = time.time()\n  dictinary_list = []\n  for i in range(0, end_value, 1):\n      dictionary_data = {k: random.random() for k in range(30)}\n      dictionary_list.append(dictionary_data)\n\n  df_final = pd.DataFrame.from_dict(dictionary_list)\n\n  end_time = time.time()\n  print('Execution time = %.6f seconds' % (end_time-start_time))\n\n그럼 리스트 합치는 건 뭐가 제일 빠르지?\n\nhttps://www.realpythonproject.com/day15-the-fastest-way-to-combine-lists-in-python/\nappend() is the fastest but it doesn’t combine the elements of both the lists. The + operator seems to be the ideal option. However, this has been done on a comparatively smaller dataset and results may vary when you try it on your own.\n\n\n인생은 항상 원하는대로 흘러가지 않기에, 다른 방법도 알아보자.\n\n\n\npandas.concat(objs, axis=0, join='outer', ignore_index=False, keys=None, levels=None, names=None, verify_integrity=False, sort=False, copy=True)\n\ns1 = pd.Series(['a', 'b'])\ns2 = pd.Series(['c', 'd'])\npd.concat([s1, s2]) # Series 두 개 합치기\n\n0    a\n1    b\n0    c\n1    d\ndtype: object\n\n\n\npd.concat([s1, s2], ignore_index=True) # 합치면서 index 새로 만들어줌\n\n0    a\n1    b\n2    c\n3    d\ndtype: object\n\n\n\ns3 = pd.concat([s1, s2], keys=['s1', 's2']) # 최외각 레벨에 새로운 index를 만들어줌\nprint(s3)\nprint(s3['s1']) \nprint(s3['s2'][0]) # 이렇게 조회가능\n\ns1  0    a\n    1    b\ns2  0    c\n    1    d\ndtype: object\n0    a\n1    b\ndtype: object\nc\n\n\n\ns3 = pd.concat([s1, s2], keys=['s1', 's2'], names=['Series name', 'Row ID'])\n# index에 이름 붙이기\nprint(s3)\nprint(s3.index)\nprint(s3.index.names)\n\nSeries name  Row ID\ns1           0         a\n             1         b\ns2           0         c\n             1         d\ndtype: object\nMultiIndex([('s1', 0),\n            ('s1', 1),\n            ('s2', 0),\n            ('s2', 1)],\n           names=['Series name', 'Row ID'])\n['Series name', 'Row ID']\n\n\n\ndf1 = pd.DataFrame([['a', 1], ['b', 2]], columns=['letter', 'number'])\nprint(df1)\ndf2 = pd.DataFrame([['c', 3], ['d', 4]], columns=['letter', 'number'])\nprint(df2)\npd.concat([df1, df2]) # Dataframe 합치기\n\n  letter  number\n0      a       1\n1      b       2\n  letter  number\n0      c       3\n1      d       4\n\n\n\n\n\n\n  \n    \n      \n      letter\n      number\n    \n  \n  \n    \n      0\n      a\n      1\n    \n    \n      1\n      b\n      2\n    \n    \n      0\n      c\n      3\n    \n    \n      1\n      d\n      4\n    \n  \n\n\n\n\n\ndf3 = pd.DataFrame([['c', 3, 'cat'], ['d', 4, 'dog']],\n                   columns=['letter', 'number', 'animal'])\nprint(df3)\npd.concat([df1, df3], sort=False) # 한 쪽에 없는 컬럼의 값은 NaN으로 삽입됨\n\n  letter  number animal\n0      c       3    cat\n1      d       4    dog\n\n\n\n\n\n\n  \n    \n      \n      letter\n      number\n      animal\n    \n  \n  \n    \n      0\n      a\n      1\n      NaN\n    \n    \n      1\n      b\n      2\n      NaN\n    \n    \n      0\n      c\n      3\n      cat\n    \n    \n      1\n      d\n      4\n      dog\n    \n  \n\n\n\n\n\npd.concat([df1, df3], join=\"inner\") # join=\"inner\"로 하면 양쪽에 다 있는 컬럼만 합쳐서 반환함\n\n\n\n\n\n  \n    \n      \n      letter\n      number\n    \n  \n  \n    \n      0\n      a\n      1\n    \n    \n      1\n      b\n      2\n    \n    \n      0\n      c\n      3\n    \n    \n      1\n      d\n      4\n    \n  \n\n\n\n\n\ndf4 = pd.DataFrame([['bird', 'polly'], ['monkey', 'george']],\n                   columns=['animal', 'name'])\npd.concat([df1, df4], axis=1) # axis=1이면 컬럼을 붙임\n\n\n\n\n\n  \n    \n      \n      letter\n      number\n      animal\n      name\n    \n  \n  \n    \n      0\n      a\n      1\n      bird\n      polly\n    \n    \n      1\n      b\n      2\n      monkey\n      george\n    \n  \n\n\n\n\n\ndf4 = pd.DataFrame([['bird', 'polly'], ['monkey', 'george'], ['dog', 'sam']],\n                   columns=['animal', 'name'])\npd.concat([df1, df4], axis=1) \n# axis=1이면 컬럼을 붙임\n# 행의 수가 다르면, 행이 적은 쪽에 NaN이 삽입된 행이 추가됨\n\n\n\n\n\n  \n    \n      \n      letter\n      number\n      animal\n      name\n    \n  \n  \n    \n      0\n      a\n      1.0\n      bird\n      polly\n    \n    \n      1\n      b\n      2.0\n      monkey\n      george\n    \n    \n      2\n      NaN\n      NaN\n      dog\n      sam\n    \n  \n\n\n\n\n\n#collapse-output\ndf5 = pd.DataFrame([1], index=['a'])\ndf6 = pd.DataFrame([2], index=['a'])\npd.concat([df5, df6], verify_integrity=True)\n# verify_integrity=True를 하면, index가 같은 것을 허용하지 않음.\n\nValueError: Indexes have overlapping values: Index(['a'], dtype='object')\n\n\n\n\n\n\nDataFrame.merge(right, how='inner', on=None, left_on=None, right_on=None, left_index=False, right_index=False, sort=False, suffixes=('_x', '_y'), copy=True, indicator=False, validate=None)\n\nright : 합칠 Dataframe\nhow : {‘left’, ‘right’, ‘outer’, ‘inner’, ‘cross’}, default ‘inner’\n\nleft: use only keys from left frame, similar to a SQL left outer join; preserve key order.\nright: use only keys from right frame, similar to a SQL right outer join; preserve key order.\nouter: use union of keys from both frames, similar to a SQL full outer join; sort keys lexicographically.\ninner: use intersection of keys from both frames, similar to a SQL inner join; preserve the order of the left keys.\ncross: creates the cartesian product from both frames, preserves the order of the left keys.\n\non : label or list\n\n조인할 컬럼이나 인덱스 레벨의 이름. 두 Dataframe에 무조건 있어야 한다.\n\nleft_on : label or list, or array-like\n\n왼쪽 Dataframe의 조인할 컬럼이나 인덱스 레벨의 이름.\n\nright_on : label or list, or array-like\n\n오른쪽 Dataframe의 조인할 컬럼이나 인덱스 레벨의 이름.\n\nleft_index : bool, default False\n\n왼쪽 index를 조인의 key로 사용할까요?\nMultiIndex인 경우, 상대 Dataframe의 key 수가 level의 수와 동일해야 함.\n\nright_index : bool, default False\n\n오른쪽 index를 조인의 key로 사용할까요?\nMultiIndex인 경우, 상대 Dataframe의 key 수가 level의 수와 동일해야 함.\n\nsort : bool, default False\n\n조인 결과 Dataframe에서 key를 사전 순서(lexicographically)로 배열함\nFalse인 경우, 조인 방법에 정의된 방법을 따라감\n\nsuffixes : list-like, default is (“_x”, “_y”)\n\n컬럼에 접미사를 붙인다. 왼쪽 오른쪽 구분용.\n기본적으로 왼쪽에 _x, 오른쪽에 _y가 붙는다.\n\ncopy : bool, default True\n\nFalse면, 가능하면 복사를 피한다.\n\nindicator : bool or str, default False\nvalidate : str, optional\n\nIf specified, checks if merge is of specified type.\n\n“one_to_one” or “1:1”: check if merge keys are unique in both left and right datasets.\n“one_to_many” or “1:m”: check if merge keys are unique in left dataset.\n“many_to_one” or “m:1”: check if merge keys are unique in right dataset.\n“many_to_many” or “m:m”: allowed, but does not result in checks.\n\n\n\n\ndf1 = pd.DataFrame({'lkey': ['foo', 'bar', 'baz', 'foo'], 'value': [1, 2, 3, 5]})\ndf2 = pd.DataFrame({'rkey': ['foo', 'bar', 'baz', 'foo'], 'value': [5, 6, 7, 8]})\ndisplay(df1)\ndisplay(df2)\n\n\n\n\n\n  \n    \n      \n      lkey\n      value\n    \n  \n  \n    \n      0\n      foo\n      1\n    \n    \n      1\n      bar\n      2\n    \n    \n      2\n      baz\n      3\n    \n    \n      3\n      foo\n      5\n    \n  \n\n\n\n\n\n\n\n\n  \n    \n      \n      rkey\n      value\n    \n  \n  \n    \n      0\n      foo\n      5\n    \n    \n      1\n      bar\n      6\n    \n    \n      2\n      baz\n      7\n    \n    \n      3\n      foo\n      8\n    \n  \n\n\n\n\n\ndisplay(df1.merge(df2, left_on='lkey', right_on='rkey'))\n# 뒤에 _x, _y가 붙은 것을 확인.\ndf1.merge(df2, left_on='lkey', right_on='rkey', suffixes=('_left', '_right'))\n# _left, _right로 바꿔 보았음\n\n\n\n\n\n  \n    \n      \n      lkey\n      value_x\n      rkey\n      value_y\n    \n  \n  \n    \n      0\n      foo\n      1\n      foo\n      5\n    \n    \n      1\n      foo\n      1\n      foo\n      8\n    \n    \n      2\n      foo\n      5\n      foo\n      5\n    \n    \n      3\n      foo\n      5\n      foo\n      8\n    \n    \n      4\n      bar\n      2\n      bar\n      6\n    \n    \n      5\n      baz\n      3\n      baz\n      7\n    \n  \n\n\n\n\n\n\n\n\n  \n    \n      \n      lkey\n      value_left\n      rkey\n      value_right\n    \n  \n  \n    \n      0\n      foo\n      1\n      foo\n      5\n    \n    \n      1\n      foo\n      1\n      foo\n      8\n    \n    \n      2\n      foo\n      5\n      foo\n      5\n    \n    \n      3\n      foo\n      5\n      foo\n      8\n    \n    \n      4\n      bar\n      2\n      bar\n      6\n    \n    \n      5\n      baz\n      3\n      baz\n      7\n    \n  \n\n\n\n\n\ndf1 = pd.DataFrame({'a': ['foo', 'bar'], 'b': [1, 2]})\ndf2 = pd.DataFrame({'a': ['foo', 'baz'], 'c': [3, 4]})\ndisplay(df1)\ndisplay(df2)\n\n\n\n\n\n  \n    \n      \n      a\n      b\n    \n  \n  \n    \n      0\n      foo\n      1\n    \n    \n      1\n      bar\n      2\n    \n  \n\n\n\n\n\n\n\n\n  \n    \n      \n      a\n      c\n    \n  \n  \n    \n      0\n      foo\n      3\n    \n    \n      1\n      baz\n      4\n    \n  \n\n\n\n\n\ndf1.merge(df2, how='inner', on='a')\n# a 컬럼을 키로 잡음. 두 Dataframe에 다 a 컬럼이 있어서 가능한것\n# inner join\n\n\n\n\n\n  \n    \n      \n      a\n      b\n      c\n    \n  \n  \n    \n      0\n      foo\n      1\n      3\n    \n  \n\n\n\n\n\ndf1.merge(df2, how='left', on='a')\n# left outer join\n\n\n\n\n\n  \n    \n      \n      a\n      b\n      c\n    \n  \n  \n    \n      0\n      foo\n      1\n      3.0\n    \n    \n      1\n      bar\n      2\n      NaN\n    \n  \n\n\n\n\n\ndf1.merge(df2, how='left', left_on='a', right_on='a')\n# left outer join\n\n\n\n\n\n  \n    \n      \n      a\n      b\n      c\n    \n  \n  \n    \n      0\n      foo\n      1\n      3.0\n    \n    \n      1\n      bar\n      2\n      NaN\n    \n  \n\n\n\n\n\n\n\nhttps://stackoverflow.com/questions/40860457/improve-pandas-merge-performance\nkey를 index로 사용한다.\n\nindex 검색 시에는 hash table을 이용하기 때문\nA short explanation why it is faster to merge by index instead of by a “normal” column: Indices have a hash table. Meaning you can look them up in amortized O(1). For a normal column you need O(n) in worst case, meaning merging two dfs with len n takes O(n^2) in worst case.\n\njoin을 쓴다.\nconcat을 쓴다.\n\n여기서의 결론 : key를 index로 사용한 후 join을 쓴다.\n\nimport random\ndf1 = pd.DataFrame({'uid_sample': random.sample(range(100000), 80000), 'value': random.sample(range(10000000), 80000)})\ndf2 = pd.DataFrame({'userId_sample2': random.sample(range(100000), 80000), 'value': random.sample(range(10000000), 80000)})\n# 80000명의 정보를 담고 있는 두 Dataframe이 있다고 하자.\n# uid_sample, userId_sample2를 key로 조인하고 싶다.\n\n\n%%timeit\ndf1.merge(df2, how='left', left_on='uid_sample', right_on='userId_sample2')\n\n19.4 ms ± 981 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\n\n%%timeit\n# key로 사용하려는 컬럼을 index로 할당\n# 36%정도 빨라졌다!\ndf3 = df1.set_index('uid_sample')\ndf4 = df2.set_index('userId_sample2')\ndf3.merge(df4, right_index=True, left_index=True)\n\n12.6 ms ± 181 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\n\n%%timeit\n# key로 사용하려는 컬럼을 index로 할당\n# join 함수 사용\n# 여기서 이미 2.5배 빨라졌다\ndf3 = df1.set_index('uid_sample')\ndf4 = df2.set_index('userId_sample2')\ndf3.join(df4, how='left', lsuffix='left', rsuffix='right')\n\n8.04 ms ± 1.21 ms per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\n\n%%timeit\n# inner, outer밖에 안 되는데 join보다 느리다.\ndf3 = df1.set_index('uid_sample')\ndf4 = df2.set_index('userId_sample2')\npd.concat([df3, df4], axis=1, join='inner')\n\n11.5 ms ± 341 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\n\n\n\n\nDataFrame.join(other, on=None, how='left', lsuffix='', rsuffix='', sort=False)\n\nother : 다른 데이터프레임, 혹은 시리즈, 혹은 데이터프레임 리스트\n\n함수를 호출한 데이터프레임에 붙일 대상\n\non : 조인 키가 될 컬럼 이름, 혹은 컬럼 이름 리스트(array-like 자료형이면 됨)\nhow\n\nleft : 함수를 호출한 데이터프레임(caller)의 index를 조인 키로 사용. on에서 컬럼을 지정했을 경우, 그 컬럼을 사용\nright : other 패러미터에 할당된 객체의 index를 사용\nouter : outer join 실행 후, 사전 순으로 정렬함. 기본적으론 양쪽 다 index를 사용. on에서 지정하면 caller만 해당 컬럼 사용.\ninner : inner join 실행. caller의 순서 보존됨.\ncross : 양쪽의 곱집합 생성. left key(caller)의 순서 보존됨.\n\nlsuffix, rsuffix : join된 결과물 컬럼의 접미사 세팅.\nsort : TRUE면, join key의 사전 순서대로 정렬됨. FALSE면, how에서의 기본 처리방식을 따름.\n\n\ncaller = pd.DataFrame({'key': ['K0', 'K1', 'K2', 'K3', 'K4', 'K5'], 'A': ['A0', 'A1', 'A2', 'A3', 'A4', 'A5']})\nother = pd.DataFrame({'key': ['K0', 'K1', 'K2'], 'B': ['B0', 'B1', 'B2']})\nother2 = pd.DataFrame({'key': ['K0', 'K1', 'K2'], 'C': ['C0', 'C1', 'C2']})\n\n\ncaller_styler = caller.style.set_table_attributes(\"style='display:inline;margin:5px'\").set_caption('caller')\nother_styler = other.style.set_table_attributes(\"style='display:inline;margin:5px'\").set_caption('other')\nother2_styler = other2.style.set_table_attributes(\"style='display:inline;margin:5px'\").set_caption('other2')\ndisplay_html(caller_styler._repr_html_() + other_styler._repr_html_() + other2_styler._repr_html_(), raw=True)\n\n\ncaller                    key        A    \n                \n                        0\n                        K0\n                        A0\n            \n            \n                        1\n                        K1\n                        A1\n            \n            \n                        2\n                        K2\n                        A2\n            \n            \n                        3\n                        K3\n                        A3\n            \n            \n                        4\n                        K4\n                        A4\n            \n            \n                        5\n                        K5\n                        A5\n            \n    other                    key        B    \n                \n                        0\n                        K0\n                        B0\n            \n            \n                        1\n                        K1\n                        B1\n            \n            \n                        2\n                        K2\n                        B2\n            \n    other2                    key        C    \n                \n                        0\n                        K0\n                        C0\n            \n            \n                        1\n                        K1\n                        C1\n            \n            \n                        2\n                        K2\n                        C2\n            \n    \n\n\n\n# 이러면 index 0,1,2,3,4,5 기준으로 join됨\ncaller.join(other, lsuffix='_caller', rsuffix='_other')\n\n\n\n\n\n  \n    \n      \n      key_caller\n      A\n      key_other\n      B\n    \n  \n  \n    \n      0\n      K0\n      A0\n      K0\n      B0\n    \n    \n      1\n      K1\n      A1\n      K1\n      B1\n    \n    \n      2\n      K2\n      A2\n      K2\n      B2\n    \n    \n      3\n      K3\n      A3\n      NaN\n      NaN\n    \n    \n      4\n      K4\n      A4\n      NaN\n      NaN\n    \n    \n      5\n      K5\n      A5\n      NaN\n      NaN\n    \n  \n\n\n\n\n\n# set_index로 조인 키로 쓰고 싶은 컬럼을 index로 만들어주는 방법이 있음\ncaller.set_index('key').join(other.set_index('key'))\n\n\n\n\n\n  \n    \n      \n      A\n      B\n    \n    \n      key\n      \n      \n    \n  \n  \n    \n      K0\n      A0\n      B0\n    \n    \n      K1\n      A1\n      B1\n    \n    \n      K2\n      A2\n      B2\n    \n    \n      K3\n      A3\n      NaN\n    \n    \n      K4\n      A4\n      NaN\n    \n    \n      K5\n      A5\n      NaN\n    \n  \n\n\n\n\n\n# 혹은, other만 조인 키로 쓰고 싶은 컬럼을 index로 만들어 주고,\n# on에다 조인 키로 쓰고 싶은 caller의 컬럼을 할당하는 방법이 있음\ncaller.join(other.set_index('key'), on='key')\n\n\n\n\n\n  \n    \n      \n      key\n      A\n      B\n    \n  \n  \n    \n      0\n      K0\n      A0\n      B0\n    \n    \n      1\n      K1\n      A1\n      B1\n    \n    \n      2\n      K2\n      A2\n      B2\n    \n    \n      3\n      K3\n      A3\n      NaN\n    \n    \n      4\n      K4\n      A4\n      NaN\n    \n    \n      5\n      K5\n      A5\n      NaN\n    \n  \n\n\n\n\n\n# 한 번에 여러 개의 Dataframe을 Join 할 때에는, index로 join하는 것만 지원한다.\n# 즉, on을 쓰지 못한다는 이야기이다.\ncaller.set_index('key').join([other.set_index('key'), other2.set_index('key')])\n\n\n\n\n\n  \n    \n      \n      A\n      B\n      C\n    \n    \n      key\n      \n      \n      \n    \n  \n  \n    \n      K0\n      A0\n      B0\n      C0\n    \n    \n      K1\n      A1\n      B1\n      C1\n    \n    \n      K2\n      A2\n      B2\n      C2\n    \n    \n      K3\n      A3\n      NaN\n      NaN\n    \n    \n      K4\n      A4\n      NaN\n      NaN\n    \n    \n      K5\n      A5\n      NaN\n      NaN\n    \n  \n\n\n\n\n\n\n\n\n\n\nDataFrame.pivot(index=None, columns=None, values=None)\n\nindex : str or object or a list of str, optional\n\n새로운 프레임의 index로 사용할 컬럼\n\ncolumns : str of object or a list of str\n\n새로운 프레임의 컬럼으로 사용할 컬럼\n\nvalues : str, object or a list of the previous, optional\n\n새로운 프레임의 값을 계산하기 위해 사용하는 컬럼\n지정하지 않으면, 남아있는 모든 컬럼을 사용한다.\n\n\n\ndf = pd.DataFrame({'foo': ['one', 'one', 'one', 'two', 'two',\n                           'two'],\n                   'bar': ['A', 'B', 'C', 'A', 'B', 'C'],\n                   'baz': [1, 2, 3, 4, 5, 6],\n                   'zoo': ['x', 'y', 'z', 'q', 'w', 't']})\ndf\n\n\n\n\n\n  \n    \n      \n      foo\n      bar\n      baz\n      zoo\n    \n  \n  \n    \n      0\n      one\n      A\n      1\n      x\n    \n    \n      1\n      one\n      B\n      2\n      y\n    \n    \n      2\n      one\n      C\n      3\n      z\n    \n    \n      3\n      two\n      A\n      4\n      q\n    \n    \n      4\n      two\n      B\n      5\n      w\n    \n    \n      5\n      two\n      C\n      6\n      t\n    \n  \n\n\n\n\n\ndf.pivot(index='foo', columns='bar', values='baz')\n\n\n\n\n\n  \n    \n      bar\n      A\n      B\n      C\n    \n    \n      foo\n      \n      \n      \n    \n  \n  \n    \n      one\n      1\n      2\n      3\n    \n    \n      two\n      4\n      5\n      6\n    \n  \n\n\n\n\n\ndf.pivot(index='foo', columns='bar')\n\n\n\n\n\n  \n    \n      \n      baz\n      zoo\n    \n    \n      bar\n      A\n      B\n      C\n      A\n      B\n      C\n    \n    \n      foo\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      one\n      1\n      2\n      3\n      x\n      y\n      z\n    \n    \n      two\n      4\n      5\n      6\n      q\n      w\n      t\n    \n  \n\n\n\n\n\ndf.pivot(index='foo', columns='bar')['baz']\n\n\n\n\n\n  \n    \n      bar\n      A\n      B\n      C\n    \n    \n      foo\n      \n      \n      \n    \n  \n  \n    \n      one\n      1\n      2\n      3\n    \n    \n      two\n      4\n      5\n      6\n    \n  \n\n\n\n\n\ndf = pd.DataFrame({\n       \"lev1\": [1, 1, 1, 2, 2, 2],\n       \"lev2\": [1, 1, 2, 1, 1, 2],\n       \"lev3\": [1, 2, 1, 2, 1, 2],\n       \"lev4\": [1, 2, 3, 4, 5, 6],\n       \"values\": [0, 1, 2, 3, 4, 5]})\ndf\n\n\n\n\n\n  \n    \n      \n      lev1\n      lev2\n      lev3\n      lev4\n      values\n    \n  \n  \n    \n      0\n      1\n      1\n      1\n      1\n      0\n    \n    \n      1\n      1\n      1\n      2\n      2\n      1\n    \n    \n      2\n      1\n      2\n      1\n      3\n      2\n    \n    \n      3\n      2\n      1\n      2\n      4\n      3\n    \n    \n      4\n      2\n      1\n      1\n      5\n      4\n    \n    \n      5\n      2\n      2\n      2\n      6\n      5\n    \n  \n\n\n\n\n\ndf.pivot(index=\"lev1\", columns=[\"lev2\", \"lev3\"] ,values=\"values\")\n# Multilevel Column\n# 해당하는 조건에 맞는 값이 없으면 NaN이 들어가게 됨\n\n\n\n\n\n  \n    \n      lev2\n      1\n      2\n    \n    \n      lev3\n      1\n      2\n      1\n      2\n    \n    \n      lev1\n      \n      \n      \n      \n    \n  \n  \n    \n      1\n      0.0\n      1.0\n      2.0\n      NaN\n    \n    \n      2\n      4.0\n      3.0\n      NaN\n      5.0\n    \n  \n\n\n\n\n\ndf.pivot(index=[\"lev1\", \"lev2\"], columns=[\"lev3\"],values=\"values\")\n# Multiindex\n\n\n\n\n\n  \n    \n      \n      lev3\n      1\n      2\n    \n    \n      lev1\n      lev2\n      \n      \n    \n  \n  \n    \n      1\n      1\n      0.0\n      1.0\n    \n    \n      2\n      2.0\n      NaN\n    \n    \n      2\n      1\n      4.0\n      3.0\n    \n    \n      2\n      NaN\n      5.0\n    \n  \n\n\n\n\n\n#collapse-output\ndf.pivot(index=[\"lev1\"], columns=[\"lev2\"],values=\"values\")\n# 인덱스, 컬럼 쌍에 중복이 발생하면 에러가 출력됨\n# ValueError: Index contains duplicate entries, cannot reshape\n\nValueError: Index contains duplicate entries, cannot reshape\n\n\n\n\n\npandas.pivot_table(data, values=None, index=None, columns=None, aggfunc='mean', fill_value=None, margins=False, dropna=True, margins_name='All', observed=False, sort=True)\n\n#hide_input\ndf = pd.DataFrame({\"A\": [\"foo\", \"foo\", \"foo\", \"foo\", \"foo\",\n                         \"bar\", \"bar\", \"bar\", \"bar\"],\n                   \"B\": [\"one\", \"one\", \"one\", \"two\", \"two\",\n                         \"one\", \"one\", \"two\", \"two\"],\n                   \"C\": [\"small\", \"large\", \"large\", \"small\",\n                         \"small\", \"large\", \"small\", \"small\",\n                         \"large\"],\n                   \"D\": [1, 2, 2, 3, 3, 4, 5, 6, 7],\n                   \"E\": [2, 4, 5, 5, 6, 6, 8, 9, 9]})\ndf\n\n\n\n\n\n  \n    \n      \n      A\n      B\n      C\n      D\n      E\n    \n  \n  \n    \n      0\n      foo\n      one\n      small\n      1\n      2\n    \n    \n      1\n      foo\n      one\n      large\n      2\n      4\n    \n    \n      2\n      foo\n      one\n      large\n      2\n      5\n    \n    \n      3\n      foo\n      two\n      small\n      3\n      5\n    \n    \n      4\n      foo\n      two\n      small\n      3\n      6\n    \n    \n      5\n      bar\n      one\n      large\n      4\n      6\n    \n    \n      6\n      bar\n      one\n      small\n      5\n      8\n    \n    \n      7\n      bar\n      two\n      small\n      6\n      9\n    \n    \n      8\n      bar\n      two\n      large\n      7\n      9\n    \n  \n\n\n\n\n\ntable = pd.pivot_table(df, values='D', index=['A', 'B'], columns=['C'], aggfunc=np.sum)\ntable # aggfunc에 집계함수를 넣게 된다. 여기서는 총합\n\n\n\n\n\n  \n    \n      \n      C\n      large\n      small\n    \n    \n      A\n      B\n      \n      \n    \n  \n  \n    \n      bar\n      one\n      4.0\n      5.0\n    \n    \n      two\n      7.0\n      6.0\n    \n    \n      foo\n      one\n      4.0\n      1.0\n    \n    \n      two\n      NaN\n      6.0\n    \n  \n\n\n\n\n\ntable = pd.pivot_table(df, values='D', index=['A', 'B'],\n                    columns=['C'], aggfunc=np.sum, fill_value=0)\ntable # fill_value에 할당된 값으로 NaN을 대체하게 됨\n\n\n\n\n\n  \n    \n      \n      C\n      large\n      small\n    \n    \n      A\n      B\n      \n      \n    \n  \n  \n    \n      bar\n      one\n      4\n      5\n    \n    \n      two\n      7\n      6\n    \n    \n      foo\n      one\n      4\n      1\n    \n    \n      two\n      0\n      6\n    \n  \n\n\n\n\n\ntable = pd.pivot_table(df, values=['D', 'E'], index=['A', 'C'],\n                    aggfunc={'D': np.mean,\n                             'E': np.sum})\ntable # aggfunc에 Dictionary를 할당하여 값마다 집계함수를 각각 다르게 설정할 수 있다.\n\n\n\n\n\n  \n    \n      \n      \n      D\n      E\n    \n    \n      A\n      C\n      \n      \n    \n  \n  \n    \n      bar\n      large\n      5.500000\n      15\n    \n    \n      small\n      5.500000\n      17\n    \n    \n      foo\n      large\n      2.000000\n      9\n    \n    \n      small\n      2.333333\n      13\n    \n  \n\n\n\n\n\ntable = pd.pivot_table(df, values=['D', 'E'], index=['A', 'C'],\n                    aggfunc={'D': np.mean,\n                             'E': [min, max, np.mean]})\ntable # 한 값에 여러 개의 집계함수 할당도 가능하다.\n\n\n\n\n\n  \n    \n      \n      \n      D\n      E\n    \n    \n      \n      \n      mean\n      max\n      mean\n      min\n    \n    \n      A\n      C\n      \n      \n      \n      \n    \n  \n  \n    \n      bar\n      large\n      5.500000\n      9.0\n      7.500000\n      6.0\n    \n    \n      small\n      5.500000\n      9.0\n      8.500000\n      8.0\n    \n    \n      foo\n      large\n      2.000000\n      5.0\n      4.500000\n      4.0\n    \n    \n      small\n      2.333333\n      6.0\n      4.333333\n      2.0\n    \n  \n\n\n\n\n\ntable = pd.pivot_table(df, values=['D', 'E'], index=['A', 'C'],\n                    aggfunc={'D': np.mean,\n                             'E': np.mean},\n                    margins=True, margins_name=\"mean\")\ntable # Values에 적용된 집계함수를 컬럼 전체에 적용한 행을 추가한다.\n# 한 Value에 집계함수를 하나만 사용했을 때 적용 가능.\n# margins_name을 지정하지 않으면 기본적으로 행 Index 이름은 All이 된다.\n\n\n\n\n\n  \n    \n      \n      \n      D\n      E\n    \n    \n      A\n      C\n      \n      \n    \n  \n  \n    \n      bar\n      large\n      5.500000\n      7.500000\n    \n    \n      small\n      5.500000\n      8.500000\n    \n    \n      foo\n      large\n      2.000000\n      4.500000\n    \n    \n      small\n      2.333333\n      4.333333\n    \n    \n      mean\n      \n      3.666667\n      6.000000\n    \n  \n\n\n\n\n\n\n\npandas.melt(frame, id_vars=None, value_vars=None, var_name=None, value_name='value', col_level=None, ignore_index=True)\n\nid_vars : tuple, list, or ndarray, optional\n\n식별자로 사용할 컬럼\n\nvalue_vars : tuple, list, or ndarray, optional\n\nUnpivot 할 컬럼. 지정하지 않으면, id_vars에 할당되지 않은 모든 컬럼을 사용\n\n\n\ndf = pd.DataFrame({'A': {0: 'a', 1: 'b', 2: 'c'},\n                   'B': {0: 1, 1: 3, 2: 5},\n                   'C': {0: 2, 1: 4, 2: 6}})\ndf\n\n\n\n\n\n  \n    \n      \n      A\n      B\n      C\n    \n  \n  \n    \n      0\n      a\n      1\n      2\n    \n    \n      1\n      b\n      3\n      4\n    \n    \n      2\n      c\n      5\n      6\n    \n  \n\n\n\n\n\npd.melt(df, id_vars=['A'], value_vars=['B'])\n\n\n\n\n\n  \n    \n      \n      A\n      variable\n      value\n    \n  \n  \n    \n      0\n      a\n      B\n      1\n    \n    \n      1\n      b\n      B\n      3\n    \n    \n      2\n      c\n      B\n      5\n    \n  \n\n\n\n\n\npd.melt(df, id_vars=['A'], value_vars=['B', 'C'])\n\n\n\n\n\n  \n    \n      \n      A\n      variable\n      value\n    \n  \n  \n    \n      0\n      a\n      B\n      1\n    \n    \n      1\n      b\n      B\n      3\n    \n    \n      2\n      c\n      B\n      5\n    \n    \n      3\n      a\n      C\n      2\n    \n    \n      4\n      b\n      C\n      4\n    \n    \n      5\n      c\n      C\n      6\n    \n  \n\n\n\n\n\npd.melt(df, id_vars=['A'], value_vars=['B'],\n        var_name='myVarname', value_name='myValname')\n# 이름은 커스터마이징 가능\n\n\n\n\n\n  \n    \n      \n      A\n      myVarname\n      myValname\n    \n  \n  \n    \n      0\n      a\n      B\n      1\n    \n    \n      1\n      b\n      B\n      3\n    \n    \n      2\n      c\n      B\n      5\n    \n  \n\n\n\n\n\npd.melt(df, id_vars=['A'], value_vars=['B', 'C'], ignore_index=False)\n# 원본 index 유지\n\n\n\n\n\n  \n    \n      \n      A\n      variable\n      value\n    \n  \n  \n    \n      0\n      a\n      B\n      1\n    \n    \n      1\n      b\n      B\n      3\n    \n    \n      2\n      c\n      B\n      5\n    \n    \n      0\n      a\n      C\n      2\n    \n    \n      1\n      b\n      C\n      4\n    \n    \n      2\n      c\n      C\n      6\n    \n  \n\n\n\n\n\n\n\n\n\n\n\ndf = pd.DataFrame({'float': [1.0],\n                   'int': [1],\n                   'datetime': [pd.Timestamp('20180310')],\n                   'string': ['foo']})\ndf.dtypes\n# 더 이상의 설명은 필요 없다!\n\nfloat              float64\nint                  int64\ndatetime    datetime64[ns]\nstring              object\ndtype: object\n\n\n\n\n\nDataFrame.select_dtypes(include=None, exclude=None)\n\nTo select all numeric types, use np.number or ‘number’\nTo select strings you must use the object dtype, but note that this will return all object dtype columns See the numpy dtype hierarchy\nTo select datetimes, use np.datetime64, ‘datetime’ or ‘datetime64’\nTo select timedeltas, use np.timedelta64, ‘timedelta’ or ‘timedelta64’\nTo select Pandas categorical dtypes, use ‘category’\nTo select Pandas datetimetz dtypes, use ‘datetimetz’ (new in 0.20.0) or ‘datetime64[ns, tz]’\n\n\ndf = pd.DataFrame({'a': [1, 2] * 3,\n                   'b': [True, False] * 3,\n                   'c': [1.0, 2.0] * 3})\ndf\n\n\n\n\n\n  \n    \n      \n      a\n      b\n      c\n    \n  \n  \n    \n      0\n      1\n      True\n      1.0\n    \n    \n      1\n      2\n      False\n      2.0\n    \n    \n      2\n      1\n      True\n      1.0\n    \n    \n      3\n      2\n      False\n      2.0\n    \n    \n      4\n      1\n      True\n      1.0\n    \n    \n      5\n      2\n      False\n      2.0\n    \n  \n\n\n\n\n\ndf.select_dtypes(include='bool')\n\n\n\n\n\n  \n    \n      \n      b\n    \n  \n  \n    \n      0\n      True\n    \n    \n      1\n      False\n    \n    \n      2\n      True\n    \n    \n      3\n      False\n    \n    \n      4\n      True\n    \n    \n      5\n      False\n    \n  \n\n\n\n\n\ndf.select_dtypes(include=['float64'])\n\n\n\n\n\n  \n    \n      \n      c\n    \n  \n  \n    \n      0\n      1.0\n    \n    \n      1\n      2.0\n    \n    \n      2\n      1.0\n    \n    \n      3\n      2.0\n    \n    \n      4\n      1.0\n    \n    \n      5\n      2.0\n    \n  \n\n\n\n\n\ndf.select_dtypes(exclude=['int64'])\n\n\n\n\n\n  \n    \n      \n      b\n      c\n    \n  \n  \n    \n      0\n      True\n      1.0\n    \n    \n      1\n      False\n      2.0\n    \n    \n      2\n      True\n      1.0\n    \n    \n      3\n      False\n      2.0\n    \n    \n      4\n      True\n      1.0\n    \n    \n      5\n      False\n      2.0\n    \n  \n\n\n\n\n\n\n\nDataFrame.astype(dtype, copy=True, errors='raise')\n\ncopy : False를 하면, 복사를 하는 게 아니고 원본에 연결되므로 변경사항이 원본에까지 전파됨\nerrors : ignore로 세팅하면, 에러 발생 시 원본을 반환하고 끝냄\n\n\nd = {'col1': [1, 2], 'col2': [3, 4]}\ndf = pd.DataFrame(data=d)\ndf.dtypes\n\ncol1    int64\ncol2    int64\ndtype: object\n\n\n\ndf.astype({'col1': 'int32'}).dtypes\n# 잘 변경됐습니다~\n\ncol1    int32\ncol2    int64\ndtype: object\n\n\n\n\n\n\n\n\n\nstring_quest = pd.read_excel(r\"C:\\Users\\limyj0708\\Documents\\data\\string\\string_quest.xlsx\",\n                             header=6, usecols=\"H,I\", sheet_name = \"string_quest\", engine=\"openpyxl\")\n\n\nheader : 몇 번째 row를 header로 할까?\nusercols : 어떤 열을 가져올까?\nsheet_name : 어떤 시트를 가져올까?\nengine : openpyxl을 사용하여야 xlsx 파일의 불러오기가 가능\n\n\n\n\n\n\n\ndataframe-image 패키지를 이용 시, linux에서 crontab으로 실행할 경우 복잡한 권한 문제에 직면하게 됨\ndataframe-image 패키지도 matplotlib 기반이므로, 그냥 matplotlib를 사용\n\n\nimport six\nimport matplotlib.pyplot as plt\nimport matplotlib.font_manager as fm\n\n# matplotlib에서 한글이 안 나오는 문제 해결\nNANUM = fm.FontProperties(fname=r'C:\\Users\\limyj0708\\AppData\\Local\\Microsoft\\Windows\\Fonts\\NanumBarunGothic.ttf')\nNANUM_bold = fm.FontProperties(fname=r'C:\\Users\\limyj0708\\AppData\\Local\\Microsoft\\Windows\\Fonts\\NanumBarunGothicBold.ttf')\n\n# centos라면 폰트 경로는 아래와 같음\n ## /usr/share/fonts/NanumFont/NanumBarunGothic.ttf\n ## /usr/share/fonts/NanumFont/NanumGothicBold.ttf\n\ndef render_mpl_table(data, col_width=3.0, row_height=0.625, font_size_header=16, font_size=14,\n                     header_color='#C2DED1', row_colors=['#f1f1f2', 'w'], edge_color='black',\n                     bbox=[0, 0, 1, 1], header_columns=0,\n                     ax=None, align_head='center', align_cell='center', **kwargs):\n    \"\"\"\n    align_head, align_cell : [ 'center' | 'right' | 'left' ] \n    \"\"\"\n    \n    if ax is None:\n        size = (np.array(data.shape[::-1]) + np.array([0, 1])) * np.array([col_width, row_height])\n        fig, ax = plt.subplots(figsize=size)\n        ax.axis('off')\n\n    mpl_table = ax.table(cellText=data.values, bbox=bbox, colLabels=data.columns, **kwargs)\n    mpl_table.auto_set_font_size(False)\n\n    for k, cell in  six.iteritems(mpl_table._cells):\n        cell.set_edgecolor(edge_color)\n        if k[0] == 0 or k[1] < header_columns:\n            cell.set_facecolor(header_color)\n            cell.set_text_props(color='black', fontproperties = NANUM_bold, fontsize=font_size_header, ha=align_head)\n        else:\n            cell.set_facecolor(row_colors[k[0]%len(row_colors)])\n            cell.set_text_props(fontproperties = NANUM, fontsize=font_size, ha=align_cell)\n    return ax\n\nimage = render_mpl_table(caller, col_width=2.0, align_head='left')\nimage\nimage.figure.savefig(\"caller.png\") # 이미지 저장\n# crontab으로 돌릴 것이라면 이미지 저장 경로도 절대경로로 지정"
  },
  {
    "objectID": "posts/2022-07-05-Ubuntu 새 유저 SSH key 추가.html",
    "href": "posts/2022-07-05-Ubuntu 새 유저 SSH key 추가.html",
    "title": "Lim's Code Archive",
    "section": "",
    "text": "badges: false\ncomments: true\nauthor: limyj0708\ncategories: [Linux]\n\n\n\n\nOracle Free tier에서 Ubuntu instance를 만들면, 기본 계정명이 ubuntu다.\n\n계정을 새로 만들면, 처음에 생성했던 SSH key로는 접속이 되지 않는다.\n\nkey를 새로 만든 후에, instance에 등록해보자.\n\n\n\n\n\nsudo adduser limyj0708 : limyj0708 계정 생성\n\npassword 설정 진행\n몇 가지 정보 적당히 씀 (Full_name 등)\n\nsudo usermod -aG sudo limyj0708 : sudo 그룹을 limyj0708에 추가하여 sudo 명령어를 사용 가능하게 세팅\n\n\n\n\n ssh-keygen -t rsa -N \"원하는 password\" -b 2048 -C \"원하는 comment\" -f \"원하는 file path\"\n\nbyte는 최소 2048로 진행\nfile path에 지정한 경로, 이름으로 공개키와 비밀키가 생성된다.\n\n\n\n\n\nlocal이 리눅스면 ssh-copy-id를 쓰고, 윈도우에서도 해당 명령어에 대응하는 powershell 명령어가 있던데, 솔직히 잘 안 되었다. 권한 문제 뜨고.\n그래서 수동으로 추가하기로 한다.\n\n\n새로 생성한 계정에서는 처음에 /.ssh 폴더가 없다. 생성해줘야 한다.\nubuntu 계정으로 접속\n\nsudo su : root 계정으로 변경\nmkdir /home/limyj0708/.ssh : .ssh 폴더 생성\nnano /home/limyj0708/.ssh/authorized_keys : authorized_keys 파일도 없어서 새로 생성된다.\n\n이제 위에서 생성했던 공개키의 내용을 붙여넣고 저장한다.\n\n\n\n\n\n\n\nOpenssh 윈도우에서도 잘 되니까, Powershell에서 아래 명령어로 접속한다.\n\nssh -i \"비밀키 경로\" limyj0708@서버ip\n\nProfit!"
  },
  {
    "objectID": "posts/2019-10-12-Map vs List Comprehension.html",
    "href": "posts/2019-10-12-Map vs List Comprehension.html",
    "title": "Lim's Code Archive",
    "section": "",
    "text": "List comprehension vs Map\n위의 Stack Overflow 질문에 아주 좋은 답변들이 달려 있어서, Upvote 상위 두 개 답변을 살펴보았다.\n\n\n\nMap이 몇몇 경우에 아주 약간 더 빠르다. (lambda 안 쓰고, 같은 기능을 하는 함수를 사용할 경우) List Comprehension은 나머지 경우에서 더 빠르며, 대부분의 파이썬 사용자들은 List Comprehension이 더 직관적이고 명확하다고 생각한다.\n# 터미널에서 아래와 같이 실행해보자\n\n$ python -mtimeit -s'xs=range(10)' 'map(hex, xs)'\n100000 loops, best of 3: 4.86 usec per loop\n# hex() -> 16진수로 변경\n\n$ python -mtimeit -s'xs=range(10)' '[hex(x) for x in xs]'\n100000 loops, best of 3: 5.58 usec per loop\n# 그냥 함수를 사용했더니, map이 근소하게 더 빠르다\n하지만 lambda를 쓰면 어떨까?\n$ python -mtimeit -s'xs=range(10)' 'map(lambda x: x+2, xs)'\n100000 loops, best of 3: 4.24 usec per loop\n$ python -mtimeit -s'xs=range(10)' '[x+2 for x in xs]'\n100000 loops, best of 3: 2.32 usec per loop\n\n# 속도가 정 반대가 되었다.\n\n\n\nLaziness\nPython에서 Map은 게으르다. 무슨 말인고 하니, 계산 결과 전체를 반환하는 것이 아니라, 계산 로직을 보관하고 있다가 값 요청이 왔을 때 계산하여 값을 제공해준다는 것이다.\n>>> map(str, range(10**100))\n<map object at 0x2201d50>\n# 리스트가 아니다\nList Comprehension이라면 전체 계산결과 리스트를 반환한다. (Not lazy)\n>>> [str(n) for n in range(10**100)]\n# 이런 짓 하지 말라는 것이다.\n# DO NOT TRY THIS AT HOME OR YOU WILL BE SAD #\n‘게으른’ List Comprehension도 Generator expression의 형태로 지원한다.\n>>> (str(n) for n in range(10**100))\n<generator object <genexpr> at 0xacbdef>"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "limyj_code_archive",
    "section": "",
    "text": "Linux\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLinux\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nlimyj0708\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nOct 7, 2022\n\n\nHarlow Malloc\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nOct 4, 2022\n\n\nTristan O’Malley\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  }
]